{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax classifier for guessing minesweeper board position and whether it has a mine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as r\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = (12,12)\n",
    "mineProbability = 0.16      # Probability that a square contain a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clears a square on the minesweeper board.\n",
    "# If it had a mine, return true\n",
    "# Otherwise if it has no adjacent mines, recursively run on adjacent squares\n",
    "# Return false\n",
    "def clearSquare(board,adjacency,row,col):\n",
    "    if board[row,col] == 1:\n",
    "        return True\n",
    "    if adjacency[row,col] >= 0:\n",
    "        return False\n",
    "    n = 0\n",
    "    for r in range(row-1,row+2):\n",
    "        for c in range(col-1,col+2):\n",
    "            if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                n += board[r,c]\n",
    "    adjacency[row,col] = n\n",
    "    if n == 0:\n",
    "        for r in range(row-1,row+2):\n",
    "            for c in range(col-1,col+2):\n",
    "                if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                    clearSquare(board,adjacency,r,c)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes a mine board and gives a mine count with mines removed, and other random squares removed\n",
    "def boardPartialMineCounts(board):\n",
    "    clearProbability = r.uniform(0.05,0.5)\n",
    "    result = np.full(dimensions,-1)\n",
    "    for index, x in np.ndenumerate(board):\n",
    "        row,col = index\n",
    "        if not(x) and result[row,col] == -1 and r.uniform(0,1) < clearProbability:\n",
    "            clearSquare(board,result,row,col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates a random training batch of size at most n\n",
    "def next_training_batch(n):\n",
    "    batch_xs = []\n",
    "    batch_ys = []\n",
    "    boards = []\n",
    "    for _ in range(n):\n",
    "        board = np.random.random(dimensions) < mineProbability\n",
    "        counts = boardPartialMineCounts(board)\n",
    "        validGuesses = np.append(((counts == -1).astype(int) - board).flatten().astype(float),\n",
    "                                 board.flatten().astype(float))\n",
    "        validGuessesSum = sum(validGuesses)\n",
    "        if validGuessesSum > 0:\n",
    "            # encode counts as one hot\n",
    "            countsOneHot = np.zeros((counts.size,10))\n",
    "            countsOneHot[np.arange(counts.size), counts.flatten() + 1] = 1\n",
    "            batch_xs.append(countsOneHot.flatten())\n",
    "            batch_ys.append(validGuesses / validGuessesSum)\n",
    "            boards.append(board)\n",
    "    return (np.asarray(batch_xs), np.asarray(batch_ys), boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "rows, cols = dimensions\n",
    "size = rows*cols\n",
    "mineCountsOneHot = tf.placeholder(tf.float32, [None, size*10], name=\"mineCountsOneHot\")\n",
    "#mineCountsOneHot = tf.reshape(tf.one_hot(mineCounts+1,10), [-1, size*10])\n",
    "W = tf.Variable(tf.random_normal([size*10, size*2], stddev=0.01), name=\"W\")\n",
    "b = tf.Variable(tf.random_normal([size*2], stddev=0.01), name=\"b\")\n",
    "y = tf.matmul(mineCountsOneHot, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validGuessAverages = tf.placeholder(tf.float32, [None, size*2], name=\"validGuessAverages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=validGuessAverages, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summaries for tensorboard\n",
    "with tf.name_scope('W_reshape'):\n",
    "    image_shaped_W = tf.reshape(W, [-1, size*10, size*2, 1])\n",
    "    tf.summary.image('W', image_shaped_W, 1000)\n",
    "\n",
    "with tf.name_scope('b_reshape'):\n",
    "    image_shaped_b = tf.reshape(b, [-1, rows*2, cols, 1])\n",
    "    tf.summary.image('b', image_shaped_b, 1000)\n",
    "\n",
    "_ = tf.summary.scalar('loss', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimiser\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create session and initialise or restore stuff\n",
    "savePath = './saves.tf.Mines6/' + str(dimensions) + '/'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('.', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore model?\n",
    "#saver.restore(sess, savePath + \"model-8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-07 14:18:04: Loss at step 0: 5.672\n",
      "Model saved in file: ./saves.tf.Mines6/(12, 12)/model-0\n",
      "2017-11-07 14:18:06: Loss at step 1: 5.6473\n",
      "2017-11-07 14:18:08: Loss at step 2: 5.63212\n",
      "2017-11-07 14:18:10: Loss at step 3: 5.61701\n",
      "2017-11-07 14:18:12: Loss at step 4: 5.61179\n",
      "2017-11-07 14:18:14: Loss at step 5: 5.6016\n",
      "2017-11-07 14:18:15: Loss at step 6: 5.59099\n",
      "2017-11-07 14:18:17: Loss at step 7: 5.57955\n",
      "2017-11-07 14:18:19: Loss at step 8: 5.57101\n",
      "2017-11-07 14:18:21: Loss at step 9: 5.56409\n",
      "2017-11-07 14:18:23: Loss at step 10: 5.55115\n",
      "2017-11-07 14:18:25: Loss at step 11: 5.53616\n",
      "2017-11-07 14:18:26: Loss at step 12: 5.53234\n",
      "2017-11-07 14:18:28: Loss at step 13: 5.52216\n",
      "2017-11-07 14:18:30: Loss at step 14: 5.51348\n",
      "2017-11-07 14:18:32: Loss at step 15: 5.50518\n",
      "2017-11-07 14:18:34: Loss at step 16: 5.49984\n",
      "2017-11-07 14:18:36: Loss at step 17: 5.49259\n",
      "2017-11-07 14:18:38: Loss at step 18: 5.48465\n",
      "2017-11-07 14:18:39: Loss at step 19: 5.4753\n",
      "2017-11-07 14:18:41: Loss at step 20: 5.46965\n",
      "2017-11-07 14:18:43: Loss at step 21: 5.46572\n",
      "2017-11-07 14:18:45: Loss at step 22: 5.45516\n",
      "2017-11-07 14:18:47: Loss at step 23: 5.44981\n",
      "2017-11-07 14:18:49: Loss at step 24: 5.44262\n",
      "2017-11-07 14:18:50: Loss at step 25: 5.43526\n",
      "2017-11-07 14:18:52: Loss at step 26: 5.43028\n",
      "2017-11-07 14:18:54: Loss at step 27: 5.42756\n",
      "2017-11-07 14:18:56: Loss at step 28: 5.42117\n",
      "2017-11-07 14:18:58: Loss at step 29: 5.4137\n",
      "2017-11-07 14:19:00: Loss at step 30: 5.40762\n",
      "2017-11-07 14:19:02: Loss at step 31: 5.40548\n",
      "2017-11-07 14:19:03: Loss at step 32: 5.39965\n",
      "2017-11-07 14:19:05: Loss at step 33: 5.39217\n",
      "2017-11-07 14:19:07: Loss at step 34: 5.37888\n",
      "2017-11-07 14:19:09: Loss at step 35: 5.38637\n",
      "2017-11-07 14:19:11: Loss at step 36: 5.37827\n",
      "2017-11-07 14:19:13: Loss at step 37: 5.37208\n",
      "2017-11-07 14:19:14: Loss at step 38: 5.37143\n",
      "2017-11-07 14:19:16: Loss at step 39: 5.3635\n",
      "2017-11-07 14:19:18: Loss at step 40: 5.35778\n",
      "2017-11-07 14:19:20: Loss at step 41: 5.35969\n",
      "2017-11-07 14:19:22: Loss at step 42: 5.35261\n",
      "2017-11-07 14:19:23: Loss at step 43: 5.34346\n",
      "2017-11-07 14:19:25: Loss at step 44: 5.33998\n",
      "2017-11-07 14:19:27: Loss at step 45: 5.33764\n",
      "2017-11-07 14:19:29: Loss at step 46: 5.33128\n",
      "2017-11-07 14:19:31: Loss at step 47: 5.3268\n",
      "2017-11-07 14:19:33: Loss at step 48: 5.32272\n",
      "2017-11-07 14:19:34: Loss at step 49: 5.32514\n",
      "2017-11-07 14:19:36: Loss at step 50: 5.31963\n",
      "2017-11-07 14:19:38: Loss at step 51: 5.31568\n",
      "2017-11-07 14:19:40: Loss at step 52: 5.30763\n",
      "2017-11-07 14:19:42: Loss at step 53: 5.30431\n",
      "2017-11-07 14:19:44: Loss at step 54: 5.29918\n",
      "2017-11-07 14:19:45: Loss at step 55: 5.30023\n",
      "2017-11-07 14:19:47: Loss at step 56: 5.29332\n",
      "2017-11-07 14:19:49: Loss at step 57: 5.29655\n",
      "2017-11-07 14:19:51: Loss at step 58: 5.28646\n",
      "2017-11-07 14:19:53: Loss at step 59: 5.2855\n",
      "2017-11-07 14:19:54: Loss at step 60: 5.27392\n",
      "2017-11-07 14:19:56: Loss at step 61: 5.27568\n",
      "2017-11-07 14:19:58: Loss at step 62: 5.27481\n",
      "2017-11-07 14:20:00: Loss at step 63: 5.26353\n",
      "2017-11-07 14:20:02: Loss at step 64: 5.26039\n",
      "2017-11-07 14:20:04: Loss at step 65: 5.26557\n",
      "2017-11-07 14:20:05: Loss at step 66: 5.2613\n",
      "2017-11-07 14:20:07: Loss at step 67: 5.2534\n",
      "2017-11-07 14:20:09: Loss at step 68: 5.24052\n",
      "2017-11-07 14:20:11: Loss at step 69: 5.24811\n",
      "2017-11-07 14:20:13: Loss at step 70: 5.24908\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for iteration in range(10001):\n",
    "    batch_xs, batch_ys, _ = next_training_batch(1000)\n",
    "    summary, loss, _ = sess.run([merged, cross_entropy, train_step],\n",
    "                                  feed_dict={mineCountsOneHot: batch_xs, validGuessAverages: batch_ys})\n",
    "    writer.add_summary(summary, iteration)\n",
    "    print('%s: Loss at step %s: %s' % (dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), iteration, loss))\n",
    "    if iteration % 100 == 0:\n",
    "        save_path = saver.save(sess, savePath + 'model', global_step=iteration)\n",
    "        print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test trained model on larger batch size\n",
    "batch_xs, batch_ys, _ = next_training_batch(1000)\n",
    "print(sess.run(cross_entropy, feed_dict={mineCountsOneHot: batch_xs, validGuessAverages: batch_ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test\n",
    "batchSize = 10000\n",
    "batch_xs, batch_ys, _ = next_training_batch(batchSize)\n",
    "\n",
    "predictions = sess.run(tf.nn.softmax(y), feed_dict={mineCountsOneHot: batch_xs, validGuessAverages: batch_ys})\n",
    "bestSquares = [pred.argmax() for pred in predictions]\n",
    "unfrees = (batch_ys == 0).astype(int)\n",
    "frees = [unfrees[i][bestSquares[i]] for i in range(batchSize)]\n",
    "print(\"Number of errors for batch size of \", batchSize)\n",
    "print(sum(frees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find boards that we failed on\n",
    "batchSize = 1000\n",
    "batch_xs, batch_ys, _ = next_training_batch(batchSize)\n",
    "\n",
    "predictions = sess.run(tf.nn.softmax(y), feed_dict={mineCountsOneHot: batch_xs, validGuessAverages: batch_ys})\n",
    "bestSquares = [pred.argmax() for pred in predictions]\n",
    "unfrees = (batch_ys == 0).astype(int)\n",
    "guesses = [unfrees[i][bestSquares[i]] for i in range(batchSize)]\n",
    "for i in range(batchSize):\n",
    "    if guesses[i] == 1:\n",
    "        print(batch_xs[i].reshape(dimensions))\n",
    "        summary = sess.run(tf.summary.image('mine_miss', tf.reshape((batch_xs[i]+1).astype(float),[-1,rows,cols,1]), 100))\n",
    "        writer.add_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_xs = [[-1,1,-1,0,0,0,-1,-1,-1,1,1,1,-1,-1,1,2,2,1,1,-1,2,1,1,-1,-1,2,2,-1,-1,2,-1,1,1,0,-1,-1,2,-1,-1,4,-1,2,-1,1,2,-1,1,0,1,2,-1,3,2,2,1,-1,2,-1,1,0,0,1,1,-1,-1,-1,-1,-1,-1,1,1,-1,-1,0,0,3,-1,4,1,2,-1,1,-1,-1,0,0,0,2,-1,-1,-1,2,-1,1,0,0,0,-1,1,2,-1,2,1,2,2,3,3,2,-1,-1,1,-1,1,-1,0,1,2,-1,-1,-1,1,1,1,-1,1,0,-1,-1,-1,-1,-1,-1,-1,-1,0,-1,-1,-1,-1,-1,1,-1,-1,-1]]\n",
    "batch_xs0 = [-1] * (size)\n",
    "batch_xs0[0] = 1\n",
    "batch_xs0[1] = 1\n",
    "batch_xs0[cols] = 1\n",
    "\n",
    "predictions = sess.run(tf.nn.softmax(y), feed_dict={mineCountsOneHot: [batch_xs0]})\n",
    "bestSquares = [pred.argmax() for pred in predictions]\n",
    "\n",
    "print(bestSquares[0] // cols, bestSquares[0] % cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./W\", sess.run(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./b\", sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(\"./model\", sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
