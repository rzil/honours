{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier for guessing minesweeper board position and whether it has a mine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as r\n",
    "import datetime as dt\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = (8,8)\n",
    "mineProbability = 0.16      # Probability that a square contain a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears a square on the minesweeper board.\n",
    "# If it had a mine, return true\n",
    "# Otherwise if it has no adjacent mines, recursively run on adjacent squares\n",
    "# Return false\n",
    "def clearSquare(board,adjacency,row,col):\n",
    "    rows,cols = dimensions\n",
    "    if board[row,col] == 1:\n",
    "        return True\n",
    "    if adjacency[row,col] >= 0:\n",
    "        return False\n",
    "    n = 0\n",
    "    for r in range(row-1,row+2):\n",
    "        for c in range(col-1,col+2):\n",
    "            if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                n += board[r,c]\n",
    "    adjacency[row,col] = n\n",
    "    if n == 0:\n",
    "        for r in range(row-1,row+2):\n",
    "            for c in range(col-1,col+2):\n",
    "                if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                    clearSquare(board,adjacency,r,c)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a mine board and gives a mine count with mines removed, and other random squares removed\n",
    "# At least one square will be clear\n",
    "def boardPartialMineCounts(board):\n",
    "    clearProbability = r.uniform(0.05,0.5)\n",
    "    result = np.full(dimensions,-1)\n",
    "    didClear = False\n",
    "    while not(didClear):\n",
    "        for index, x in np.random.permutation(list(np.ndenumerate(board))):\n",
    "            row,col = index\n",
    "            if not(x) and result[row,col] == -1 and r.uniform(0,1) < clearProbability:\n",
    "                clearSquare(board,result,row,col)\n",
    "                didClear = True\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 8, in boardPartialMineCounts\n",
      "    for index, x in np.random.permutation(list(np.ndenumerate(board))):\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  [Previous line repeated 11 more times]\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 15, in clearSquare\n",
      "    n += board[r,c]\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"/home/ruben/tensorflow/lib/python3.6/site-packages/numpy/lib/index_tricks.py\", line 528, in __next__\n",
      "    return self.iter.coords, next(self.iter)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 8, in boardPartialMineCounts\n",
      "    for index, x in np.random.permutation(list(np.ndenumerate(board))):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 15, in clearSquare\n",
      "    n += board[r,c]\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 15, in clearSquare\n",
      "    n += board[r,c]\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 21, in clearSquare\n",
      "    clearSquare(board,adjacency,r,c)\n",
      "  [Previous line repeated 5 more times]\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 15, in clearSquare\n",
      "    n += board[r,c]\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 7, in clearSquare\n",
      "    if board[row,col] == 1:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-4-2dcf4148d008>\", line 11, in boardPartialMineCounts\n",
      "    clearSquare(board,result,row,col)\n",
      "  File \"<ipython-input-3-fe9f3b44f6bc>\", line 15, in clearSquare\n",
      "    n += board[r,c]\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Generates a random training batch of size n\n",
    "def randomBoard(i):\n",
    "    return(np.random.random(dimensions) < mineProbability)\n",
    "\n",
    "def encodeCountsOneHot(counts):\n",
    "    countsOneHot = np.zeros((counts.size,10))\n",
    "    countsOneHot[np.arange(counts.size), counts.flatten() + 1] = 1\n",
    "    return(countsOneHot.flatten())\n",
    "\n",
    "def validGuesses(boardAndCounts):\n",
    "    board,counts = boardAndCounts\n",
    "    validGuesses = np.append(((counts == -1).astype(int) - board).flatten().astype(float),\n",
    "        board.flatten().astype(float))\n",
    "    validGuessesSum = sum(validGuesses)\n",
    "    if validGuessesSum > 0:\n",
    "        return(validGuesses / validGuessesSum)\n",
    "    else:\n",
    "        return(np.zeros(board.size*2))\n",
    "\n",
    "try:\n",
    "    cpus = mp.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2   # arbitrary default\n",
    "\n",
    "pool = mp.Pool(processes=cpus)\n",
    "\n",
    "def next_training_batch(n):\n",
    "    boards = pool.map(randomBoard, range(n))\n",
    "    counts = pool.map(boardPartialMineCounts, boards)\n",
    "    batch_xs = pool.map(encodeCountsOneHot, counts)\n",
    "    batch_ys = pool.map(validGuesses, zip(boards,counts))\n",
    "    return(batch_xs, batch_ys, boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = dimensions\n",
    "size = rows*cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=size, input_dim=size*10))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((rows, cols, 1)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((4*4*32,)))\n",
    "\n",
    "#model.add(tf.keras.layers.Dense(units=1024))\n",
    "#model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=size*2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"/media/ruben/BigDisk/tensorflow/tensorflow-logs/tf.Mines8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('{0}/model-3500.h5'.format(savePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 06:52:33: Loss at step 3501: 0.036921218037605286\n",
      "2017-11-11 06:52:35: Loss at step 3502: 0.03676200285553932\n",
      "2017-11-11 06:52:38: Loss at step 3503: 0.036942172795534134\n",
      "2017-11-11 06:52:40: Loss at step 3504: 0.036985404789447784\n",
      "2017-11-11 06:52:42: Loss at step 3505: 0.03677597641944885\n",
      "2017-11-11 06:52:44: Loss at step 3506: 0.03680059313774109\n",
      "2017-11-11 06:52:46: Loss at step 3507: 0.03687511757016182\n",
      "2017-11-11 06:52:48: Loss at step 3508: 0.036910898983478546\n",
      "2017-11-11 06:52:50: Loss at step 3509: 0.03695701062679291\n",
      "2017-11-11 06:52:53: Loss at step 3510: 0.03688173368573189\n",
      "2017-11-11 06:52:55: Loss at step 3511: 0.03668404370546341\n",
      "2017-11-11 06:52:57: Loss at step 3512: 0.03682910278439522\n",
      "2017-11-11 06:52:59: Loss at step 3513: 0.036751989275217056\n",
      "2017-11-11 06:53:01: Loss at step 3514: 0.036820635199546814\n",
      "2017-11-11 06:53:03: Loss at step 3515: 0.03678290545940399\n",
      "2017-11-11 06:53:06: Loss at step 3516: 0.036781515926122665\n",
      "2017-11-11 06:53:08: Loss at step 3517: 0.036831025034189224\n",
      "2017-11-11 06:53:10: Loss at step 3518: 0.0368858203291893\n",
      "2017-11-11 06:53:13: Loss at step 3519: 0.03683767095208168\n",
      "2017-11-11 06:53:15: Loss at step 3520: 0.03684648871421814\n",
      "2017-11-11 06:53:17: Loss at step 3521: 0.03687470033764839\n",
      "2017-11-11 06:53:19: Loss at step 3522: 0.03674947842955589\n",
      "2017-11-11 06:53:21: Loss at step 3523: 0.0368412584066391\n",
      "2017-11-11 06:53:23: Loss at step 3524: 0.036776911467313766\n",
      "2017-11-11 06:53:26: Loss at step 3525: 0.03681548684835434\n",
      "2017-11-11 06:53:28: Loss at step 3526: 0.03690539672970772\n",
      "2017-11-11 06:53:30: Loss at step 3527: 0.036798108369112015\n",
      "2017-11-11 06:53:32: Loss at step 3528: 0.036691803485155106\n",
      "2017-11-11 06:53:34: Loss at step 3529: 0.03675353527069092\n",
      "2017-11-11 06:53:36: Loss at step 3530: 0.036751244217157364\n",
      "2017-11-11 06:53:39: Loss at step 3531: 0.03681902214884758\n",
      "2017-11-11 06:53:41: Loss at step 3532: 0.03677716851234436\n",
      "2017-11-11 06:53:43: Loss at step 3533: 0.036772482097148895\n",
      "2017-11-11 06:53:45: Loss at step 3534: 0.03683270514011383\n",
      "2017-11-11 06:53:47: Loss at step 3535: 0.03675392270088196\n",
      "2017-11-11 06:53:50: Loss at step 3536: 0.03690709173679352\n",
      "2017-11-11 06:53:52: Loss at step 3537: 0.03691001236438751\n",
      "2017-11-11 06:53:54: Loss at step 3538: 0.03681446239352226\n",
      "2017-11-11 06:53:56: Loss at step 3539: 0.036815039813518524\n",
      "2017-11-11 06:53:58: Loss at step 3540: 0.03687582537531853\n",
      "2017-11-11 06:54:00: Loss at step 3541: 0.03678156062960625\n",
      "2017-11-11 06:54:02: Loss at step 3542: 0.03696093708276749\n",
      "2017-11-11 06:54:04: Loss at step 3543: 0.03685147315263748\n",
      "2017-11-11 06:54:07: Loss at step 3544: 0.03676741197705269\n",
      "2017-11-11 06:54:09: Loss at step 3545: 0.03674808889627457\n",
      "2017-11-11 06:54:11: Loss at step 3546: 0.036768846213817596\n",
      "2017-11-11 06:54:13: Loss at step 3547: 0.03679179027676582\n",
      "2017-11-11 06:54:15: Loss at step 3548: 0.03680291771888733\n",
      "2017-11-11 06:54:17: Loss at step 3549: 0.036790888756513596\n",
      "2017-11-11 06:54:20: Loss at step 3550: 0.03688078746199608\n",
      "2017-11-11 06:54:22: Loss at step 3551: 0.036789391189813614\n",
      "2017-11-11 06:54:24: Loss at step 3552: 0.03697816655039787\n",
      "2017-11-11 06:54:26: Loss at step 3553: 0.03692067414522171\n",
      "2017-11-11 06:54:28: Loss at step 3554: 0.036959197372198105\n",
      "2017-11-11 06:54:30: Loss at step 3555: 0.03674791380763054\n",
      "2017-11-11 06:54:32: Loss at step 3556: 0.036693986505270004\n",
      "2017-11-11 06:54:34: Loss at step 3557: 0.036948010325431824\n",
      "2017-11-11 06:54:36: Loss at step 3558: 0.03678020089864731\n",
      "2017-11-11 06:54:39: Loss at step 3559: 0.03687015920877457\n",
      "2017-11-11 06:54:41: Loss at step 3560: 0.03693945333361626\n",
      "2017-11-11 06:54:43: Loss at step 3561: 0.0369119830429554\n",
      "2017-11-11 06:54:45: Loss at step 3562: 0.03683965653181076\n",
      "2017-11-11 06:54:47: Loss at step 3563: 0.036741480231285095\n",
      "2017-11-11 06:54:50: Loss at step 3564: 0.036706436425447464\n",
      "2017-11-11 06:54:52: Loss at step 3565: 0.03679713234305382\n",
      "2017-11-11 06:54:54: Loss at step 3566: 0.03671770542860031\n",
      "2017-11-11 06:54:56: Loss at step 3567: 0.03684677556157112\n",
      "2017-11-11 06:54:58: Loss at step 3568: 0.03687819838523865\n",
      "2017-11-11 06:55:00: Loss at step 3569: 0.036890409886837006\n",
      "2017-11-11 06:55:03: Loss at step 3570: 0.03674520179629326\n",
      "2017-11-11 06:55:05: Loss at step 3571: 0.03677091747522354\n",
      "2017-11-11 06:55:07: Loss at step 3572: 0.036856312304735184\n",
      "2017-11-11 06:55:09: Loss at step 3573: 0.03678907826542854\n",
      "2017-11-11 06:55:11: Loss at step 3574: 0.03677907586097717\n",
      "2017-11-11 06:55:14: Loss at step 3575: 0.036765459924936295\n",
      "2017-11-11 06:55:16: Loss at step 3576: 0.036691706627607346\n",
      "2017-11-11 06:55:18: Loss at step 3577: 0.03681453689932823\n",
      "2017-11-11 06:55:20: Loss at step 3578: 0.03688912093639374\n",
      "2017-11-11 06:55:22: Loss at step 3579: 0.03678781911730766\n",
      "2017-11-11 06:55:24: Loss at step 3580: 0.03683307394385338\n",
      "2017-11-11 06:55:27: Loss at step 3581: 0.036733873188495636\n",
      "2017-11-11 06:55:29: Loss at step 3582: 0.036772340536117554\n",
      "2017-11-11 06:55:31: Loss at step 3583: 0.03683244809508324\n",
      "2017-11-11 06:55:33: Loss at step 3584: 0.03690298646688461\n",
      "2017-11-11 06:55:35: Loss at step 3585: 0.036793287843465805\n",
      "2017-11-11 06:55:37: Loss at step 3586: 0.03683425113558769\n",
      "2017-11-11 06:55:39: Loss at step 3587: 0.03683134540915489\n",
      "2017-11-11 06:55:41: Loss at step 3588: 0.03680026903748512\n",
      "2017-11-11 06:55:43: Loss at step 3589: 0.036746300756931305\n",
      "2017-11-11 06:55:46: Loss at step 3590: 0.036757368594408035\n",
      "2017-11-11 06:55:48: Loss at step 3591: 0.0369187630712986\n",
      "2017-11-11 06:55:50: Loss at step 3592: 0.03679812699556351\n",
      "2017-11-11 06:55:52: Loss at step 3593: 0.03680996224284172\n",
      "2017-11-11 06:55:54: Loss at step 3594: 0.036832891404628754\n",
      "2017-11-11 06:55:56: Loss at step 3595: 0.03694061189889908\n",
      "2017-11-11 06:55:59: Loss at step 3596: 0.036847103387117386\n",
      "2017-11-11 06:56:01: Loss at step 3597: 0.03688420355319977\n",
      "2017-11-11 06:56:03: Loss at step 3598: 0.03679485246539116\n",
      "2017-11-11 06:56:05: Loss at step 3599: 0.03688526153564453\n",
      "2017-11-11 06:56:07: Loss at step 3600: 0.03694882243871689\n",
      "2017-11-11 06:56:09: Loss at step 3601: 0.03689442202448845\n",
      "2017-11-11 06:56:11: Loss at step 3602: 0.03685920685529709\n",
      "2017-11-11 06:56:13: Loss at step 3603: 0.036953624337911606\n",
      "2017-11-11 06:56:16: Loss at step 3604: 0.03689301013946533\n",
      "2017-11-11 06:56:18: Loss at step 3605: 0.03682311251759529\n",
      "2017-11-11 06:56:20: Loss at step 3606: 0.036784008145332336\n",
      "2017-11-11 06:56:23: Loss at step 3607: 0.036861851811409\n",
      "2017-11-11 06:56:25: Loss at step 3608: 0.036728907376527786\n",
      "2017-11-11 06:56:27: Loss at step 3609: 0.03676103055477142\n",
      "2017-11-11 06:56:29: Loss at step 3610: 0.036879438906908035\n",
      "2017-11-11 06:56:31: Loss at step 3611: 0.036801405251026154\n",
      "2017-11-11 06:56:33: Loss at step 3612: 0.036846812814474106\n",
      "2017-11-11 06:56:35: Loss at step 3613: 0.03682003915309906\n",
      "2017-11-11 06:56:37: Loss at step 3614: 0.036753345280885696\n",
      "2017-11-11 06:56:40: Loss at step 3615: 0.03676366060972214\n",
      "2017-11-11 06:56:42: Loss at step 3616: 0.036691177636384964\n",
      "2017-11-11 06:56:44: Loss at step 3617: 0.036870408803224564\n",
      "2017-11-11 06:56:46: Loss at step 3618: 0.036835428327322006\n",
      "2017-11-11 06:56:48: Loss at step 3619: 0.0368315763771534\n",
      "2017-11-11 06:56:50: Loss at step 3620: 0.03679966926574707\n",
      "2017-11-11 06:56:53: Loss at step 3621: 0.03683555871248245\n",
      "2017-11-11 06:56:55: Loss at step 3622: 0.0367741584777832\n",
      "2017-11-11 06:56:57: Loss at step 3623: 0.0367901474237442\n",
      "2017-11-11 06:56:59: Loss at step 3624: 0.03681645169854164\n",
      "2017-11-11 06:57:01: Loss at step 3625: 0.036773331463336945\n",
      "2017-11-11 06:57:03: Loss at step 3626: 0.03675239905714989\n",
      "2017-11-11 06:57:06: Loss at step 3627: 0.03674349933862686\n",
      "2017-11-11 06:57:08: Loss at step 3628: 0.03680502250790596\n",
      "2017-11-11 06:57:10: Loss at step 3629: 0.03680313751101494\n",
      "2017-11-11 06:57:12: Loss at step 3630: 0.03678647801280022\n",
      "2017-11-11 06:57:14: Loss at step 3631: 0.03671019896864891\n",
      "2017-11-11 06:57:16: Loss at step 3632: 0.03689451143145561\n",
      "2017-11-11 06:57:19: Loss at step 3633: 0.03675350174307823\n",
      "2017-11-11 06:57:21: Loss at step 3634: 0.03672989085316658\n",
      "2017-11-11 06:57:23: Loss at step 3635: 0.03678000345826149\n",
      "2017-11-11 06:57:25: Loss at step 3636: 0.03686380013823509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 06:57:27: Loss at step 3637: 0.036783184856176376\n",
      "2017-11-11 06:57:30: Loss at step 3638: 0.036862097680568695\n",
      "2017-11-11 06:57:32: Loss at step 3639: 0.03683163598179817\n",
      "2017-11-11 06:57:34: Loss at step 3640: 0.03677985817193985\n",
      "2017-11-11 06:57:36: Loss at step 3641: 0.03683692589402199\n",
      "2017-11-11 06:57:38: Loss at step 3642: 0.03678060695528984\n",
      "2017-11-11 06:57:41: Loss at step 3643: 0.036833830177783966\n",
      "2017-11-11 06:57:43: Loss at step 3644: 0.036763835698366165\n",
      "2017-11-11 06:57:45: Loss at step 3645: 0.03690661862492561\n",
      "2017-11-11 06:57:47: Loss at step 3646: 0.03689483180642128\n",
      "2017-11-11 06:57:49: Loss at step 3647: 0.03680440038442612\n",
      "2017-11-11 06:57:51: Loss at step 3648: 0.03680308908224106\n",
      "2017-11-11 06:57:53: Loss at step 3649: 0.03684573993086815\n",
      "2017-11-11 06:57:55: Loss at step 3650: 0.03679755702614784\n",
      "2017-11-11 06:57:57: Loss at step 3651: 0.036869700998067856\n",
      "2017-11-11 06:58:00: Loss at step 3652: 0.036748021841049194\n",
      "2017-11-11 06:58:02: Loss at step 3653: 0.03679000213742256\n",
      "2017-11-11 06:58:04: Loss at step 3654: 0.03676868602633476\n",
      "2017-11-11 06:58:06: Loss at step 3655: 0.03678898885846138\n",
      "2017-11-11 06:58:09: Loss at step 3656: 0.03672191500663757\n",
      "2017-11-11 06:58:11: Loss at step 3657: 0.036812715232372284\n",
      "2017-11-11 06:58:13: Loss at step 3658: 0.03676240146160126\n",
      "2017-11-11 06:58:15: Loss at step 3659: 0.0367637500166893\n",
      "2017-11-11 06:58:17: Loss at step 3660: 0.03676411882042885\n",
      "2017-11-11 06:58:19: Loss at step 3661: 0.036858782172203064\n",
      "2017-11-11 06:58:21: Loss at step 3662: 0.036771245300769806\n",
      "2017-11-11 06:58:23: Loss at step 3663: 0.03681578487157822\n",
      "2017-11-11 06:58:26: Loss at step 3664: 0.03683074191212654\n",
      "2017-11-11 06:58:28: Loss at step 3665: 0.03674985468387604\n",
      "2017-11-11 06:58:30: Loss at step 3666: 0.036867279559373856\n",
      "2017-11-11 06:58:32: Loss at step 3667: 0.0368255190551281\n",
      "2017-11-11 06:58:34: Loss at step 3668: 0.03683791309595108\n",
      "2017-11-11 06:58:37: Loss at step 3669: 0.03678084537386894\n",
      "2017-11-11 06:58:39: Loss at step 3670: 0.03674894571304321\n",
      "2017-11-11 06:58:41: Loss at step 3671: 0.03684958443045616\n",
      "2017-11-11 06:58:43: Loss at step 3672: 0.036804456263780594\n",
      "2017-11-11 06:58:45: Loss at step 3673: 0.03672942891716957\n",
      "2017-11-11 06:58:47: Loss at step 3674: 0.0368337444961071\n",
      "2017-11-11 06:58:49: Loss at step 3675: 0.03679456561803818\n",
      "2017-11-11 06:58:51: Loss at step 3676: 0.03683185577392578\n",
      "2017-11-11 06:58:54: Loss at step 3677: 0.03685615956783295\n",
      "2017-11-11 06:58:56: Loss at step 3678: 0.03691578656435013\n",
      "2017-11-11 06:58:58: Loss at step 3679: 0.0368213914334774\n",
      "2017-11-11 06:59:00: Loss at step 3680: 0.03679482266306877\n",
      "2017-11-11 06:59:02: Loss at step 3681: 0.036720920354127884\n",
      "2017-11-11 06:59:04: Loss at step 3682: 0.03682626411318779\n",
      "2017-11-11 06:59:07: Loss at step 3683: 0.03668912872672081\n",
      "2017-11-11 06:59:09: Loss at step 3684: 0.0367216058075428\n",
      "2017-11-11 06:59:11: Loss at step 3685: 0.03684156388044357\n",
      "2017-11-11 06:59:13: Loss at step 3686: 0.036889348179101944\n",
      "2017-11-11 06:59:15: Loss at step 3687: 0.0368068665266037\n",
      "2017-11-11 06:59:17: Loss at step 3688: 0.03681648150086403\n",
      "2017-11-11 06:59:19: Loss at step 3689: 0.036749113351106644\n",
      "2017-11-11 06:59:21: Loss at step 3690: 0.036745358258485794\n",
      "2017-11-11 06:59:24: Loss at step 3691: 0.03668360039591789\n",
      "2017-11-11 06:59:26: Loss at step 3692: 0.036866694688797\n",
      "2017-11-11 06:59:28: Loss at step 3693: 0.03674061968922615\n",
      "2017-11-11 06:59:30: Loss at step 3694: 0.036855436861515045\n",
      "2017-11-11 06:59:32: Loss at step 3695: 0.036846015602350235\n",
      "2017-11-11 06:59:34: Loss at step 3696: 0.03685897961258888\n",
      "2017-11-11 06:59:36: Loss at step 3697: 0.03681620582938194\n",
      "2017-11-11 06:59:39: Loss at step 3698: 0.036719463765621185\n",
      "2017-11-11 06:59:41: Loss at step 3699: 0.03674892708659172\n",
      "2017-11-11 06:59:43: Loss at step 3700: 0.03675578907132149\n",
      "2017-11-11 06:59:45: Loss at step 3701: 0.03673204779624939\n",
      "2017-11-11 06:59:47: Loss at step 3702: 0.03683479502797127\n",
      "2017-11-11 06:59:49: Loss at step 3703: 0.03681829944252968\n",
      "2017-11-11 06:59:52: Loss at step 3704: 0.036814868450164795\n",
      "2017-11-11 06:59:54: Loss at step 3705: 0.03673990070819855\n",
      "2017-11-11 06:59:56: Loss at step 3706: 0.03680408373475075\n",
      "2017-11-11 06:59:58: Loss at step 3707: 0.036795105785131454\n",
      "2017-11-11 07:00:00: Loss at step 3708: 0.03683491796255112\n",
      "2017-11-11 07:00:02: Loss at step 3709: 0.03683906048536301\n",
      "2017-11-11 07:00:05: Loss at step 3710: 0.036748845130205154\n",
      "2017-11-11 07:00:07: Loss at step 3711: 0.03674449399113655\n",
      "2017-11-11 07:00:09: Loss at step 3712: 0.03679012879729271\n",
      "2017-11-11 07:00:11: Loss at step 3713: 0.036819398403167725\n",
      "2017-11-11 07:00:13: Loss at step 3714: 0.03694954887032509\n",
      "2017-11-11 07:00:15: Loss at step 3715: 0.03670916706323624\n",
      "2017-11-11 07:00:18: Loss at step 3716: 0.03678680583834648\n",
      "2017-11-11 07:00:20: Loss at step 3717: 0.03677413612604141\n",
      "2017-11-11 07:00:22: Loss at step 3718: 0.03678559511899948\n",
      "2017-11-11 07:00:24: Loss at step 3719: 0.03677927330136299\n",
      "2017-11-11 07:00:26: Loss at step 3720: 0.03674335405230522\n",
      "2017-11-11 07:00:28: Loss at step 3721: 0.03685814142227173\n",
      "2017-11-11 07:00:31: Loss at step 3722: 0.03677814453840256\n",
      "2017-11-11 07:00:33: Loss at step 3723: 0.03682427480816841\n",
      "2017-11-11 07:00:35: Loss at step 3724: 0.03682198002934456\n",
      "2017-11-11 07:00:37: Loss at step 3725: 0.036843881011009216\n",
      "2017-11-11 07:00:39: Loss at step 3726: 0.03686441481113434\n",
      "2017-11-11 07:00:41: Loss at step 3727: 0.036856088787317276\n",
      "2017-11-11 07:00:43: Loss at step 3728: 0.036769136786460876\n",
      "2017-11-11 07:00:46: Loss at step 3729: 0.03677797690033913\n",
      "2017-11-11 07:00:48: Loss at step 3730: 0.03674139454960823\n",
      "2017-11-11 07:00:50: Loss at step 3731: 0.03680849447846413\n",
      "2017-11-11 07:00:52: Loss at step 3732: 0.03681160882115364\n",
      "2017-11-11 07:00:54: Loss at step 3733: 0.03679291158914566\n",
      "2017-11-11 07:00:57: Loss at step 3734: 0.036892980337142944\n",
      "2017-11-11 07:00:59: Loss at step 3735: 0.03687998279929161\n",
      "2017-11-11 07:01:01: Loss at step 3736: 0.036828868091106415\n",
      "2017-11-11 07:01:03: Loss at step 3737: 0.03693574294447899\n",
      "2017-11-11 07:01:05: Loss at step 3738: 0.03688080981373787\n",
      "2017-11-11 07:01:07: Loss at step 3739: 0.036884237080812454\n",
      "2017-11-11 07:01:09: Loss at step 3740: 0.03683529794216156\n",
      "2017-11-11 07:01:11: Loss at step 3741: 0.0368766188621521\n",
      "2017-11-11 07:01:14: Loss at step 3742: 0.036873530596494675\n",
      "2017-11-11 07:01:16: Loss at step 3743: 0.03670022636651993\n",
      "2017-11-11 07:01:18: Loss at step 3744: 0.03684275224804878\n",
      "2017-11-11 07:01:20: Loss at step 3745: 0.0368441566824913\n",
      "2017-11-11 07:01:22: Loss at step 3746: 0.03692350536584854\n",
      "2017-11-11 07:01:25: Loss at step 3747: 0.03685451298952103\n",
      "2017-11-11 07:01:27: Loss at step 3748: 0.03678935021162033\n",
      "2017-11-11 07:01:29: Loss at step 3749: 0.03696485236287117\n",
      "2017-11-11 07:01:31: Loss at step 3750: 0.0368955060839653\n",
      "2017-11-11 07:01:33: Loss at step 3751: 0.03692157566547394\n",
      "2017-11-11 07:01:35: Loss at step 3752: 0.03691675886511803\n",
      "2017-11-11 07:01:37: Loss at step 3753: 0.03689630702137947\n",
      "2017-11-11 07:01:39: Loss at step 3754: 0.036935608834028244\n",
      "2017-11-11 07:01:42: Loss at step 3755: 0.03696136549115181\n",
      "2017-11-11 07:01:44: Loss at step 3756: 0.036858197301626205\n",
      "2017-11-11 07:01:46: Loss at step 3757: 0.036796629428863525\n",
      "2017-11-11 07:01:48: Loss at step 3758: 0.03689822554588318\n",
      "2017-11-11 07:01:50: Loss at step 3759: 0.03681369498372078\n",
      "2017-11-11 07:01:52: Loss at step 3760: 0.036900464445352554\n",
      "2017-11-11 07:01:55: Loss at step 3761: 0.036880698055028915\n",
      "2017-11-11 07:01:57: Loss at step 3762: 0.036919817328453064\n",
      "2017-11-11 07:01:59: Loss at step 3763: 0.036844246089458466\n",
      "2017-11-11 07:02:01: Loss at step 3764: 0.03683314844965935\n",
      "2017-11-11 07:02:03: Loss at step 3765: 0.03688119351863861\n",
      "2017-11-11 07:02:05: Loss at step 3766: 0.03696206957101822\n",
      "2017-11-11 07:02:07: Loss at step 3767: 0.036831654608249664\n",
      "2017-11-11 07:02:09: Loss at step 3768: 0.03684839606285095\n",
      "2017-11-11 07:02:12: Loss at step 3769: 0.0368606336414814\n",
      "2017-11-11 07:02:14: Loss at step 3770: 0.036953385919332504\n",
      "2017-11-11 07:02:16: Loss at step 3771: 0.03693389892578125\n",
      "2017-11-11 07:02:18: Loss at step 3772: 0.03694317117333412\n",
      "2017-11-11 07:02:20: Loss at step 3773: 0.036838941276073456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:02:22: Loss at step 3774: 0.03683708235621452\n",
      "2017-11-11 07:02:25: Loss at step 3775: 0.03674326837062836\n",
      "2017-11-11 07:02:27: Loss at step 3776: 0.036864910274744034\n",
      "2017-11-11 07:02:29: Loss at step 3777: 0.03692103922367096\n",
      "2017-11-11 07:02:31: Loss at step 3778: 0.03693343326449394\n",
      "2017-11-11 07:02:33: Loss at step 3779: 0.03693638741970062\n",
      "2017-11-11 07:02:35: Loss at step 3780: 0.03694935142993927\n",
      "2017-11-11 07:02:38: Loss at step 3781: 0.036845654249191284\n",
      "2017-11-11 07:02:40: Loss at step 3782: 0.03680649772286415\n",
      "2017-11-11 07:02:42: Loss at step 3783: 0.036757029592990875\n",
      "2017-11-11 07:02:44: Loss at step 3784: 0.036862291395664215\n",
      "2017-11-11 07:02:46: Loss at step 3785: 0.036874134093523026\n",
      "2017-11-11 07:02:48: Loss at step 3786: 0.03688378259539604\n",
      "2017-11-11 07:02:51: Loss at step 3787: 0.036756839603185654\n",
      "2017-11-11 07:02:53: Loss at step 3788: 0.036815494298934937\n",
      "2017-11-11 07:02:55: Loss at step 3789: 0.036760784685611725\n",
      "2017-11-11 07:02:57: Loss at step 3790: 0.03674517944455147\n",
      "2017-11-11 07:02:59: Loss at step 3791: 0.036845359951257706\n",
      "2017-11-11 07:03:01: Loss at step 3792: 0.03686567768454552\n",
      "2017-11-11 07:03:04: Loss at step 3793: 0.0367879681289196\n",
      "2017-11-11 07:03:06: Loss at step 3794: 0.03694708272814751\n",
      "2017-11-11 07:03:08: Loss at step 3795: 0.036854565143585205\n",
      "2017-11-11 07:03:10: Loss at step 3796: 0.036726292222738266\n",
      "2017-11-11 07:03:12: Loss at step 3797: 0.03666527196764946\n",
      "2017-11-11 07:03:14: Loss at step 3798: 0.036809101700782776\n",
      "2017-11-11 07:03:17: Loss at step 3799: 0.03666934743523598\n",
      "2017-11-11 07:03:19: Loss at step 3800: 0.036978594958782196\n",
      "2017-11-11 07:03:21: Loss at step 3801: 0.03684278577566147\n",
      "2017-11-11 07:03:23: Loss at step 3802: 0.03680640831589699\n",
      "2017-11-11 07:03:25: Loss at step 3803: 0.03669792041182518\n",
      "2017-11-11 07:03:27: Loss at step 3804: 0.03678590804338455\n",
      "2017-11-11 07:03:29: Loss at step 3805: 0.03670339286327362\n",
      "2017-11-11 07:03:31: Loss at step 3806: 0.036715373396873474\n",
      "2017-11-11 07:03:34: Loss at step 3807: 0.03677475452423096\n",
      "2017-11-11 07:03:36: Loss at step 3808: 0.03683038055896759\n",
      "2017-11-11 07:03:38: Loss at step 3809: 0.03685016930103302\n",
      "2017-11-11 07:03:40: Loss at step 3810: 0.03675481677055359\n",
      "2017-11-11 07:03:42: Loss at step 3811: 0.03670460358262062\n",
      "2017-11-11 07:03:44: Loss at step 3812: 0.03674362227320671\n",
      "2017-11-11 07:03:46: Loss at step 3813: 0.03679334744811058\n",
      "2017-11-11 07:03:49: Loss at step 3814: 0.03685690462589264\n",
      "2017-11-11 07:03:51: Loss at step 3815: 0.03685013949871063\n",
      "2017-11-11 07:03:53: Loss at step 3816: 0.03691145032644272\n",
      "2017-11-11 07:03:55: Loss at step 3817: 0.036751486361026764\n",
      "2017-11-11 07:03:57: Loss at step 3818: 0.03675545006990433\n",
      "2017-11-11 07:03:59: Loss at step 3819: 0.0368083119392395\n",
      "2017-11-11 07:04:01: Loss at step 3820: 0.03676183894276619\n",
      "2017-11-11 07:04:03: Loss at step 3821: 0.036833375692367554\n",
      "2017-11-11 07:04:05: Loss at step 3822: 0.03683168813586235\n",
      "2017-11-11 07:04:08: Loss at step 3823: 0.036815643310546875\n",
      "2017-11-11 07:04:10: Loss at step 3824: 0.03682830557227135\n",
      "2017-11-11 07:04:12: Loss at step 3825: 0.03682538494467735\n",
      "2017-11-11 07:04:14: Loss at step 3826: 0.0367346927523613\n",
      "2017-11-11 07:04:16: Loss at step 3827: 0.03681837022304535\n",
      "2017-11-11 07:04:18: Loss at step 3828: 0.03680496662855148\n",
      "2017-11-11 07:04:20: Loss at step 3829: 0.03680580481886864\n",
      "2017-11-11 07:04:22: Loss at step 3830: 0.036847420036792755\n",
      "2017-11-11 07:04:25: Loss at step 3831: 0.036819685250520706\n",
      "2017-11-11 07:04:27: Loss at step 3832: 0.03675204515457153\n",
      "2017-11-11 07:04:29: Loss at step 3833: 0.03679654374718666\n",
      "2017-11-11 07:04:31: Loss at step 3834: 0.03684363514184952\n",
      "2017-11-11 07:04:33: Loss at step 3835: 0.036851320415735245\n",
      "2017-11-11 07:04:35: Loss at step 3836: 0.036835767328739166\n",
      "2017-11-11 07:04:38: Loss at step 3837: 0.03684935346245766\n",
      "2017-11-11 07:04:40: Loss at step 3838: 0.03677551448345184\n",
      "2017-11-11 07:04:42: Loss at step 3839: 0.03683735057711601\n",
      "2017-11-11 07:04:44: Loss at step 3840: 0.03673744201660156\n",
      "2017-11-11 07:04:46: Loss at step 3841: 0.03675326332449913\n",
      "2017-11-11 07:04:48: Loss at step 3842: 0.03679342195391655\n",
      "2017-11-11 07:04:50: Loss at step 3843: 0.03687895089387894\n",
      "2017-11-11 07:04:52: Loss at step 3844: 0.036829978227615356\n",
      "2017-11-11 07:04:55: Loss at step 3845: 0.03690018877387047\n",
      "2017-11-11 07:04:57: Loss at step 3846: 0.03692419081926346\n",
      "2017-11-11 07:04:59: Loss at step 3847: 0.03690529242157936\n",
      "2017-11-11 07:05:01: Loss at step 3848: 0.036860015243291855\n",
      "2017-11-11 07:05:03: Loss at step 3849: 0.036854419857263565\n",
      "2017-11-11 07:05:05: Loss at step 3850: 0.036882854998111725\n",
      "2017-11-11 07:05:07: Loss at step 3851: 0.03684476017951965\n",
      "2017-11-11 07:05:09: Loss at step 3852: 0.036896832287311554\n",
      "2017-11-11 07:05:12: Loss at step 3853: 0.03682880848646164\n",
      "2017-11-11 07:05:14: Loss at step 3854: 0.03680871054530144\n",
      "2017-11-11 07:05:16: Loss at step 3855: 0.036801986396312714\n",
      "2017-11-11 07:05:18: Loss at step 3856: 0.036837223917245865\n",
      "2017-11-11 07:05:20: Loss at step 3857: 0.03669489175081253\n",
      "2017-11-11 07:05:22: Loss at step 3858: 0.036842815577983856\n",
      "2017-11-11 07:05:24: Loss at step 3859: 0.03682270273566246\n",
      "2017-11-11 07:05:26: Loss at step 3860: 0.03684169054031372\n",
      "2017-11-11 07:05:29: Loss at step 3861: 0.03687991946935654\n",
      "2017-11-11 07:05:31: Loss at step 3862: 0.036894574761390686\n",
      "2017-11-11 07:05:33: Loss at step 3863: 0.03689766302704811\n",
      "2017-11-11 07:05:35: Loss at step 3864: 0.03684540465474129\n",
      "2017-11-11 07:05:37: Loss at step 3865: 0.03687746822834015\n",
      "2017-11-11 07:05:39: Loss at step 3866: 0.03680811822414398\n",
      "2017-11-11 07:05:41: Loss at step 3867: 0.03681332245469093\n",
      "2017-11-11 07:05:43: Loss at step 3868: 0.03681481257081032\n",
      "2017-11-11 07:05:46: Loss at step 3869: 0.036762818694114685\n",
      "2017-11-11 07:05:48: Loss at step 3870: 0.03683004528284073\n",
      "2017-11-11 07:05:50: Loss at step 3871: 0.0368061400949955\n",
      "2017-11-11 07:05:52: Loss at step 3872: 0.03680895268917084\n",
      "2017-11-11 07:05:54: Loss at step 3873: 0.03685733303427696\n",
      "2017-11-11 07:05:56: Loss at step 3874: 0.036893267184495926\n",
      "2017-11-11 07:05:58: Loss at step 3875: 0.036798518151044846\n",
      "2017-11-11 07:06:00: Loss at step 3876: 0.036850083619356155\n",
      "2017-11-11 07:06:03: Loss at step 3877: 0.036655064672231674\n",
      "2017-11-11 07:06:05: Loss at step 3878: 0.03675302863121033\n",
      "2017-11-11 07:06:07: Loss at step 3879: 0.03675054758787155\n",
      "2017-11-11 07:06:09: Loss at step 3880: 0.03674163669347763\n",
      "2017-11-11 07:06:11: Loss at step 3881: 0.036749809980392456\n",
      "2017-11-11 07:06:13: Loss at step 3882: 0.03675905615091324\n",
      "2017-11-11 07:06:15: Loss at step 3883: 0.03682438284158707\n",
      "2017-11-11 07:06:17: Loss at step 3884: 0.036780402064323425\n",
      "2017-11-11 07:06:20: Loss at step 3885: 0.0367867536842823\n",
      "2017-11-11 07:06:22: Loss at step 3886: 0.0367724746465683\n",
      "2017-11-11 07:06:24: Loss at step 3887: 0.03683864697813988\n",
      "2017-11-11 07:06:26: Loss at step 3888: 0.0367884524166584\n",
      "2017-11-11 07:06:28: Loss at step 3889: 0.03675420209765434\n",
      "2017-11-11 07:06:30: Loss at step 3890: 0.03680948540568352\n",
      "2017-11-11 07:06:33: Loss at step 3891: 0.03676687180995941\n",
      "2017-11-11 07:06:35: Loss at step 3892: 0.03675968199968338\n",
      "2017-11-11 07:06:37: Loss at step 3893: 0.03670431673526764\n",
      "2017-11-11 07:06:39: Loss at step 3894: 0.03672102093696594\n",
      "2017-11-11 07:06:41: Loss at step 3895: 0.03668273240327835\n",
      "2017-11-11 07:06:43: Loss at step 3896: 0.03676975518465042\n",
      "2017-11-11 07:06:45: Loss at step 3897: 0.03684760630130768\n",
      "2017-11-11 07:06:48: Loss at step 3898: 0.0368434377014637\n",
      "2017-11-11 07:06:50: Loss at step 3899: 0.03674653172492981\n",
      "2017-11-11 07:06:52: Loss at step 3900: 0.0368913896381855\n",
      "2017-11-11 07:06:54: Loss at step 3901: 0.036786891520023346\n",
      "2017-11-11 07:06:56: Loss at step 3902: 0.03682757541537285\n",
      "2017-11-11 07:06:58: Loss at step 3903: 0.036694932729005814\n",
      "2017-11-11 07:07:00: Loss at step 3904: 0.036782506853342056\n",
      "2017-11-11 07:07:02: Loss at step 3905: 0.036830317229032516\n",
      "2017-11-11 07:07:05: Loss at step 3906: 0.036942120641469955\n",
      "2017-11-11 07:07:07: Loss at step 3907: 0.036782488226890564\n",
      "2017-11-11 07:07:09: Loss at step 3908: 0.03678920120000839\n",
      "2017-11-11 07:07:11: Loss at step 3909: 0.03682655468583107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:07:13: Loss at step 3910: 0.03674496337771416\n",
      "2017-11-11 07:07:15: Loss at step 3911: 0.0367145799100399\n",
      "2017-11-11 07:07:17: Loss at step 3912: 0.03667007014155388\n",
      "2017-11-11 07:07:20: Loss at step 3913: 0.03679288178682327\n",
      "2017-11-11 07:07:22: Loss at step 3914: 0.03669046610593796\n",
      "2017-11-11 07:07:24: Loss at step 3915: 0.03671008348464966\n",
      "2017-11-11 07:07:26: Loss at step 3916: 0.03673507273197174\n",
      "2017-11-11 07:07:28: Loss at step 3917: 0.03674417734146118\n",
      "2017-11-11 07:07:30: Loss at step 3918: 0.03686463460326195\n",
      "2017-11-11 07:07:32: Loss at step 3919: 0.036740049719810486\n",
      "2017-11-11 07:07:35: Loss at step 3920: 0.036817535758018494\n",
      "2017-11-11 07:07:37: Loss at step 3921: 0.03676178678870201\n",
      "2017-11-11 07:07:39: Loss at step 3922: 0.03680400177836418\n",
      "2017-11-11 07:07:41: Loss at step 3923: 0.03674845024943352\n",
      "2017-11-11 07:07:43: Loss at step 3924: 0.03675954043865204\n",
      "2017-11-11 07:07:45: Loss at step 3925: 0.03685895726084709\n",
      "2017-11-11 07:07:48: Loss at step 3926: 0.0367698036134243\n",
      "2017-11-11 07:07:50: Loss at step 3927: 0.03675774484872818\n",
      "2017-11-11 07:07:52: Loss at step 3928: 0.036798954010009766\n",
      "2017-11-11 07:07:54: Loss at step 3929: 0.03676392138004303\n",
      "2017-11-11 07:07:56: Loss at step 3930: 0.036811329424381256\n",
      "2017-11-11 07:07:58: Loss at step 3931: 0.036795474588871\n",
      "2017-11-11 07:08:00: Loss at step 3932: 0.036669500172138214\n",
      "2017-11-11 07:08:02: Loss at step 3933: 0.03686482459306717\n",
      "2017-11-11 07:08:05: Loss at step 3934: 0.036953724920749664\n",
      "2017-11-11 07:08:07: Loss at step 3935: 0.036697834730148315\n",
      "2017-11-11 07:08:09: Loss at step 3936: 0.036736227571964264\n",
      "2017-11-11 07:08:11: Loss at step 3937: 0.03687596321105957\n",
      "2017-11-11 07:08:13: Loss at step 3938: 0.03685412555932999\n",
      "2017-11-11 07:08:15: Loss at step 3939: 0.03685363009572029\n",
      "2017-11-11 07:08:17: Loss at step 3940: 0.03685123473405838\n",
      "2017-11-11 07:08:20: Loss at step 3941: 0.03683457896113396\n",
      "2017-11-11 07:08:22: Loss at step 3942: 0.03681618347764015\n",
      "2017-11-11 07:08:24: Loss at step 3943: 0.03684509918093681\n",
      "2017-11-11 07:08:26: Loss at step 3944: 0.036812782287597656\n",
      "2017-11-11 07:08:28: Loss at step 3945: 0.036736756563186646\n",
      "2017-11-11 07:08:30: Loss at step 3946: 0.0367678664624691\n",
      "2017-11-11 07:08:32: Loss at step 3947: 0.03676407411694527\n",
      "2017-11-11 07:08:35: Loss at step 3948: 0.03669445589184761\n",
      "2017-11-11 07:08:37: Loss at step 3949: 0.03680621460080147\n",
      "2017-11-11 07:08:39: Loss at step 3950: 0.03677866980433464\n",
      "2017-11-11 07:08:41: Loss at step 3951: 0.03675151988863945\n",
      "2017-11-11 07:08:43: Loss at step 3952: 0.036767832934856415\n",
      "2017-11-11 07:08:45: Loss at step 3953: 0.03675466775894165\n",
      "2017-11-11 07:08:47: Loss at step 3954: 0.036666661500930786\n",
      "2017-11-11 07:08:50: Loss at step 3955: 0.03671208396553993\n",
      "2017-11-11 07:08:52: Loss at step 3956: 0.03679024055600166\n",
      "2017-11-11 07:08:54: Loss at step 3957: 0.03680489957332611\n",
      "2017-11-11 07:08:56: Loss at step 3958: 0.036844585090875626\n",
      "2017-11-11 07:08:58: Loss at step 3959: 0.03676486760377884\n",
      "2017-11-11 07:09:00: Loss at step 3960: 0.03678317740559578\n",
      "2017-11-11 07:09:02: Loss at step 3961: 0.03688114508986473\n",
      "2017-11-11 07:09:05: Loss at step 3962: 0.03682801499962807\n",
      "2017-11-11 07:09:07: Loss at step 3963: 0.036828525364398956\n",
      "2017-11-11 07:09:09: Loss at step 3964: 0.036748554557561874\n",
      "2017-11-11 07:09:11: Loss at step 3965: 0.0368366539478302\n",
      "2017-11-11 07:09:13: Loss at step 3966: 0.036713287234306335\n",
      "2017-11-11 07:09:15: Loss at step 3967: 0.0366344191133976\n",
      "2017-11-11 07:09:17: Loss at step 3968: 0.03674381226301193\n",
      "2017-11-11 07:09:20: Loss at step 3969: 0.036792103201150894\n",
      "2017-11-11 07:09:22: Loss at step 3970: 0.036773681640625\n",
      "2017-11-11 07:09:24: Loss at step 3971: 0.03686189278960228\n",
      "2017-11-11 07:09:26: Loss at step 3972: 0.03679222613573074\n",
      "2017-11-11 07:09:28: Loss at step 3973: 0.03683452308177948\n",
      "2017-11-11 07:09:30: Loss at step 3974: 0.036804359406232834\n",
      "2017-11-11 07:09:32: Loss at step 3975: 0.036800190806388855\n",
      "2017-11-11 07:09:35: Loss at step 3976: 0.036737192422151566\n",
      "2017-11-11 07:09:37: Loss at step 3977: 0.03675500676035881\n",
      "2017-11-11 07:09:39: Loss at step 3978: 0.036855101585388184\n",
      "2017-11-11 07:09:41: Loss at step 3979: 0.036849942058324814\n",
      "2017-11-11 07:09:43: Loss at step 3980: 0.03676470369100571\n",
      "2017-11-11 07:09:45: Loss at step 3981: 0.036752186715602875\n",
      "2017-11-11 07:09:47: Loss at step 3982: 0.03675844147801399\n",
      "2017-11-11 07:09:50: Loss at step 3983: 0.036779388785362244\n",
      "2017-11-11 07:09:52: Loss at step 3984: 0.03679630905389786\n",
      "2017-11-11 07:09:54: Loss at step 3985: 0.036816444247961044\n",
      "2017-11-11 07:09:56: Loss at step 3986: 0.03674789518117905\n",
      "2017-11-11 07:09:58: Loss at step 3987: 0.03684290871024132\n",
      "2017-11-11 07:10:00: Loss at step 3988: 0.03690009191632271\n",
      "2017-11-11 07:10:02: Loss at step 3989: 0.036770109087228775\n",
      "2017-11-11 07:10:04: Loss at step 3990: 0.036739643663167953\n",
      "2017-11-11 07:10:07: Loss at step 3991: 0.03679459169507027\n",
      "2017-11-11 07:10:09: Loss at step 3992: 0.03675524890422821\n",
      "2017-11-11 07:10:11: Loss at step 3993: 0.03677544370293617\n",
      "2017-11-11 07:10:13: Loss at step 3994: 0.03682759404182434\n",
      "2017-11-11 07:10:15: Loss at step 3995: 0.036751117557287216\n",
      "2017-11-11 07:10:17: Loss at step 3996: 0.0367596261203289\n",
      "2017-11-11 07:10:19: Loss at step 3997: 0.0367557518184185\n",
      "2017-11-11 07:10:22: Loss at step 3998: 0.036679938435554504\n",
      "2017-11-11 07:10:24: Loss at step 3999: 0.036798276007175446\n",
      "2017-11-11 07:10:26: Loss at step 4000: 0.036832913756370544\n",
      "2017-11-11 07:10:28: Loss at step 4001: 0.036727748811244965\n",
      "2017-11-11 07:10:30: Loss at step 4002: 0.0368274450302124\n",
      "2017-11-11 07:10:32: Loss at step 4003: 0.036809761077165604\n",
      "2017-11-11 07:10:35: Loss at step 4004: 0.036858465522527695\n",
      "2017-11-11 07:10:37: Loss at step 4005: 0.036861542612314224\n",
      "2017-11-11 07:10:39: Loss at step 4006: 0.036904048174619675\n",
      "2017-11-11 07:10:41: Loss at step 4007: 0.03674283251166344\n",
      "2017-11-11 07:10:43: Loss at step 4008: 0.03678514063358307\n",
      "2017-11-11 07:10:45: Loss at step 4009: 0.036718375980854034\n",
      "2017-11-11 07:10:47: Loss at step 4010: 0.036790844053030014\n",
      "2017-11-11 07:10:49: Loss at step 4011: 0.03675111010670662\n",
      "2017-11-11 07:10:52: Loss at step 4012: 0.03683629259467125\n",
      "2017-11-11 07:10:54: Loss at step 4013: 0.03686215728521347\n",
      "2017-11-11 07:10:56: Loss at step 4014: 0.03689039871096611\n",
      "2017-11-11 07:10:58: Loss at step 4015: 0.03680645301938057\n",
      "2017-11-11 07:11:00: Loss at step 4016: 0.03674377501010895\n",
      "2017-11-11 07:11:02: Loss at step 4017: 0.03683572635054588\n",
      "2017-11-11 07:11:04: Loss at step 4018: 0.03685656562447548\n",
      "2017-11-11 07:11:07: Loss at step 4019: 0.03687683865427971\n",
      "2017-11-11 07:11:09: Loss at step 4020: 0.0368800163269043\n",
      "2017-11-11 07:11:11: Loss at step 4021: 0.03682968020439148\n",
      "2017-11-11 07:11:13: Loss at step 4022: 0.03666846826672554\n",
      "2017-11-11 07:11:15: Loss at step 4023: 0.03681052848696709\n",
      "2017-11-11 07:11:17: Loss at step 4024: 0.036810748279094696\n",
      "2017-11-11 07:11:19: Loss at step 4025: 0.036772698163986206\n",
      "2017-11-11 07:11:21: Loss at step 4026: 0.03678852692246437\n",
      "2017-11-11 07:11:24: Loss at step 4027: 0.03683455288410187\n",
      "2017-11-11 07:11:26: Loss at step 4028: 0.03681294992566109\n",
      "2017-11-11 07:11:28: Loss at step 4029: 0.036855023354291916\n",
      "2017-11-11 07:11:30: Loss at step 4030: 0.03671154007315636\n",
      "2017-11-11 07:11:32: Loss at step 4031: 0.03682156279683113\n",
      "2017-11-11 07:11:34: Loss at step 4032: 0.03680085763335228\n",
      "2017-11-11 07:11:36: Loss at step 4033: 0.03676605597138405\n",
      "2017-11-11 07:11:39: Loss at step 4034: 0.03680811822414398\n",
      "2017-11-11 07:11:41: Loss at step 4035: 0.036791715770959854\n",
      "2017-11-11 07:11:43: Loss at step 4036: 0.03684061020612717\n",
      "2017-11-11 07:11:45: Loss at step 4037: 0.036771174520254135\n",
      "2017-11-11 07:11:47: Loss at step 4038: 0.036767926067113876\n",
      "2017-11-11 07:11:49: Loss at step 4039: 0.03684976324439049\n",
      "2017-11-11 07:11:51: Loss at step 4040: 0.03683416172862053\n",
      "2017-11-11 07:11:53: Loss at step 4041: 0.03679834306240082\n",
      "2017-11-11 07:11:56: Loss at step 4042: 0.03684930503368378\n",
      "2017-11-11 07:11:58: Loss at step 4043: 0.03679532930254936\n",
      "2017-11-11 07:12:00: Loss at step 4044: 0.036823906004428864\n",
      "2017-11-11 07:12:02: Loss at step 4045: 0.036807380616664886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:12:04: Loss at step 4046: 0.03684917092323303\n",
      "2017-11-11 07:12:06: Loss at step 4047: 0.03681929409503937\n",
      "2017-11-11 07:12:08: Loss at step 4048: 0.03674627095460892\n",
      "2017-11-11 07:12:10: Loss at step 4049: 0.0369245745241642\n",
      "2017-11-11 07:12:13: Loss at step 4050: 0.03674446418881416\n",
      "2017-11-11 07:12:15: Loss at step 4051: 0.036795489490032196\n",
      "2017-11-11 07:12:17: Loss at step 4052: 0.03675193339586258\n",
      "2017-11-11 07:12:19: Loss at step 4053: 0.03674726560711861\n",
      "2017-11-11 07:12:21: Loss at step 4054: 0.03670770302414894\n",
      "2017-11-11 07:12:23: Loss at step 4055: 0.03677024692296982\n",
      "2017-11-11 07:12:25: Loss at step 4056: 0.03679041564464569\n",
      "2017-11-11 07:12:28: Loss at step 4057: 0.03675834834575653\n",
      "2017-11-11 07:12:30: Loss at step 4058: 0.03678632155060768\n",
      "2017-11-11 07:12:32: Loss at step 4059: 0.03678560629487038\n",
      "2017-11-11 07:12:34: Loss at step 4060: 0.036765944212675095\n",
      "2017-11-11 07:12:36: Loss at step 4061: 0.03680328279733658\n",
      "2017-11-11 07:12:38: Loss at step 4062: 0.03679991886019707\n",
      "2017-11-11 07:12:40: Loss at step 4063: 0.03682092949748039\n",
      "2017-11-11 07:12:43: Loss at step 4064: 0.03674548491835594\n",
      "2017-11-11 07:12:45: Loss at step 4065: 0.036814864724874496\n",
      "2017-11-11 07:12:47: Loss at step 4066: 0.036829590797424316\n",
      "2017-11-11 07:12:49: Loss at step 4067: 0.03680189326405525\n",
      "2017-11-11 07:12:51: Loss at step 4068: 0.036761701107025146\n",
      "2017-11-11 07:12:53: Loss at step 4069: 0.036802541464567184\n",
      "2017-11-11 07:12:55: Loss at step 4070: 0.03675254061818123\n",
      "2017-11-11 07:12:58: Loss at step 4071: 0.036773037165403366\n",
      "2017-11-11 07:13:00: Loss at step 4072: 0.03681105747818947\n",
      "2017-11-11 07:13:02: Loss at step 4073: 0.036837074905633926\n",
      "2017-11-11 07:13:04: Loss at step 4074: 0.036736976355314255\n",
      "2017-11-11 07:13:06: Loss at step 4075: 0.03680561110377312\n",
      "2017-11-11 07:13:08: Loss at step 4076: 0.03680434450507164\n",
      "2017-11-11 07:13:10: Loss at step 4077: 0.03688172996044159\n",
      "2017-11-11 07:13:12: Loss at step 4078: 0.03680279850959778\n",
      "2017-11-11 07:13:15: Loss at step 4079: 0.03685881197452545\n",
      "2017-11-11 07:13:17: Loss at step 4080: 0.03672601655125618\n",
      "2017-11-11 07:13:19: Loss at step 4081: 0.03686393424868584\n",
      "2017-11-11 07:13:21: Loss at step 4082: 0.03674948588013649\n",
      "2017-11-11 07:13:23: Loss at step 4083: 0.03669646754860878\n",
      "2017-11-11 07:13:25: Loss at step 4084: 0.03670685738325119\n",
      "2017-11-11 07:13:27: Loss at step 4085: 0.03679859638214111\n",
      "2017-11-11 07:13:30: Loss at step 4086: 0.036860156804323196\n",
      "2017-11-11 07:13:32: Loss at step 4087: 0.036884911358356476\n",
      "2017-11-11 07:13:34: Loss at step 4088: 0.03684764355421066\n",
      "2017-11-11 07:13:36: Loss at step 4089: 0.036863330751657486\n",
      "2017-11-11 07:13:38: Loss at step 4090: 0.03678944706916809\n",
      "2017-11-11 07:13:40: Loss at step 4091: 0.0368204340338707\n",
      "2017-11-11 07:13:42: Loss at step 4092: 0.03684708848595619\n",
      "2017-11-11 07:13:44: Loss at step 4093: 0.03674875944852829\n",
      "2017-11-11 07:13:47: Loss at step 4094: 0.03689023479819298\n",
      "2017-11-11 07:13:49: Loss at step 4095: 0.03683142364025116\n",
      "2017-11-11 07:13:51: Loss at step 4096: 0.03680034354329109\n",
      "2017-11-11 07:13:53: Loss at step 4097: 0.03684860095381737\n",
      "2017-11-11 07:13:55: Loss at step 4098: 0.0367773212492466\n",
      "2017-11-11 07:13:58: Loss at step 4099: 0.03685922175645828\n",
      "2017-11-11 07:14:00: Loss at step 4100: 0.03685533255338669\n",
      "2017-11-11 07:14:02: Loss at step 4101: 0.03676312789320946\n",
      "2017-11-11 07:14:04: Loss at step 4102: 0.036848414689302444\n",
      "2017-11-11 07:14:06: Loss at step 4103: 0.03679271414875984\n",
      "2017-11-11 07:14:08: Loss at step 4104: 0.036836933344602585\n",
      "2017-11-11 07:14:10: Loss at step 4105: 0.036782000213861465\n",
      "2017-11-11 07:14:12: Loss at step 4106: 0.03681521490216255\n",
      "2017-11-11 07:14:15: Loss at step 4107: 0.036741483956575394\n",
      "2017-11-11 07:14:17: Loss at step 4108: 0.03687518462538719\n",
      "2017-11-11 07:14:19: Loss at step 4109: 0.0368032343685627\n",
      "2017-11-11 07:14:21: Loss at step 4110: 0.03683330863714218\n",
      "2017-11-11 07:14:23: Loss at step 4111: 0.03686662018299103\n",
      "2017-11-11 07:14:25: Loss at step 4112: 0.03679365664720535\n",
      "2017-11-11 07:14:27: Loss at step 4113: 0.03680006042122841\n",
      "2017-11-11 07:14:30: Loss at step 4114: 0.03679082170128822\n",
      "2017-11-11 07:14:32: Loss at step 4115: 0.03672727569937706\n",
      "2017-11-11 07:14:34: Loss at step 4116: 0.03676871210336685\n",
      "2017-11-11 07:14:36: Loss at step 4117: 0.03682525455951691\n",
      "2017-11-11 07:14:38: Loss at step 4118: 0.036789670586586\n",
      "2017-11-11 07:14:40: Loss at step 4119: 0.036777906119823456\n",
      "2017-11-11 07:14:42: Loss at step 4120: 0.0367412231862545\n",
      "2017-11-11 07:14:45: Loss at step 4121: 0.036812637001276016\n",
      "2017-11-11 07:14:47: Loss at step 4122: 0.03676396608352661\n",
      "2017-11-11 07:14:49: Loss at step 4123: 0.036801643669605255\n",
      "2017-11-11 07:14:51: Loss at step 4124: 0.03683726117014885\n",
      "2017-11-11 07:14:53: Loss at step 4125: 0.03689087554812431\n",
      "2017-11-11 07:14:55: Loss at step 4126: 0.03687811642885208\n",
      "2017-11-11 07:14:57: Loss at step 4127: 0.03681567311286926\n",
      "2017-11-11 07:15:00: Loss at step 4128: 0.03694390878081322\n",
      "2017-11-11 07:15:02: Loss at step 4129: 0.03682670369744301\n",
      "2017-11-11 07:15:04: Loss at step 4130: 0.036847759038209915\n",
      "2017-11-11 07:15:06: Loss at step 4131: 0.036864716559648514\n",
      "2017-11-11 07:15:08: Loss at step 4132: 0.03679042309522629\n",
      "2017-11-11 07:15:10: Loss at step 4133: 0.03681762143969536\n",
      "2017-11-11 07:15:12: Loss at step 4134: 0.036858897656202316\n",
      "2017-11-11 07:15:14: Loss at step 4135: 0.03683736175298691\n",
      "2017-11-11 07:15:17: Loss at step 4136: 0.03686203062534332\n",
      "2017-11-11 07:15:19: Loss at step 4137: 0.036851461976766586\n",
      "2017-11-11 07:15:21: Loss at step 4138: 0.0368083193898201\n",
      "2017-11-11 07:15:23: Loss at step 4139: 0.03669439256191254\n",
      "2017-11-11 07:15:25: Loss at step 4140: 0.036796122789382935\n",
      "2017-11-11 07:15:27: Loss at step 4141: 0.03686833009123802\n",
      "2017-11-11 07:15:29: Loss at step 4142: 0.03683258220553398\n",
      "2017-11-11 07:15:31: Loss at step 4143: 0.03675825893878937\n",
      "2017-11-11 07:15:34: Loss at step 4144: 0.036824822425842285\n",
      "2017-11-11 07:15:36: Loss at step 4145: 0.03680801019072533\n",
      "2017-11-11 07:15:38: Loss at step 4146: 0.03678165748715401\n",
      "2017-11-11 07:15:40: Loss at step 4147: 0.03681601211428642\n",
      "2017-11-11 07:15:42: Loss at step 4148: 0.03680073097348213\n",
      "2017-11-11 07:15:44: Loss at step 4149: 0.03681498393416405\n",
      "2017-11-11 07:15:46: Loss at step 4150: 0.03674742579460144\n",
      "2017-11-11 07:15:49: Loss at step 4151: 0.036741748452186584\n",
      "2017-11-11 07:15:51: Loss at step 4152: 0.036866120994091034\n",
      "2017-11-11 07:15:53: Loss at step 4153: 0.03674855828285217\n",
      "2017-11-11 07:15:55: Loss at step 4154: 0.036748871207237244\n",
      "2017-11-11 07:15:57: Loss at step 4155: 0.03677397966384888\n",
      "2017-11-11 07:15:59: Loss at step 4156: 0.03690635412931442\n",
      "2017-11-11 07:16:01: Loss at step 4157: 0.03684096783399582\n",
      "2017-11-11 07:16:04: Loss at step 4158: 0.03674374520778656\n",
      "2017-11-11 07:16:06: Loss at step 4159: 0.03682101145386696\n",
      "2017-11-11 07:16:08: Loss at step 4160: 0.036776117980480194\n",
      "2017-11-11 07:16:10: Loss at step 4161: 0.036833230406045914\n",
      "2017-11-11 07:16:12: Loss at step 4162: 0.03682659938931465\n",
      "2017-11-11 07:16:14: Loss at step 4163: 0.03681989014148712\n",
      "2017-11-11 07:16:16: Loss at step 4164: 0.03674189746379852\n",
      "2017-11-11 07:16:19: Loss at step 4165: 0.03682241216301918\n",
      "2017-11-11 07:16:21: Loss at step 4166: 0.03687006235122681\n",
      "2017-11-11 07:16:23: Loss at step 4167: 0.03682290017604828\n",
      "2017-11-11 07:16:25: Loss at step 4168: 0.0368228405714035\n",
      "2017-11-11 07:16:27: Loss at step 4169: 0.03673340007662773\n",
      "2017-11-11 07:16:29: Loss at step 4170: 0.036853667348623276\n",
      "2017-11-11 07:16:32: Loss at step 4171: 0.036793265491724014\n",
      "2017-11-11 07:16:34: Loss at step 4172: 0.036838043481111526\n",
      "2017-11-11 07:16:36: Loss at step 4173: 0.03673218563199043\n",
      "2017-11-11 07:16:38: Loss at step 4174: 0.03686158359050751\n",
      "2017-11-11 07:16:40: Loss at step 4175: 0.03678127005696297\n",
      "2017-11-11 07:16:42: Loss at step 4176: 0.03676733747124672\n",
      "2017-11-11 07:16:44: Loss at step 4177: 0.03679579496383667\n",
      "2017-11-11 07:16:46: Loss at step 4178: 0.03687925636768341\n",
      "2017-11-11 07:16:49: Loss at step 4179: 0.03683885931968689\n",
      "2017-11-11 07:16:51: Loss at step 4180: 0.03677356243133545\n",
      "2017-11-11 07:16:53: Loss at step 4181: 0.03691842779517174\n",
      "2017-11-11 07:16:55: Loss at step 4182: 0.03680163249373436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:16:57: Loss at step 4183: 0.03676663711667061\n",
      "2017-11-11 07:16:59: Loss at step 4184: 0.036852363497018814\n",
      "2017-11-11 07:17:01: Loss at step 4185: 0.03685683012008667\n",
      "2017-11-11 07:17:04: Loss at step 4186: 0.0368259996175766\n",
      "2017-11-11 07:17:06: Loss at step 4187: 0.03690797835588455\n",
      "2017-11-11 07:17:08: Loss at step 4188: 0.03691825270652771\n",
      "2017-11-11 07:17:10: Loss at step 4189: 0.036797668784856796\n",
      "2017-11-11 07:17:12: Loss at step 4190: 0.03681565448641777\n",
      "2017-11-11 07:17:14: Loss at step 4191: 0.036838188767433167\n",
      "2017-11-11 07:17:16: Loss at step 4192: 0.03677576780319214\n",
      "2017-11-11 07:17:18: Loss at step 4193: 0.03683064132928848\n",
      "2017-11-11 07:17:21: Loss at step 4194: 0.036912817507982254\n",
      "2017-11-11 07:17:23: Loss at step 4195: 0.03689521923661232\n",
      "2017-11-11 07:17:25: Loss at step 4196: 0.03682512789964676\n",
      "2017-11-11 07:17:27: Loss at step 4197: 0.03688908740878105\n",
      "2017-11-11 07:17:29: Loss at step 4198: 0.03683195635676384\n",
      "2017-11-11 07:17:31: Loss at step 4199: 0.03686053305864334\n",
      "2017-11-11 07:17:34: Loss at step 4200: 0.03694179654121399\n",
      "2017-11-11 07:17:36: Loss at step 4201: 0.03687143698334694\n",
      "2017-11-11 07:17:38: Loss at step 4202: 0.03679376095533371\n",
      "2017-11-11 07:17:40: Loss at step 4203: 0.03670787438750267\n",
      "2017-11-11 07:17:42: Loss at step 4204: 0.03689839318394661\n",
      "2017-11-11 07:17:44: Loss at step 4205: 0.03683396428823471\n",
      "2017-11-11 07:17:46: Loss at step 4206: 0.036826372146606445\n",
      "2017-11-11 07:17:49: Loss at step 4207: 0.0369204543530941\n",
      "2017-11-11 07:17:51: Loss at step 4208: 0.03691231831908226\n",
      "2017-11-11 07:17:53: Loss at step 4209: 0.03693731874227524\n",
      "2017-11-11 07:17:55: Loss at step 4210: 0.036795031279325485\n",
      "2017-11-11 07:17:57: Loss at step 4211: 0.03682461008429527\n",
      "2017-11-11 07:17:59: Loss at step 4212: 0.0368572436273098\n",
      "2017-11-11 07:18:01: Loss at step 4213: 0.036819152534008026\n",
      "2017-11-11 07:18:03: Loss at step 4214: 0.036751359701156616\n",
      "2017-11-11 07:18:06: Loss at step 4215: 0.036876678466796875\n",
      "2017-11-11 07:18:08: Loss at step 4216: 0.036866456270217896\n",
      "2017-11-11 07:18:10: Loss at step 4217: 0.03675253689289093\n",
      "2017-11-11 07:18:12: Loss at step 4218: 0.036740269511938095\n",
      "2017-11-11 07:18:14: Loss at step 4219: 0.03681551665067673\n",
      "2017-11-11 07:18:16: Loss at step 4220: 0.036775920540094376\n",
      "2017-11-11 07:18:18: Loss at step 4221: 0.03678615763783455\n",
      "2017-11-11 07:18:21: Loss at step 4222: 0.03682399541139603\n",
      "2017-11-11 07:18:23: Loss at step 4223: 0.03682522475719452\n",
      "2017-11-11 07:18:25: Loss at step 4224: 0.03680655360221863\n",
      "2017-11-11 07:18:27: Loss at step 4225: 0.03679550811648369\n",
      "2017-11-11 07:18:29: Loss at step 4226: 0.03686816990375519\n",
      "2017-11-11 07:18:31: Loss at step 4227: 0.036746785044670105\n",
      "2017-11-11 07:18:34: Loss at step 4228: 0.0367622897028923\n",
      "2017-11-11 07:18:36: Loss at step 4229: 0.03685832396149635\n",
      "2017-11-11 07:18:38: Loss at step 4230: 0.036793891340494156\n",
      "2017-11-11 07:18:40: Loss at step 4231: 0.03674933314323425\n",
      "2017-11-11 07:18:42: Loss at step 4232: 0.03678488731384277\n",
      "2017-11-11 07:18:44: Loss at step 4233: 0.03677218779921532\n",
      "2017-11-11 07:18:46: Loss at step 4234: 0.03682338073849678\n",
      "2017-11-11 07:18:48: Loss at step 4235: 0.03676773980259895\n",
      "2017-11-11 07:18:51: Loss at step 4236: 0.03693268820643425\n",
      "2017-11-11 07:18:53: Loss at step 4237: 0.03679002821445465\n",
      "2017-11-11 07:18:55: Loss at step 4238: 0.036754198372364044\n",
      "2017-11-11 07:18:57: Loss at step 4239: 0.036838091909885406\n",
      "2017-11-11 07:18:59: Loss at step 4240: 0.03679853677749634\n",
      "2017-11-11 07:19:01: Loss at step 4241: 0.03683317080140114\n",
      "2017-11-11 07:19:03: Loss at step 4242: 0.036803849041461945\n",
      "2017-11-11 07:19:06: Loss at step 4243: 0.036738473922014236\n",
      "2017-11-11 07:19:08: Loss at step 4244: 0.03669033572077751\n",
      "2017-11-11 07:19:10: Loss at step 4245: 0.036908943206071854\n",
      "2017-11-11 07:19:12: Loss at step 4246: 0.036793846637010574\n",
      "2017-11-11 07:19:14: Loss at step 4247: 0.036881834268569946\n",
      "2017-11-11 07:19:16: Loss at step 4248: 0.036909643560647964\n",
      "2017-11-11 07:19:18: Loss at step 4249: 0.036759164184331894\n",
      "2017-11-11 07:19:20: Loss at step 4250: 0.03675210103392601\n",
      "2017-11-11 07:19:23: Loss at step 4251: 0.03684687986969948\n",
      "2017-11-11 07:19:25: Loss at step 4252: 0.03690161928534508\n",
      "2017-11-11 07:19:27: Loss at step 4253: 0.036767564713954926\n",
      "2017-11-11 07:19:29: Loss at step 4254: 0.036773569881916046\n",
      "2017-11-11 07:19:31: Loss at step 4255: 0.036763500422239304\n",
      "2017-11-11 07:19:33: Loss at step 4256: 0.036786820739507675\n",
      "2017-11-11 07:19:35: Loss at step 4257: 0.036803651601076126\n",
      "2017-11-11 07:19:38: Loss at step 4258: 0.03681803122162819\n",
      "2017-11-11 07:19:40: Loss at step 4259: 0.03682086989283562\n",
      "2017-11-11 07:19:42: Loss at step 4260: 0.03674693778157234\n",
      "2017-11-11 07:19:44: Loss at step 4261: 0.03686266764998436\n",
      "2017-11-11 07:19:46: Loss at step 4262: 0.03683341294527054\n",
      "2017-11-11 07:19:48: Loss at step 4263: 0.03685967996716499\n",
      "2017-11-11 07:19:50: Loss at step 4264: 0.036783959716558456\n",
      "2017-11-11 07:19:52: Loss at step 4265: 0.03675902634859085\n",
      "2017-11-11 07:19:55: Loss at step 4266: 0.036859214305877686\n",
      "2017-11-11 07:19:57: Loss at step 4267: 0.03674904257059097\n",
      "2017-11-11 07:19:59: Loss at step 4268: 0.03678764030337334\n",
      "2017-11-11 07:20:01: Loss at step 4269: 0.03675450384616852\n",
      "2017-11-11 07:20:03: Loss at step 4270: 0.03677775710821152\n",
      "2017-11-11 07:20:05: Loss at step 4271: 0.03680015355348587\n",
      "2017-11-11 07:20:07: Loss at step 4272: 0.036857835948467255\n",
      "2017-11-11 07:20:10: Loss at step 4273: 0.03685774281620979\n",
      "2017-11-11 07:20:12: Loss at step 4274: 0.0367225706577301\n",
      "2017-11-11 07:20:14: Loss at step 4275: 0.03680508956313133\n",
      "2017-11-11 07:20:16: Loss at step 4276: 0.03684920817613602\n",
      "2017-11-11 07:20:18: Loss at step 4277: 0.03683679923415184\n",
      "2017-11-11 07:20:20: Loss at step 4278: 0.03686690703034401\n",
      "2017-11-11 07:20:22: Loss at step 4279: 0.036820124834775925\n",
      "2017-11-11 07:20:25: Loss at step 4280: 0.03673691302537918\n",
      "2017-11-11 07:20:27: Loss at step 4281: 0.036827459931373596\n",
      "2017-11-11 07:20:29: Loss at step 4282: 0.036730438470840454\n",
      "2017-11-11 07:20:31: Loss at step 4283: 0.03675838187336922\n",
      "2017-11-11 07:20:33: Loss at step 4284: 0.036783307790756226\n",
      "2017-11-11 07:20:35: Loss at step 4285: 0.03682391345500946\n",
      "2017-11-11 07:20:38: Loss at step 4286: 0.0368289016187191\n",
      "2017-11-11 07:20:40: Loss at step 4287: 0.03679792582988739\n",
      "2017-11-11 07:20:42: Loss at step 4288: 0.03685639053583145\n",
      "2017-11-11 07:20:44: Loss at step 4289: 0.03683643415570259\n",
      "2017-11-11 07:20:46: Loss at step 4290: 0.03684685379266739\n",
      "2017-11-11 07:20:48: Loss at step 4291: 0.03679536283016205\n",
      "2017-11-11 07:20:50: Loss at step 4292: 0.03673667088150978\n",
      "2017-11-11 07:20:52: Loss at step 4293: 0.03685750812292099\n",
      "2017-11-11 07:20:55: Loss at step 4294: 0.03677961602807045\n",
      "2017-11-11 07:20:57: Loss at step 4295: 0.03681130334734917\n",
      "2017-11-11 07:20:59: Loss at step 4296: 0.03676815330982208\n",
      "2017-11-11 07:21:01: Loss at step 4297: 0.03690754249691963\n",
      "2017-11-11 07:21:03: Loss at step 4298: 0.03676425665616989\n",
      "2017-11-11 07:21:05: Loss at step 4299: 0.036753807216882706\n",
      "2017-11-11 07:21:08: Loss at step 4300: 0.03679923713207245\n",
      "2017-11-11 07:21:10: Loss at step 4301: 0.03676622360944748\n",
      "2017-11-11 07:21:12: Loss at step 4302: 0.03676537051796913\n",
      "2017-11-11 07:21:14: Loss at step 4303: 0.03683403879404068\n",
      "2017-11-11 07:21:16: Loss at step 4304: 0.0367717482149601\n",
      "2017-11-11 07:21:18: Loss at step 4305: 0.0367937907576561\n",
      "2017-11-11 07:21:20: Loss at step 4306: 0.03676806762814522\n",
      "2017-11-11 07:21:23: Loss at step 4307: 0.03676088526844978\n",
      "2017-11-11 07:21:25: Loss at step 4308: 0.03686961531639099\n",
      "2017-11-11 07:21:27: Loss at step 4309: 0.03688269108533859\n",
      "2017-11-11 07:21:29: Loss at step 4310: 0.03687196224927902\n",
      "2017-11-11 07:21:31: Loss at step 4311: 0.036956787109375\n",
      "2017-11-11 07:21:33: Loss at step 4312: 0.03689686208963394\n",
      "2017-11-11 07:21:35: Loss at step 4313: 0.03681837022304535\n",
      "2017-11-11 07:21:37: Loss at step 4314: 0.036835815757513046\n",
      "2017-11-11 07:21:40: Loss at step 4315: 0.0367923267185688\n",
      "2017-11-11 07:21:42: Loss at step 4316: 0.03681911155581474\n",
      "2017-11-11 07:21:44: Loss at step 4317: 0.03680521994829178\n",
      "2017-11-11 07:21:46: Loss at step 4318: 0.03683413192629814\n",
      "2017-11-11 07:21:48: Loss at step 4319: 0.036884795874357224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:21:50: Loss at step 4320: 0.03679546341300011\n",
      "2017-11-11 07:21:52: Loss at step 4321: 0.03676209971308708\n",
      "2017-11-11 07:21:54: Loss at step 4322: 0.036793053150177\n",
      "2017-11-11 07:21:57: Loss at step 4323: 0.03677064925432205\n",
      "2017-11-11 07:21:59: Loss at step 4324: 0.036790989339351654\n",
      "2017-11-11 07:22:01: Loss at step 4325: 0.036791060119867325\n",
      "2017-11-11 07:22:03: Loss at step 4326: 0.03679526224732399\n",
      "2017-11-11 07:22:05: Loss at step 4327: 0.03674650192260742\n",
      "2017-11-11 07:22:07: Loss at step 4328: 0.03673017770051956\n",
      "2017-11-11 07:22:09: Loss at step 4329: 0.0368083119392395\n",
      "2017-11-11 07:22:12: Loss at step 4330: 0.03680064529180527\n",
      "2017-11-11 07:22:14: Loss at step 4331: 0.03681934252381325\n",
      "2017-11-11 07:22:16: Loss at step 4332: 0.03682645410299301\n",
      "2017-11-11 07:22:18: Loss at step 4333: 0.03687991574406624\n",
      "2017-11-11 07:22:20: Loss at step 4334: 0.036810535937547684\n",
      "2017-11-11 07:22:22: Loss at step 4335: 0.03684905543923378\n",
      "2017-11-11 07:22:24: Loss at step 4336: 0.03681160882115364\n",
      "2017-11-11 07:22:27: Loss at step 4337: 0.03687295690178871\n",
      "2017-11-11 07:22:29: Loss at step 4338: 0.03680884465575218\n",
      "2017-11-11 07:22:31: Loss at step 4339: 0.03685391694307327\n",
      "2017-11-11 07:22:33: Loss at step 4340: 0.03681831806898117\n",
      "2017-11-11 07:22:35: Loss at step 4341: 0.03679164499044418\n",
      "2017-11-11 07:22:37: Loss at step 4342: 0.036680515855550766\n",
      "2017-11-11 07:22:39: Loss at step 4343: 0.0367419496178627\n",
      "2017-11-11 07:22:42: Loss at step 4344: 0.03680085390806198\n",
      "2017-11-11 07:22:44: Loss at step 4345: 0.03681279346346855\n",
      "2017-11-11 07:22:46: Loss at step 4346: 0.036767348647117615\n",
      "2017-11-11 07:22:48: Loss at step 4347: 0.036824584007263184\n",
      "2017-11-11 07:22:50: Loss at step 4348: 0.03681830316781998\n",
      "2017-11-11 07:22:52: Loss at step 4349: 0.036813221871852875\n",
      "2017-11-11 07:22:54: Loss at step 4350: 0.03677130863070488\n",
      "2017-11-11 07:22:57: Loss at step 4351: 0.03674525022506714\n",
      "2017-11-11 07:22:59: Loss at step 4352: 0.03684651479125023\n",
      "2017-11-11 07:23:01: Loss at step 4353: 0.03677339106798172\n",
      "2017-11-11 07:23:03: Loss at step 4354: 0.03677525743842125\n",
      "2017-11-11 07:23:05: Loss at step 4355: 0.03673625737428665\n",
      "2017-11-11 07:23:07: Loss at step 4356: 0.03685680031776428\n",
      "2017-11-11 07:23:09: Loss at step 4357: 0.03678242862224579\n",
      "2017-11-11 07:23:11: Loss at step 4358: 0.036782555282115936\n",
      "2017-11-11 07:23:14: Loss at step 4359: 0.03679781034588814\n",
      "2017-11-11 07:23:16: Loss at step 4360: 0.036802858114242554\n",
      "2017-11-11 07:23:18: Loss at step 4361: 0.036775194108486176\n",
      "2017-11-11 07:23:20: Loss at step 4362: 0.0368090383708477\n",
      "2017-11-11 07:23:22: Loss at step 4363: 0.03672916442155838\n",
      "2017-11-11 07:23:24: Loss at step 4364: 0.03672625869512558\n",
      "2017-11-11 07:23:26: Loss at step 4365: 0.036780547350645065\n",
      "2017-11-11 07:23:29: Loss at step 4366: 0.036771342158317566\n",
      "2017-11-11 07:23:31: Loss at step 4367: 0.03679315000772476\n",
      "2017-11-11 07:23:33: Loss at step 4368: 0.03678136691451073\n",
      "2017-11-11 07:23:35: Loss at step 4369: 0.0367247611284256\n",
      "2017-11-11 07:23:37: Loss at step 4370: 0.03678042069077492\n",
      "2017-11-11 07:23:39: Loss at step 4371: 0.03678488731384277\n",
      "2017-11-11 07:23:41: Loss at step 4372: 0.03676086291670799\n",
      "2017-11-11 07:23:44: Loss at step 4373: 0.03675642982125282\n",
      "2017-11-11 07:23:46: Loss at step 4374: 0.03677615523338318\n",
      "2017-11-11 07:23:48: Loss at step 4375: 0.03681587055325508\n",
      "2017-11-11 07:23:50: Loss at step 4376: 0.03677171468734741\n",
      "2017-11-11 07:23:52: Loss at step 4377: 0.036863941699266434\n",
      "2017-11-11 07:23:54: Loss at step 4378: 0.03683161735534668\n",
      "2017-11-11 07:23:56: Loss at step 4379: 0.0367378294467926\n",
      "2017-11-11 07:23:59: Loss at step 4380: 0.03678501769900322\n",
      "2017-11-11 07:24:01: Loss at step 4381: 0.03683125972747803\n",
      "2017-11-11 07:24:03: Loss at step 4382: 0.03678396716713905\n",
      "2017-11-11 07:24:05: Loss at step 4383: 0.03689057007431984\n",
      "2017-11-11 07:24:07: Loss at step 4384: 0.036869581788778305\n",
      "2017-11-11 07:24:09: Loss at step 4385: 0.03683962672948837\n",
      "2017-11-11 07:24:11: Loss at step 4386: 0.0368826724588871\n",
      "2017-11-11 07:24:14: Loss at step 4387: 0.03672734275460243\n",
      "2017-11-11 07:24:16: Loss at step 4388: 0.036773134022951126\n",
      "2017-11-11 07:24:18: Loss at step 4389: 0.03683367371559143\n",
      "2017-11-11 07:24:20: Loss at step 4390: 0.036881666630506516\n",
      "2017-11-11 07:24:22: Loss at step 4391: 0.03679154813289642\n",
      "2017-11-11 07:24:24: Loss at step 4392: 0.03678061068058014\n",
      "2017-11-11 07:24:26: Loss at step 4393: 0.03675517067313194\n",
      "2017-11-11 07:24:29: Loss at step 4394: 0.03676777333021164\n",
      "2017-11-11 07:24:31: Loss at step 4395: 0.036755941808223724\n",
      "2017-11-11 07:24:33: Loss at step 4396: 0.03674450144171715\n",
      "2017-11-11 07:24:35: Loss at step 4397: 0.036745600402355194\n",
      "2017-11-11 07:24:37: Loss at step 4398: 0.036805689334869385\n",
      "2017-11-11 07:24:39: Loss at step 4399: 0.03673265874385834\n",
      "2017-11-11 07:24:41: Loss at step 4400: 0.03685620427131653\n",
      "2017-11-11 07:24:44: Loss at step 4401: 0.036839574575424194\n",
      "2017-11-11 07:24:46: Loss at step 4402: 0.03689795359969139\n",
      "2017-11-11 07:24:48: Loss at step 4403: 0.03679001331329346\n",
      "2017-11-11 07:24:50: Loss at step 4404: 0.03675641492009163\n",
      "2017-11-11 07:24:52: Loss at step 4405: 0.036780934780836105\n",
      "2017-11-11 07:24:54: Loss at step 4406: 0.03682159632444382\n",
      "2017-11-11 07:24:56: Loss at step 4407: 0.036788005381822586\n",
      "2017-11-11 07:24:59: Loss at step 4408: 0.03687889501452446\n",
      "2017-11-11 07:25:01: Loss at step 4409: 0.03685813024640083\n",
      "2017-11-11 07:25:03: Loss at step 4410: 0.03680935502052307\n",
      "2017-11-11 07:25:05: Loss at step 4411: 0.03688756749033928\n",
      "2017-11-11 07:25:07: Loss at step 4412: 0.03687174618244171\n",
      "2017-11-11 07:25:09: Loss at step 4413: 0.03683125600218773\n",
      "2017-11-11 07:25:11: Loss at step 4414: 0.03679775446653366\n",
      "2017-11-11 07:25:13: Loss at step 4415: 0.036826856434345245\n",
      "2017-11-11 07:25:16: Loss at step 4416: 0.036789752542972565\n",
      "2017-11-11 07:25:18: Loss at step 4417: 0.03687926009297371\n",
      "2017-11-11 07:25:20: Loss at step 4418: 0.03683334216475487\n",
      "2017-11-11 07:25:22: Loss at step 4419: 0.03676890209317207\n",
      "2017-11-11 07:25:24: Loss at step 4420: 0.036825310438871384\n",
      "2017-11-11 07:25:26: Loss at step 4421: 0.03692205250263214\n",
      "2017-11-11 07:25:28: Loss at step 4422: 0.03688439354300499\n",
      "2017-11-11 07:25:31: Loss at step 4423: 0.036842308938503265\n",
      "2017-11-11 07:25:33: Loss at step 4424: 0.036756303161382675\n",
      "2017-11-11 07:25:35: Loss at step 4425: 0.036892689764499664\n",
      "2017-11-11 07:25:37: Loss at step 4426: 0.03676602244377136\n",
      "2017-11-11 07:25:39: Loss at step 4427: 0.03674869239330292\n",
      "2017-11-11 07:25:41: Loss at step 4428: 0.03676148131489754\n",
      "2017-11-11 07:25:43: Loss at step 4429: 0.0367828905582428\n",
      "2017-11-11 07:25:45: Loss at step 4430: 0.036791395395994186\n",
      "2017-11-11 07:25:48: Loss at step 4431: 0.03681793063879013\n",
      "2017-11-11 07:25:50: Loss at step 4432: 0.03685919940471649\n",
      "2017-11-11 07:25:52: Loss at step 4433: 0.036776911467313766\n",
      "2017-11-11 07:25:54: Loss at step 4434: 0.03683154657483101\n",
      "2017-11-11 07:25:56: Loss at step 4435: 0.0368044450879097\n",
      "2017-11-11 07:25:58: Loss at step 4436: 0.0367821604013443\n",
      "2017-11-11 07:26:00: Loss at step 4437: 0.03682640939950943\n",
      "2017-11-11 07:26:03: Loss at step 4438: 0.036828845739364624\n",
      "2017-11-11 07:26:05: Loss at step 4439: 0.036694902926683426\n",
      "2017-11-11 07:26:07: Loss at step 4440: 0.03683741018176079\n",
      "2017-11-11 07:26:09: Loss at step 4441: 0.03672904148697853\n",
      "2017-11-11 07:26:11: Loss at step 4442: 0.036740247160196304\n",
      "2017-11-11 07:26:13: Loss at step 4443: 0.03682463988661766\n",
      "2017-11-11 07:26:15: Loss at step 4444: 0.03683207556605339\n",
      "2017-11-11 07:26:18: Loss at step 4445: 0.03674488514661789\n",
      "2017-11-11 07:26:20: Loss at step 4446: 0.03676970675587654\n",
      "2017-11-11 07:26:22: Loss at step 4447: 0.03682411462068558\n",
      "2017-11-11 07:26:24: Loss at step 4448: 0.03685205802321434\n",
      "2017-11-11 07:26:26: Loss at step 4449: 0.036754533648490906\n",
      "2017-11-11 07:26:28: Loss at step 4450: 0.03682751581072807\n",
      "2017-11-11 07:26:30: Loss at step 4451: 0.03677954152226448\n",
      "2017-11-11 07:26:33: Loss at step 4452: 0.036840133368968964\n",
      "2017-11-11 07:26:35: Loss at step 4453: 0.036911740899086\n",
      "2017-11-11 07:26:37: Loss at step 4454: 0.036793019622564316\n",
      "2017-11-11 07:26:39: Loss at step 4455: 0.036857541650533676\n",
      "2017-11-11 07:26:41: Loss at step 4456: 0.03684672713279724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:26:43: Loss at step 4457: 0.036934178322553635\n",
      "2017-11-11 07:26:45: Loss at step 4458: 0.036819156259298325\n",
      "2017-11-11 07:26:48: Loss at step 4459: 0.03684905171394348\n",
      "2017-11-11 07:26:50: Loss at step 4460: 0.03687296807765961\n",
      "2017-11-11 07:26:52: Loss at step 4461: 0.03674305975437164\n",
      "2017-11-11 07:26:54: Loss at step 4462: 0.03685790300369263\n",
      "2017-11-11 07:26:56: Loss at step 4463: 0.03679955005645752\n",
      "2017-11-11 07:26:58: Loss at step 4464: 0.03679758310317993\n",
      "2017-11-11 07:27:00: Loss at step 4465: 0.036778297275304794\n",
      "2017-11-11 07:27:03: Loss at step 4466: 0.036836203187704086\n",
      "2017-11-11 07:27:05: Loss at step 4467: 0.03690183162689209\n",
      "2017-11-11 07:27:07: Loss at step 4468: 0.03683513030409813\n",
      "2017-11-11 07:27:09: Loss at step 4469: 0.03676626831293106\n",
      "2017-11-11 07:27:11: Loss at step 4470: 0.03682905435562134\n",
      "2017-11-11 07:27:13: Loss at step 4471: 0.03675508871674538\n",
      "2017-11-11 07:27:15: Loss at step 4472: 0.03683052211999893\n",
      "2017-11-11 07:27:17: Loss at step 4473: 0.036891065537929535\n",
      "2017-11-11 07:27:20: Loss at step 4474: 0.036843739449977875\n",
      "2017-11-11 07:27:22: Loss at step 4475: 0.036784544587135315\n",
      "2017-11-11 07:27:24: Loss at step 4476: 0.03672939911484718\n",
      "2017-11-11 07:27:26: Loss at step 4477: 0.03681236132979393\n",
      "2017-11-11 07:27:28: Loss at step 4478: 0.03689223527908325\n",
      "2017-11-11 07:27:30: Loss at step 4479: 0.036821283400058746\n",
      "2017-11-11 07:27:32: Loss at step 4480: 0.03681746870279312\n",
      "2017-11-11 07:27:35: Loss at step 4481: 0.036816637963056564\n",
      "2017-11-11 07:27:37: Loss at step 4482: 0.03682860732078552\n",
      "2017-11-11 07:27:39: Loss at step 4483: 0.03691425547003746\n",
      "2017-11-11 07:27:41: Loss at step 4484: 0.036781858652830124\n",
      "2017-11-11 07:27:43: Loss at step 4485: 0.03685220330953598\n",
      "2017-11-11 07:27:45: Loss at step 4486: 0.03668944537639618\n",
      "2017-11-11 07:27:47: Loss at step 4487: 0.03677685558795929\n",
      "2017-11-11 07:27:50: Loss at step 4488: 0.03673253953456879\n",
      "2017-11-11 07:27:52: Loss at step 4489: 0.03687901794910431\n",
      "2017-11-11 07:27:54: Loss at step 4490: 0.03685157373547554\n",
      "2017-11-11 07:27:56: Loss at step 4491: 0.03684329241514206\n",
      "2017-11-11 07:27:58: Loss at step 4492: 0.03680434450507164\n",
      "2017-11-11 07:28:00: Loss at step 4493: 0.03683095797896385\n",
      "2017-11-11 07:28:02: Loss at step 4494: 0.03688895329833031\n",
      "2017-11-11 07:28:04: Loss at step 4495: 0.03680383414030075\n",
      "2017-11-11 07:28:07: Loss at step 4496: 0.03685528039932251\n",
      "2017-11-11 07:28:09: Loss at step 4497: 0.036920152604579926\n",
      "2017-11-11 07:28:11: Loss at step 4498: 0.03685646876692772\n",
      "2017-11-11 07:28:13: Loss at step 4499: 0.03682919591665268\n",
      "2017-11-11 07:28:15: Loss at step 4500: 0.036931268870830536\n",
      "2017-11-11 07:28:17: Loss at step 4501: 0.036881040781736374\n",
      "2017-11-11 07:28:19: Loss at step 4502: 0.03687600418925285\n",
      "2017-11-11 07:28:22: Loss at step 4503: 0.03692426159977913\n",
      "2017-11-11 07:28:24: Loss at step 4504: 0.03686162084341049\n",
      "2017-11-11 07:28:26: Loss at step 4505: 0.03692115470767021\n",
      "2017-11-11 07:28:28: Loss at step 4506: 0.036857057362794876\n",
      "2017-11-11 07:28:30: Loss at step 4507: 0.03696935623884201\n",
      "2017-11-11 07:28:32: Loss at step 4508: 0.0369100384414196\n",
      "2017-11-11 07:28:35: Loss at step 4509: 0.036891888827085495\n",
      "2017-11-11 07:28:37: Loss at step 4510: 0.03694099932909012\n",
      "2017-11-11 07:28:39: Loss at step 4511: 0.03710529953241348\n",
      "2017-11-11 07:28:41: Loss at step 4512: 0.03703586384654045\n",
      "2017-11-11 07:28:43: Loss at step 4513: 0.036967478692531586\n",
      "2017-11-11 07:28:45: Loss at step 4514: 0.03703967481851578\n",
      "2017-11-11 07:28:47: Loss at step 4515: 0.037043023854494095\n",
      "2017-11-11 07:28:49: Loss at step 4516: 0.03697999194264412\n",
      "2017-11-11 07:28:52: Loss at step 4517: 0.037006184458732605\n",
      "2017-11-11 07:28:54: Loss at step 4518: 0.03698175773024559\n",
      "2017-11-11 07:28:56: Loss at step 4519: 0.03686180338263512\n",
      "2017-11-11 07:28:58: Loss at step 4520: 0.03702166676521301\n",
      "2017-11-11 07:29:00: Loss at step 4521: 0.03698767349123955\n",
      "2017-11-11 07:29:02: Loss at step 4522: 0.0368880070745945\n",
      "2017-11-11 07:29:04: Loss at step 4523: 0.03692387416958809\n",
      "2017-11-11 07:29:07: Loss at step 4524: 0.03694656491279602\n",
      "2017-11-11 07:29:09: Loss at step 4525: 0.03689957410097122\n",
      "2017-11-11 07:29:11: Loss at step 4526: 0.036838408559560776\n",
      "2017-11-11 07:29:13: Loss at step 4527: 0.03690813109278679\n",
      "2017-11-11 07:29:15: Loss at step 4528: 0.036805033683776855\n",
      "2017-11-11 07:29:17: Loss at step 4529: 0.036844585090875626\n",
      "2017-11-11 07:29:19: Loss at step 4530: 0.03691922500729561\n",
      "2017-11-11 07:29:22: Loss at step 4531: 0.03682173416018486\n",
      "2017-11-11 07:29:24: Loss at step 4532: 0.03684462979435921\n",
      "2017-11-11 07:29:26: Loss at step 4533: 0.03689516708254814\n",
      "2017-11-11 07:29:28: Loss at step 4534: 0.03679018095135689\n",
      "2017-11-11 07:29:30: Loss at step 4535: 0.036832500249147415\n",
      "2017-11-11 07:29:32: Loss at step 4536: 0.036805883049964905\n",
      "2017-11-11 07:29:34: Loss at step 4537: 0.036767151206731796\n",
      "2017-11-11 07:29:37: Loss at step 4538: 0.036781784147024155\n",
      "2017-11-11 07:29:39: Loss at step 4539: 0.03677268698811531\n",
      "2017-11-11 07:29:41: Loss at step 4540: 0.03681802377104759\n",
      "2017-11-11 07:29:43: Loss at step 4541: 0.03674384951591492\n",
      "2017-11-11 07:29:45: Loss at step 4542: 0.03680247813463211\n",
      "2017-11-11 07:29:47: Loss at step 4543: 0.03673939406871796\n",
      "2017-11-11 07:29:49: Loss at step 4544: 0.036823298782110214\n",
      "2017-11-11 07:29:51: Loss at step 4545: 0.03670124709606171\n",
      "2017-11-11 07:29:54: Loss at step 4546: 0.03680679574608803\n",
      "2017-11-11 07:29:56: Loss at step 4547: 0.036844488233327866\n",
      "2017-11-11 07:29:58: Loss at step 4548: 0.036775633692741394\n",
      "2017-11-11 07:30:00: Loss at step 4549: 0.03680657967925072\n",
      "2017-11-11 07:30:02: Loss at step 4550: 0.03679024428129196\n",
      "2017-11-11 07:30:04: Loss at step 4551: 0.03685791417956352\n",
      "2017-11-11 07:30:06: Loss at step 4552: 0.036773212254047394\n",
      "2017-11-11 07:30:09: Loss at step 4553: 0.03684864565730095\n",
      "2017-11-11 07:30:11: Loss at step 4554: 0.03677528724074364\n",
      "2017-11-11 07:30:13: Loss at step 4555: 0.03672725707292557\n",
      "2017-11-11 07:30:15: Loss at step 4556: 0.03682407736778259\n",
      "2017-11-11 07:30:17: Loss at step 4557: 0.03692014142870903\n",
      "2017-11-11 07:30:19: Loss at step 4558: 0.03683355078101158\n",
      "2017-11-11 07:30:21: Loss at step 4559: 0.03670544549822807\n",
      "2017-11-11 07:30:24: Loss at step 4560: 0.0367460772395134\n",
      "2017-11-11 07:30:26: Loss at step 4561: 0.03686021640896797\n",
      "2017-11-11 07:30:28: Loss at step 4562: 0.03676055371761322\n",
      "2017-11-11 07:30:30: Loss at step 4563: 0.03670526668429375\n",
      "2017-11-11 07:30:32: Loss at step 4564: 0.036732856184244156\n",
      "2017-11-11 07:30:34: Loss at step 4565: 0.03679944574832916\n",
      "2017-11-11 07:30:37: Loss at step 4566: 0.036841485649347305\n",
      "2017-11-11 07:30:39: Loss at step 4567: 0.03685867041349411\n",
      "2017-11-11 07:30:41: Loss at step 4568: 0.03683316707611084\n",
      "2017-11-11 07:30:43: Loss at step 4569: 0.036846257746219635\n",
      "2017-11-11 07:30:45: Loss at step 4570: 0.036794018000364304\n",
      "2017-11-11 07:30:47: Loss at step 4571: 0.03676682710647583\n",
      "2017-11-11 07:30:49: Loss at step 4572: 0.03678756579756737\n",
      "2017-11-11 07:30:51: Loss at step 4573: 0.03681620582938194\n",
      "2017-11-11 07:30:54: Loss at step 4574: 0.036806270480155945\n",
      "2017-11-11 07:30:56: Loss at step 4575: 0.036809291690588\n",
      "2017-11-11 07:30:58: Loss at step 4576: 0.03678475320339203\n",
      "2017-11-11 07:31:00: Loss at step 4577: 0.03686489909887314\n",
      "2017-11-11 07:31:02: Loss at step 4578: 0.03684133291244507\n",
      "2017-11-11 07:31:04: Loss at step 4579: 0.036766666918992996\n",
      "2017-11-11 07:31:06: Loss at step 4580: 0.03679237142205238\n",
      "2017-11-11 07:31:09: Loss at step 4581: 0.03680643439292908\n",
      "2017-11-11 07:31:11: Loss at step 4582: 0.036786098033189774\n",
      "2017-11-11 07:31:13: Loss at step 4583: 0.03683089092373848\n",
      "2017-11-11 07:31:15: Loss at step 4584: 0.036816176027059555\n",
      "2017-11-11 07:31:17: Loss at step 4585: 0.03676057606935501\n",
      "2017-11-11 07:31:19: Loss at step 4586: 0.03679724782705307\n",
      "2017-11-11 07:31:21: Loss at step 4587: 0.03682297095656395\n",
      "2017-11-11 07:31:24: Loss at step 4588: 0.036874376237392426\n",
      "2017-11-11 07:31:26: Loss at step 4589: 0.036852069199085236\n",
      "2017-11-11 07:31:28: Loss at step 4590: 0.03677237033843994\n",
      "2017-11-11 07:31:30: Loss at step 4591: 0.03676483407616615\n",
      "2017-11-11 07:31:32: Loss at step 4592: 0.036796581000089645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:31:34: Loss at step 4593: 0.03677451238036156\n",
      "2017-11-11 07:31:36: Loss at step 4594: 0.03673306107521057\n",
      "2017-11-11 07:31:39: Loss at step 4595: 0.036736443638801575\n",
      "2017-11-11 07:31:41: Loss at step 4596: 0.036779116839170456\n",
      "2017-11-11 07:31:43: Loss at step 4597: 0.03668849170207977\n",
      "2017-11-11 07:31:45: Loss at step 4598: 0.0368763767182827\n",
      "2017-11-11 07:31:47: Loss at step 4599: 0.036811549216508865\n",
      "2017-11-11 07:31:49: Loss at step 4600: 0.03674056753516197\n",
      "2017-11-11 07:31:51: Loss at step 4601: 0.03679642826318741\n",
      "2017-11-11 07:31:53: Loss at step 4602: 0.03677472472190857\n",
      "2017-11-11 07:31:56: Loss at step 4603: 0.03678395226597786\n",
      "2017-11-11 07:31:58: Loss at step 4604: 0.036830704659223557\n",
      "2017-11-11 07:32:00: Loss at step 4605: 0.03686445951461792\n",
      "2017-11-11 07:32:02: Loss at step 4606: 0.03686058893799782\n",
      "2017-11-11 07:32:04: Loss at step 4607: 0.036823224276304245\n",
      "2017-11-11 07:32:06: Loss at step 4608: 0.036815814673900604\n",
      "2017-11-11 07:32:08: Loss at step 4609: 0.03678331896662712\n",
      "2017-11-11 07:32:11: Loss at step 4610: 0.03683320805430412\n",
      "2017-11-11 07:32:13: Loss at step 4611: 0.03684650734066963\n",
      "2017-11-11 07:32:15: Loss at step 4612: 0.03672679886221886\n",
      "2017-11-11 07:32:17: Loss at step 4613: 0.03672538325190544\n",
      "2017-11-11 07:32:19: Loss at step 4614: 0.03673470392823219\n",
      "2017-11-11 07:32:21: Loss at step 4615: 0.03673567622900009\n",
      "2017-11-11 07:32:23: Loss at step 4616: 0.0368005707859993\n",
      "2017-11-11 07:32:26: Loss at step 4617: 0.0367860421538353\n",
      "2017-11-11 07:32:28: Loss at step 4618: 0.03680022060871124\n",
      "2017-11-11 07:32:30: Loss at step 4619: 0.03674859553575516\n",
      "2017-11-11 07:32:32: Loss at step 4620: 0.03673188388347626\n",
      "2017-11-11 07:32:34: Loss at step 4621: 0.03686801716685295\n",
      "2017-11-11 07:32:36: Loss at step 4622: 0.03674745187163353\n",
      "2017-11-11 07:32:38: Loss at step 4623: 0.036771804094314575\n",
      "2017-11-11 07:32:41: Loss at step 4624: 0.03672240301966667\n",
      "2017-11-11 07:32:43: Loss at step 4625: 0.036815330386161804\n",
      "2017-11-11 07:32:45: Loss at step 4626: 0.036822445690631866\n",
      "2017-11-11 07:32:47: Loss at step 4627: 0.036706067621707916\n",
      "2017-11-11 07:32:49: Loss at step 4628: 0.03675789758563042\n",
      "2017-11-11 07:32:51: Loss at step 4629: 0.036832354962825775\n",
      "2017-11-11 07:32:53: Loss at step 4630: 0.036837153136730194\n",
      "2017-11-11 07:32:56: Loss at step 4631: 0.03687000647187233\n",
      "2017-11-11 07:32:58: Loss at step 4632: 0.036692991852760315\n",
      "2017-11-11 07:33:00: Loss at step 4633: 0.03672884404659271\n",
      "2017-11-11 07:33:02: Loss at step 4634: 0.036732789129018784\n",
      "2017-11-11 07:33:04: Loss at step 4635: 0.03679133951663971\n",
      "2017-11-11 07:33:06: Loss at step 4636: 0.03680849447846413\n",
      "2017-11-11 07:33:08: Loss at step 4637: 0.036786384880542755\n",
      "2017-11-11 07:33:11: Loss at step 4638: 0.03676290810108185\n",
      "2017-11-11 07:33:13: Loss at step 4639: 0.03673342242836952\n",
      "2017-11-11 07:33:15: Loss at step 4640: 0.03679657354950905\n",
      "2017-11-11 07:33:17: Loss at step 4641: 0.03672686964273453\n",
      "2017-11-11 07:33:19: Loss at step 4642: 0.0367649607360363\n",
      "2017-11-11 07:33:21: Loss at step 4643: 0.03689318895339966\n",
      "2017-11-11 07:33:23: Loss at step 4644: 0.036766260862350464\n",
      "2017-11-11 07:33:26: Loss at step 4645: 0.03679030016064644\n",
      "2017-11-11 07:33:28: Loss at step 4646: 0.03675997257232666\n",
      "2017-11-11 07:33:30: Loss at step 4647: 0.036725565791130066\n",
      "2017-11-11 07:33:32: Loss at step 4648: 0.036824967712163925\n",
      "2017-11-11 07:33:34: Loss at step 4649: 0.036809783428907394\n",
      "2017-11-11 07:33:36: Loss at step 4650: 0.03679046034812927\n",
      "2017-11-11 07:33:38: Loss at step 4651: 0.03673798218369484\n",
      "2017-11-11 07:33:41: Loss at step 4652: 0.03688023239374161\n",
      "2017-11-11 07:33:43: Loss at step 4653: 0.0368182510137558\n",
      "2017-11-11 07:33:45: Loss at step 4654: 0.03680184856057167\n",
      "2017-11-11 07:33:47: Loss at step 4655: 0.03675193339586258\n",
      "2017-11-11 07:33:49: Loss at step 4656: 0.036847203969955444\n",
      "2017-11-11 07:33:51: Loss at step 4657: 0.03680490702390671\n",
      "2017-11-11 07:33:53: Loss at step 4658: 0.03684047982096672\n",
      "2017-11-11 07:33:55: Loss at step 4659: 0.03674933686852455\n",
      "2017-11-11 07:33:58: Loss at step 4660: 0.03675760328769684\n",
      "2017-11-11 07:34:00: Loss at step 4661: 0.036769475787878036\n",
      "2017-11-11 07:34:02: Loss at step 4662: 0.036936547607183456\n",
      "2017-11-11 07:34:04: Loss at step 4663: 0.03676503151655197\n",
      "2017-11-11 07:34:06: Loss at step 4664: 0.036752354353666306\n",
      "2017-11-11 07:34:08: Loss at step 4665: 0.03671405836939812\n",
      "2017-11-11 07:34:10: Loss at step 4666: 0.03675772622227669\n",
      "2017-11-11 07:34:13: Loss at step 4667: 0.036898985505104065\n",
      "2017-11-11 07:34:15: Loss at step 4668: 0.03679422289133072\n",
      "2017-11-11 07:34:17: Loss at step 4669: 0.03684886544942856\n",
      "2017-11-11 07:34:19: Loss at step 4670: 0.036824412643909454\n",
      "2017-11-11 07:34:21: Loss at step 4671: 0.03681602329015732\n",
      "2017-11-11 07:34:23: Loss at step 4672: 0.0368223637342453\n",
      "2017-11-11 07:34:25: Loss at step 4673: 0.03678765147924423\n",
      "2017-11-11 07:34:28: Loss at step 4674: 0.03686380758881569\n",
      "2017-11-11 07:34:30: Loss at step 4675: 0.03673924505710602\n",
      "2017-11-11 07:34:32: Loss at step 4676: 0.03679455444216728\n",
      "2017-11-11 07:34:34: Loss at step 4677: 0.03685350343585014\n",
      "2017-11-11 07:34:36: Loss at step 4678: 0.036795295774936676\n",
      "2017-11-11 07:34:38: Loss at step 4679: 0.036903563886880875\n",
      "2017-11-11 07:34:40: Loss at step 4680: 0.036857377737760544\n",
      "2017-11-11 07:34:43: Loss at step 4681: 0.03681640326976776\n",
      "2017-11-11 07:34:45: Loss at step 4682: 0.03688922896981239\n",
      "2017-11-11 07:34:47: Loss at step 4683: 0.03679797425866127\n",
      "2017-11-11 07:34:49: Loss at step 4684: 0.03675626218318939\n",
      "2017-11-11 07:34:51: Loss at step 4685: 0.03676597401499748\n",
      "2017-11-11 07:34:53: Loss at step 4686: 0.03678150475025177\n",
      "2017-11-11 07:34:55: Loss at step 4687: 0.036944035440683365\n",
      "2017-11-11 07:34:58: Loss at step 4688: 0.03678670525550842\n",
      "2017-11-11 07:35:00: Loss at step 4689: 0.036787357181310654\n",
      "2017-11-11 07:35:02: Loss at step 4690: 0.03678423538804054\n",
      "2017-11-11 07:35:04: Loss at step 4691: 0.03675391525030136\n",
      "2017-11-11 07:35:06: Loss at step 4692: 0.0367642380297184\n",
      "2017-11-11 07:35:08: Loss at step 4693: 0.036690037697553635\n",
      "2017-11-11 07:35:10: Loss at step 4694: 0.036766473203897476\n",
      "2017-11-11 07:35:13: Loss at step 4695: 0.036799266934394836\n",
      "2017-11-11 07:35:15: Loss at step 4696: 0.03674664348363876\n",
      "2017-11-11 07:35:17: Loss at step 4697: 0.03680398687720299\n",
      "2017-11-11 07:35:19: Loss at step 4698: 0.0367257185280323\n",
      "2017-11-11 07:35:21: Loss at step 4699: 0.036779966205358505\n",
      "2017-11-11 07:35:23: Loss at step 4700: 0.036671459674835205\n",
      "2017-11-11 07:35:26: Loss at step 4701: 0.03676163777709007\n",
      "2017-11-11 07:35:28: Loss at step 4702: 0.03674290329217911\n",
      "2017-11-11 07:35:30: Loss at step 4703: 0.036670904606580734\n",
      "2017-11-11 07:35:32: Loss at step 4704: 0.036857374012470245\n",
      "2017-11-11 07:35:34: Loss at step 4705: 0.036745306104421616\n",
      "2017-11-11 07:35:36: Loss at step 4706: 0.036736395210027695\n",
      "2017-11-11 07:35:39: Loss at step 4707: 0.03668598085641861\n",
      "2017-11-11 07:35:41: Loss at step 4708: 0.03668620437383652\n",
      "2017-11-11 07:35:43: Loss at step 4709: 0.036740295588970184\n",
      "2017-11-11 07:35:45: Loss at step 4710: 0.036863554269075394\n",
      "2017-11-11 07:35:47: Loss at step 4711: 0.03680538013577461\n",
      "2017-11-11 07:35:49: Loss at step 4712: 0.03683155030012131\n",
      "2017-11-11 07:35:52: Loss at step 4713: 0.03678082302212715\n",
      "2017-11-11 07:35:54: Loss at step 4714: 0.03684692829847336\n",
      "2017-11-11 07:35:56: Loss at step 4715: 0.03679891303181648\n",
      "2017-11-11 07:35:58: Loss at step 4716: 0.03685550391674042\n",
      "2017-11-11 07:36:00: Loss at step 4717: 0.036797910928726196\n",
      "2017-11-11 07:36:02: Loss at step 4718: 0.03671962767839432\n",
      "2017-11-11 07:36:05: Loss at step 4719: 0.03678373992443085\n",
      "2017-11-11 07:36:07: Loss at step 4720: 0.036786992102861404\n",
      "2017-11-11 07:36:09: Loss at step 4721: 0.0367797389626503\n",
      "2017-11-11 07:36:11: Loss at step 4722: 0.03674793988466263\n",
      "2017-11-11 07:36:13: Loss at step 4723: 0.036674290895462036\n",
      "2017-11-11 07:36:15: Loss at step 4724: 0.036633171141147614\n",
      "2017-11-11 07:36:17: Loss at step 4725: 0.03673558309674263\n",
      "2017-11-11 07:36:20: Loss at step 4726: 0.03679034486413002\n",
      "2017-11-11 07:36:22: Loss at step 4727: 0.03677404299378395\n",
      "2017-11-11 07:36:24: Loss at step 4728: 0.0367596298456192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:36:26: Loss at step 4729: 0.03675154969096184\n",
      "2017-11-11 07:36:28: Loss at step 4730: 0.036789700388908386\n",
      "2017-11-11 07:36:30: Loss at step 4731: 0.03675869479775429\n",
      "2017-11-11 07:36:33: Loss at step 4732: 0.03682006150484085\n",
      "2017-11-11 07:36:35: Loss at step 4733: 0.03668932616710663\n",
      "2017-11-11 07:36:37: Loss at step 4734: 0.036763422191143036\n",
      "2017-11-11 07:36:39: Loss at step 4735: 0.036730583757162094\n",
      "2017-11-11 07:36:41: Loss at step 4736: 0.036821939051151276\n",
      "2017-11-11 07:36:43: Loss at step 4737: 0.03676929324865341\n",
      "2017-11-11 07:36:45: Loss at step 4738: 0.03680058568716049\n",
      "2017-11-11 07:36:48: Loss at step 4739: 0.03677249327301979\n",
      "2017-11-11 07:36:50: Loss at step 4740: 0.036715321242809296\n",
      "2017-11-11 07:36:52: Loss at step 4741: 0.03679325059056282\n",
      "2017-11-11 07:36:54: Loss at step 4742: 0.0368347242474556\n",
      "2017-11-11 07:36:56: Loss at step 4743: 0.03682859241962433\n",
      "2017-11-11 07:36:58: Loss at step 4744: 0.03678324073553085\n",
      "2017-11-11 07:37:01: Loss at step 4745: 0.036721423268318176\n",
      "2017-11-11 07:37:03: Loss at step 4746: 0.03674192354083061\n",
      "2017-11-11 07:37:05: Loss at step 4747: 0.03673572838306427\n",
      "2017-11-11 07:37:07: Loss at step 4748: 0.0367560051381588\n",
      "2017-11-11 07:37:09: Loss at step 4749: 0.036797501146793365\n",
      "2017-11-11 07:37:11: Loss at step 4750: 0.036709703505039215\n",
      "2017-11-11 07:37:13: Loss at step 4751: 0.036757856607437134\n",
      "2017-11-11 07:37:16: Loss at step 4752: 0.03678019344806671\n",
      "2017-11-11 07:37:18: Loss at step 4753: 0.03680938854813576\n",
      "2017-11-11 07:37:20: Loss at step 4754: 0.03670581057667732\n",
      "2017-11-11 07:37:22: Loss at step 4755: 0.03683865815401077\n",
      "2017-11-11 07:37:24: Loss at step 4756: 0.03678987920284271\n",
      "2017-11-11 07:37:26: Loss at step 4757: 0.03682570531964302\n",
      "2017-11-11 07:37:28: Loss at step 4758: 0.036748964339494705\n",
      "2017-11-11 07:37:31: Loss at step 4759: 0.0367298386991024\n",
      "2017-11-11 07:37:33: Loss at step 4760: 0.03684577718377113\n",
      "2017-11-11 07:37:35: Loss at step 4761: 0.03676076978445053\n",
      "2017-11-11 07:37:37: Loss at step 4762: 0.036778323352336884\n",
      "2017-11-11 07:37:39: Loss at step 4763: 0.036837153136730194\n",
      "2017-11-11 07:37:41: Loss at step 4764: 0.03681732714176178\n",
      "2017-11-11 07:37:43: Loss at step 4765: 0.036714766174554825\n",
      "2017-11-11 07:37:45: Loss at step 4766: 0.03679192438721657\n",
      "2017-11-11 07:37:48: Loss at step 4767: 0.036669209599494934\n",
      "2017-11-11 07:37:50: Loss at step 4768: 0.036763258278369904\n",
      "2017-11-11 07:37:52: Loss at step 4769: 0.03673948720097542\n",
      "2017-11-11 07:37:54: Loss at step 4770: 0.03678188845515251\n",
      "2017-11-11 07:37:56: Loss at step 4771: 0.03675616905093193\n",
      "2017-11-11 07:37:59: Loss at step 4772: 0.036800891160964966\n",
      "2017-11-11 07:38:01: Loss at step 4773: 0.03679153695702553\n",
      "2017-11-11 07:38:03: Loss at step 4774: 0.03679487481713295\n",
      "2017-11-11 07:38:05: Loss at step 4775: 0.03675771504640579\n",
      "2017-11-11 07:38:07: Loss at step 4776: 0.03675011545419693\n",
      "2017-11-11 07:38:09: Loss at step 4777: 0.03678559139370918\n",
      "2017-11-11 07:38:12: Loss at step 4778: 0.03671516850590706\n",
      "2017-11-11 07:38:14: Loss at step 4779: 0.03676397725939751\n",
      "2017-11-11 07:38:16: Loss at step 4780: 0.03674990311264992\n",
      "2017-11-11 07:38:18: Loss at step 4781: 0.03674597293138504\n",
      "2017-11-11 07:38:20: Loss at step 4782: 0.036874376237392426\n",
      "2017-11-11 07:38:22: Loss at step 4783: 0.03675735741853714\n",
      "2017-11-11 07:38:25: Loss at step 4784: 0.03685061261057854\n",
      "2017-11-11 07:38:27: Loss at step 4785: 0.036773186177015305\n",
      "2017-11-11 07:38:29: Loss at step 4786: 0.036826856434345245\n",
      "2017-11-11 07:38:31: Loss at step 4787: 0.03673151135444641\n",
      "2017-11-11 07:38:34: Loss at step 4788: 0.03681536018848419\n",
      "2017-11-11 07:38:36: Loss at step 4789: 0.036774687469005585\n",
      "2017-11-11 07:38:38: Loss at step 4790: 0.036767084151506424\n",
      "2017-11-11 07:38:41: Loss at step 4791: 0.03668535128235817\n",
      "2017-11-11 07:38:43: Loss at step 4792: 0.036803748458623886\n",
      "2017-11-11 07:38:45: Loss at step 4793: 0.03681716322898865\n",
      "2017-11-11 07:38:47: Loss at step 4794: 0.036781150847673416\n",
      "2017-11-11 07:38:49: Loss at step 4795: 0.036722809076309204\n",
      "2017-11-11 07:38:52: Loss at step 4796: 0.03680019453167915\n",
      "2017-11-11 07:38:54: Loss at step 4797: 0.03675105795264244\n",
      "2017-11-11 07:38:56: Loss at step 4798: 0.03677374869585037\n",
      "2017-11-11 07:38:58: Loss at step 4799: 0.03683428838849068\n",
      "2017-11-11 07:39:00: Loss at step 4800: 0.03679754212498665\n",
      "2017-11-11 07:39:03: Loss at step 4801: 0.0368218868970871\n",
      "2017-11-11 07:39:05: Loss at step 4802: 0.03674835339188576\n",
      "2017-11-11 07:39:07: Loss at step 4803: 0.03677763417363167\n",
      "2017-11-11 07:39:09: Loss at step 4804: 0.03669029474258423\n",
      "2017-11-11 07:39:11: Loss at step 4805: 0.03678899630904198\n",
      "2017-11-11 07:39:13: Loss at step 4806: 0.036816343665122986\n",
      "2017-11-11 07:39:15: Loss at step 4807: 0.03675328940153122\n",
      "2017-11-11 07:39:18: Loss at step 4808: 0.0367337241768837\n",
      "2017-11-11 07:39:20: Loss at step 4809: 0.03677104040980339\n",
      "2017-11-11 07:39:22: Loss at step 4810: 0.03671812266111374\n",
      "2017-11-11 07:39:24: Loss at step 4811: 0.036744531244039536\n",
      "2017-11-11 07:39:26: Loss at step 4812: 0.03679553046822548\n",
      "2017-11-11 07:39:28: Loss at step 4813: 0.03677760064601898\n",
      "2017-11-11 07:39:30: Loss at step 4814: 0.036677036434412\n",
      "2017-11-11 07:39:33: Loss at step 4815: 0.036767009645700455\n",
      "2017-11-11 07:39:35: Loss at step 4816: 0.036728013306856155\n",
      "2017-11-11 07:39:37: Loss at step 4817: 0.03676071763038635\n",
      "2017-11-11 07:39:39: Loss at step 4818: 0.0366891585290432\n",
      "2017-11-11 07:39:41: Loss at step 4819: 0.03678810968995094\n",
      "2017-11-11 07:39:44: Loss at step 4820: 0.0367511510848999\n",
      "2017-11-11 07:39:46: Loss at step 4821: 0.03666018694639206\n",
      "2017-11-11 07:39:48: Loss at step 4822: 0.036738768219947815\n",
      "2017-11-11 07:39:50: Loss at step 4823: 0.036809489130973816\n",
      "2017-11-11 07:39:52: Loss at step 4824: 0.036823827773332596\n",
      "2017-11-11 07:39:54: Loss at step 4825: 0.03674805164337158\n",
      "2017-11-11 07:39:57: Loss at step 4826: 0.03682003170251846\n",
      "2017-11-11 07:39:59: Loss at step 4827: 0.03681058809161186\n",
      "2017-11-11 07:40:01: Loss at step 4828: 0.03674599528312683\n",
      "2017-11-11 07:40:03: Loss at step 4829: 0.03674989193677902\n",
      "2017-11-11 07:40:05: Loss at step 4830: 0.03672247380018234\n",
      "2017-11-11 07:40:07: Loss at step 4831: 0.03674780949950218\n",
      "2017-11-11 07:40:10: Loss at step 4832: 0.03681570664048195\n",
      "2017-11-11 07:40:12: Loss at step 4833: 0.03675299510359764\n",
      "2017-11-11 07:40:14: Loss at step 4834: 0.03667990118265152\n",
      "2017-11-11 07:40:16: Loss at step 4835: 0.036835651844739914\n",
      "2017-11-11 07:40:18: Loss at step 4836: 0.03678419068455696\n",
      "2017-11-11 07:40:20: Loss at step 4837: 0.03683333098888397\n",
      "2017-11-11 07:40:23: Loss at step 4838: 0.0367116741836071\n",
      "2017-11-11 07:40:25: Loss at step 4839: 0.036678362637758255\n",
      "2017-11-11 07:40:27: Loss at step 4840: 0.036765724420547485\n",
      "2017-11-11 07:40:29: Loss at step 4841: 0.036751747131347656\n",
      "2017-11-11 07:40:31: Loss at step 4842: 0.03674374520778656\n",
      "2017-11-11 07:40:33: Loss at step 4843: 0.03676528483629227\n",
      "2017-11-11 07:40:36: Loss at step 4844: 0.036778856068849564\n",
      "2017-11-11 07:40:38: Loss at step 4845: 0.03679027035832405\n",
      "2017-11-11 07:40:40: Loss at step 4846: 0.03682228550314903\n",
      "2017-11-11 07:40:42: Loss at step 4847: 0.036764197051525116\n",
      "2017-11-11 07:40:44: Loss at step 4848: 0.03669331967830658\n",
      "2017-11-11 07:40:46: Loss at step 4849: 0.03673778846859932\n",
      "2017-11-11 07:40:49: Loss at step 4850: 0.03684421256184578\n",
      "2017-11-11 07:40:51: Loss at step 4851: 0.03672739118337631\n",
      "2017-11-11 07:40:53: Loss at step 4852: 0.03664349764585495\n",
      "2017-11-11 07:40:55: Loss at step 4853: 0.03670366853475571\n",
      "2017-11-11 07:40:58: Loss at step 4854: 0.03682384639978409\n",
      "2017-11-11 07:41:00: Loss at step 4855: 0.03686060011386871\n",
      "2017-11-11 07:41:02: Loss at step 4856: 0.036819517612457275\n",
      "2017-11-11 07:41:04: Loss at step 4857: 0.036773502826690674\n",
      "2017-11-11 07:41:06: Loss at step 4858: 0.036741893738508224\n",
      "2017-11-11 07:41:08: Loss at step 4859: 0.036762550473213196\n",
      "2017-11-11 07:41:11: Loss at step 4860: 0.03680881857872009\n",
      "2017-11-11 07:41:13: Loss at step 4861: 0.036798521876335144\n",
      "2017-11-11 07:41:15: Loss at step 4862: 0.03673770651221275\n",
      "2017-11-11 07:41:17: Loss at step 4863: 0.03671625256538391\n",
      "2017-11-11 07:41:19: Loss at step 4864: 0.03682883456349373\n",
      "2017-11-11 07:41:21: Loss at step 4865: 0.036796119064092636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:41:24: Loss at step 4866: 0.03675248846411705\n",
      "2017-11-11 07:41:26: Loss at step 4867: 0.03673023357987404\n",
      "2017-11-11 07:41:28: Loss at step 4868: 0.03673931583762169\n",
      "2017-11-11 07:41:30: Loss at step 4869: 0.0367916002869606\n",
      "2017-11-11 07:41:32: Loss at step 4870: 0.03674592450261116\n",
      "2017-11-11 07:41:34: Loss at step 4871: 0.0367649644613266\n",
      "2017-11-11 07:41:36: Loss at step 4872: 0.036749791353940964\n",
      "2017-11-11 07:41:39: Loss at step 4873: 0.036732017993927\n",
      "2017-11-11 07:41:41: Loss at step 4874: 0.03671158850193024\n",
      "2017-11-11 07:41:43: Loss at step 4875: 0.03679746389389038\n",
      "2017-11-11 07:41:45: Loss at step 4876: 0.036800824105739594\n",
      "2017-11-11 07:41:47: Loss at step 4877: 0.036812543869018555\n",
      "2017-11-11 07:41:49: Loss at step 4878: 0.03680940717458725\n",
      "2017-11-11 07:41:51: Loss at step 4879: 0.03672867268323898\n",
      "2017-11-11 07:41:54: Loss at step 4880: 0.03678452596068382\n",
      "2017-11-11 07:41:56: Loss at step 4881: 0.03681967034935951\n",
      "2017-11-11 07:41:58: Loss at step 4882: 0.036773353815078735\n",
      "2017-11-11 07:42:00: Loss at step 4883: 0.03674505650997162\n",
      "2017-11-11 07:42:02: Loss at step 4884: 0.036734797060489655\n",
      "2017-11-11 07:42:04: Loss at step 4885: 0.036733467131853104\n",
      "2017-11-11 07:42:07: Loss at step 4886: 0.03677205741405487\n",
      "2017-11-11 07:42:09: Loss at step 4887: 0.03683919459581375\n",
      "2017-11-11 07:42:11: Loss at step 4888: 0.03677283227443695\n",
      "2017-11-11 07:42:13: Loss at step 4889: 0.03674730658531189\n",
      "2017-11-11 07:42:15: Loss at step 4890: 0.036749206483364105\n",
      "2017-11-11 07:42:17: Loss at step 4891: 0.03678089380264282\n",
      "2017-11-11 07:42:19: Loss at step 4892: 0.03683725744485855\n",
      "2017-11-11 07:42:22: Loss at step 4893: 0.036774829030036926\n",
      "2017-11-11 07:42:24: Loss at step 4894: 0.03683001920580864\n",
      "2017-11-11 07:42:26: Loss at step 4895: 0.036872126162052155\n",
      "2017-11-11 07:42:28: Loss at step 4896: 0.036771561950445175\n",
      "2017-11-11 07:42:30: Loss at step 4897: 0.03682754561305046\n",
      "2017-11-11 07:42:32: Loss at step 4898: 0.0367523692548275\n",
      "2017-11-11 07:42:35: Loss at step 4899: 0.036848656833171844\n",
      "2017-11-11 07:42:37: Loss at step 4900: 0.03683391585946083\n",
      "2017-11-11 07:42:39: Loss at step 4901: 0.036776959896087646\n",
      "2017-11-11 07:42:41: Loss at step 4902: 0.03679856285452843\n",
      "2017-11-11 07:42:43: Loss at step 4903: 0.03680715337395668\n",
      "2017-11-11 07:42:45: Loss at step 4904: 0.036855317652225494\n",
      "2017-11-11 07:42:47: Loss at step 4905: 0.03670930489897728\n",
      "2017-11-11 07:42:49: Loss at step 4906: 0.03676500543951988\n",
      "2017-11-11 07:42:52: Loss at step 4907: 0.036712322384119034\n",
      "2017-11-11 07:42:54: Loss at step 4908: 0.03676550090312958\n",
      "2017-11-11 07:42:56: Loss at step 4909: 0.03683933988213539\n",
      "2017-11-11 07:42:58: Loss at step 4910: 0.03683175519108772\n",
      "2017-11-11 07:43:00: Loss at step 4911: 0.03680306673049927\n",
      "2017-11-11 07:43:02: Loss at step 4912: 0.03680465742945671\n",
      "2017-11-11 07:43:05: Loss at step 4913: 0.03676123172044754\n",
      "2017-11-11 07:43:07: Loss at step 4914: 0.03673972934484482\n",
      "2017-11-11 07:43:09: Loss at step 4915: 0.03670359402894974\n",
      "2017-11-11 07:43:11: Loss at step 4916: 0.03673246502876282\n",
      "2017-11-11 07:43:13: Loss at step 4917: 0.036858100444078445\n",
      "2017-11-11 07:43:15: Loss at step 4918: 0.036780405789613724\n",
      "2017-11-11 07:43:17: Loss at step 4919: 0.03679969534277916\n",
      "2017-11-11 07:43:20: Loss at step 4920: 0.03662960231304169\n",
      "2017-11-11 07:43:22: Loss at step 4921: 0.03678391873836517\n",
      "2017-11-11 07:43:24: Loss at step 4922: 0.036795176565647125\n",
      "2017-11-11 07:43:26: Loss at step 4923: 0.03682904690504074\n",
      "2017-11-11 07:43:28: Loss at step 4924: 0.03678237274289131\n",
      "2017-11-11 07:43:30: Loss at step 4925: 0.03676888346672058\n",
      "2017-11-11 07:43:32: Loss at step 4926: 0.03668069466948509\n",
      "2017-11-11 07:43:35: Loss at step 4927: 0.03673537075519562\n",
      "2017-11-11 07:43:37: Loss at step 4928: 0.03681615740060806\n",
      "2017-11-11 07:43:39: Loss at step 4929: 0.036853283643722534\n",
      "2017-11-11 07:43:41: Loss at step 4930: 0.03684278205037117\n",
      "2017-11-11 07:43:43: Loss at step 4931: 0.036794159561395645\n",
      "2017-11-11 07:43:46: Loss at step 4932: 0.03681926801800728\n",
      "2017-11-11 07:43:48: Loss at step 4933: 0.03672809526324272\n",
      "2017-11-11 07:43:50: Loss at step 4934: 0.03671850636601448\n",
      "2017-11-11 07:43:52: Loss at step 4935: 0.03675739839673042\n",
      "2017-11-11 07:43:55: Loss at step 4936: 0.036789774894714355\n",
      "2017-11-11 07:43:57: Loss at step 4937: 0.03676668182015419\n",
      "2017-11-11 07:43:59: Loss at step 4938: 0.03676663339138031\n",
      "2017-11-11 07:44:02: Loss at step 4939: 0.03679635748267174\n",
      "2017-11-11 07:44:04: Loss at step 4940: 0.03675408288836479\n",
      "2017-11-11 07:44:06: Loss at step 4941: 0.03677836060523987\n",
      "2017-11-11 07:44:09: Loss at step 4942: 0.03674590587615967\n",
      "2017-11-11 07:44:11: Loss at step 4943: 0.036821234971284866\n",
      "2017-11-11 07:44:13: Loss at step 4944: 0.03683086112141609\n",
      "2017-11-11 07:44:16: Loss at step 4945: 0.036803364753723145\n",
      "2017-11-11 07:44:18: Loss at step 4946: 0.03683839738368988\n",
      "2017-11-11 07:44:20: Loss at step 4947: 0.03668021410703659\n",
      "2017-11-11 07:44:22: Loss at step 4948: 0.036674682050943375\n",
      "2017-11-11 07:44:25: Loss at step 4949: 0.036707837134599686\n",
      "2017-11-11 07:44:27: Loss at step 4950: 0.0367249920964241\n",
      "2017-11-11 07:44:29: Loss at step 4951: 0.03673351928591728\n",
      "2017-11-11 07:44:31: Loss at step 4952: 0.03686566650867462\n",
      "2017-11-11 07:44:33: Loss at step 4953: 0.036832068115472794\n",
      "2017-11-11 07:44:35: Loss at step 4954: 0.03678547963500023\n",
      "2017-11-11 07:44:37: Loss at step 4955: 0.0367748998105526\n",
      "2017-11-11 07:44:40: Loss at step 4956: 0.036745838820934296\n",
      "2017-11-11 07:44:42: Loss at step 4957: 0.03683645278215408\n",
      "2017-11-11 07:44:44: Loss at step 4958: 0.03692071884870529\n",
      "2017-11-11 07:44:46: Loss at step 4959: 0.03684001415967941\n",
      "2017-11-11 07:44:48: Loss at step 4960: 0.0367913581430912\n",
      "2017-11-11 07:44:51: Loss at step 4961: 0.036797188222408295\n",
      "2017-11-11 07:44:53: Loss at step 4962: 0.03677823767066002\n",
      "2017-11-11 07:44:55: Loss at step 4963: 0.036753032356500626\n",
      "2017-11-11 07:44:58: Loss at step 4964: 0.03660571947693825\n",
      "2017-11-11 07:45:00: Loss at step 4965: 0.036768704652786255\n",
      "2017-11-11 07:45:02: Loss at step 4966: 0.03682398051023483\n",
      "2017-11-11 07:45:05: Loss at step 4967: 0.0367596298456192\n",
      "2017-11-11 07:45:07: Loss at step 4968: 0.03685026988387108\n",
      "2017-11-11 07:45:09: Loss at step 4969: 0.03676825016736984\n",
      "2017-11-11 07:45:11: Loss at step 4970: 0.0368000790476799\n",
      "2017-11-11 07:45:13: Loss at step 4971: 0.03675470128655434\n",
      "2017-11-11 07:45:15: Loss at step 4972: 0.03679700568318367\n",
      "2017-11-11 07:45:18: Loss at step 4973: 0.03686247020959854\n",
      "2017-11-11 07:45:20: Loss at step 4974: 0.03671201318502426\n",
      "2017-11-11 07:45:22: Loss at step 4975: 0.03679918125271797\n",
      "2017-11-11 07:45:24: Loss at step 4976: 0.03677824139595032\n",
      "2017-11-11 07:45:27: Loss at step 4977: 0.03682582825422287\n",
      "2017-11-11 07:45:29: Loss at step 4978: 0.03669826686382294\n",
      "2017-11-11 07:45:31: Loss at step 4979: 0.036817487329244614\n",
      "2017-11-11 07:45:34: Loss at step 4980: 0.03671420365571976\n",
      "2017-11-11 07:45:36: Loss at step 4981: 0.036822859197854996\n",
      "2017-11-11 07:45:38: Loss at step 4982: 0.03666537627577782\n",
      "2017-11-11 07:45:40: Loss at step 4983: 0.03667778894305229\n",
      "2017-11-11 07:45:42: Loss at step 4984: 0.03686811402440071\n",
      "2017-11-11 07:45:45: Loss at step 4985: 0.036748625338077545\n",
      "2017-11-11 07:45:47: Loss at step 4986: 0.03680937737226486\n",
      "2017-11-11 07:45:49: Loss at step 4987: 0.03684191778302193\n",
      "2017-11-11 07:45:51: Loss at step 4988: 0.03680190443992615\n",
      "2017-11-11 07:45:53: Loss at step 4989: 0.03678010776638985\n",
      "2017-11-11 07:45:55: Loss at step 4990: 0.03672683238983154\n",
      "2017-11-11 07:45:58: Loss at step 4991: 0.03679131716489792\n",
      "2017-11-11 07:46:00: Loss at step 4992: 0.03684132918715477\n",
      "2017-11-11 07:46:02: Loss at step 4993: 0.036738649010658264\n",
      "2017-11-11 07:46:04: Loss at step 4994: 0.036676470190286636\n",
      "2017-11-11 07:46:06: Loss at step 4995: 0.03676220774650574\n",
      "2017-11-11 07:46:09: Loss at step 4996: 0.03678453341126442\n",
      "2017-11-11 07:46:11: Loss at step 4997: 0.03689638525247574\n",
      "2017-11-11 07:46:13: Loss at step 4998: 0.036752428859472275\n",
      "2017-11-11 07:46:15: Loss at step 4999: 0.03683314099907875\n",
      "2017-11-11 07:46:17: Loss at step 5000: 0.03675755113363266\n",
      "2017-11-11 07:46:19: Loss at step 5001: 0.03681758791208267\n",
      "2017-11-11 07:46:22: Loss at step 5002: 0.036730654537677765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:46:24: Loss at step 5003: 0.03663784638047218\n",
      "2017-11-11 07:46:26: Loss at step 5004: 0.03676796704530716\n",
      "2017-11-11 07:46:28: Loss at step 5005: 0.03672107681632042\n",
      "2017-11-11 07:46:30: Loss at step 5006: 0.03670649230480194\n",
      "2017-11-11 07:46:32: Loss at step 5007: 0.03683760389685631\n",
      "2017-11-11 07:46:35: Loss at step 5008: 0.0367915965616703\n",
      "2017-11-11 07:46:37: Loss at step 5009: 0.0368579626083374\n",
      "2017-11-11 07:46:39: Loss at step 5010: 0.03677348792552948\n",
      "2017-11-11 07:46:41: Loss at step 5011: 0.03686247393488884\n",
      "2017-11-11 07:46:43: Loss at step 5012: 0.036813411861658096\n",
      "2017-11-11 07:46:45: Loss at step 5013: 0.036714885383844376\n",
      "2017-11-11 07:46:48: Loss at step 5014: 0.03671903535723686\n",
      "2017-11-11 07:46:50: Loss at step 5015: 0.036780886352062225\n",
      "2017-11-11 07:46:52: Loss at step 5016: 0.036802131682634354\n",
      "2017-11-11 07:46:54: Loss at step 5017: 0.0368625745177269\n",
      "2017-11-11 07:46:56: Loss at step 5018: 0.036751888692379\n",
      "2017-11-11 07:46:58: Loss at step 5019: 0.03691001608967781\n",
      "2017-11-11 07:47:01: Loss at step 5020: 0.03683234378695488\n",
      "2017-11-11 07:47:03: Loss at step 5021: 0.03686688840389252\n",
      "2017-11-11 07:47:05: Loss at step 5022: 0.036801453679800034\n",
      "2017-11-11 07:47:07: Loss at step 5023: 0.03683094307780266\n",
      "2017-11-11 07:47:09: Loss at step 5024: 0.03687003254890442\n",
      "2017-11-11 07:47:11: Loss at step 5025: 0.036904290318489075\n",
      "2017-11-11 07:47:14: Loss at step 5026: 0.03680259361863136\n",
      "2017-11-11 07:47:16: Loss at step 5027: 0.03675316646695137\n",
      "2017-11-11 07:47:18: Loss at step 5028: 0.036878425627946854\n",
      "2017-11-11 07:47:20: Loss at step 5029: 0.03685261681675911\n",
      "2017-11-11 07:47:22: Loss at step 5030: 0.03680158033967018\n",
      "2017-11-11 07:47:25: Loss at step 5031: 0.036769989877939224\n",
      "2017-11-11 07:47:27: Loss at step 5032: 0.03674955293536186\n",
      "2017-11-11 07:47:29: Loss at step 5033: 0.03682948648929596\n",
      "2017-11-11 07:47:31: Loss at step 5034: 0.0367821529507637\n",
      "2017-11-11 07:47:33: Loss at step 5035: 0.03687320277094841\n",
      "2017-11-11 07:47:35: Loss at step 5036: 0.036862000823020935\n",
      "2017-11-11 07:47:38: Loss at step 5037: 0.03675922751426697\n",
      "2017-11-11 07:47:40: Loss at step 5038: 0.03681240975856781\n",
      "2017-11-11 07:47:42: Loss at step 5039: 0.036771539598703384\n",
      "2017-11-11 07:47:44: Loss at step 5040: 0.0367974191904068\n",
      "2017-11-11 07:47:46: Loss at step 5041: 0.03686060756444931\n",
      "2017-11-11 07:47:48: Loss at step 5042: 0.036795422434806824\n",
      "2017-11-11 07:47:51: Loss at step 5043: 0.03670454025268555\n",
      "2017-11-11 07:47:53: Loss at step 5044: 0.03680867329239845\n",
      "2017-11-11 07:47:55: Loss at step 5045: 0.03675086796283722\n",
      "2017-11-11 07:47:57: Loss at step 5046: 0.0367828905582428\n",
      "2017-11-11 07:47:59: Loss at step 5047: 0.036754362285137177\n",
      "2017-11-11 07:48:01: Loss at step 5048: 0.03681207820773125\n",
      "2017-11-11 07:48:03: Loss at step 5049: 0.036886245012283325\n",
      "2017-11-11 07:48:06: Loss at step 5050: 0.03690535947680473\n",
      "2017-11-11 07:48:08: Loss at step 5051: 0.03678440302610397\n",
      "2017-11-11 07:48:10: Loss at step 5052: 0.03674600273370743\n",
      "2017-11-11 07:48:12: Loss at step 5053: 0.03680277243256569\n",
      "2017-11-11 07:48:14: Loss at step 5054: 0.03682179003953934\n",
      "2017-11-11 07:48:16: Loss at step 5055: 0.03678726777434349\n",
      "2017-11-11 07:48:18: Loss at step 5056: 0.0368005596101284\n",
      "2017-11-11 07:48:21: Loss at step 5057: 0.036768946796655655\n",
      "2017-11-11 07:48:23: Loss at step 5058: 0.03685801848769188\n",
      "2017-11-11 07:48:25: Loss at step 5059: 0.03688444942235947\n",
      "2017-11-11 07:48:27: Loss at step 5060: 0.03678964078426361\n",
      "2017-11-11 07:48:29: Loss at step 5061: 0.03682177886366844\n",
      "2017-11-11 07:48:32: Loss at step 5062: 0.0368543416261673\n",
      "2017-11-11 07:48:34: Loss at step 5063: 0.03687746077775955\n",
      "2017-11-11 07:48:36: Loss at step 5064: 0.0368221178650856\n",
      "2017-11-11 07:48:38: Loss at step 5065: 0.036916583776474\n",
      "2017-11-11 07:48:40: Loss at step 5066: 0.03684884309768677\n",
      "2017-11-11 07:48:42: Loss at step 5067: 0.03689790889620781\n",
      "2017-11-11 07:48:45: Loss at step 5068: 0.03685437887907028\n",
      "2017-11-11 07:48:47: Loss at step 5069: 0.03677825257182121\n",
      "2017-11-11 07:48:49: Loss at step 5070: 0.03675519675016403\n",
      "2017-11-11 07:48:51: Loss at step 5071: 0.03677794709801674\n",
      "2017-11-11 07:48:53: Loss at step 5072: 0.03673543781042099\n",
      "2017-11-11 07:48:55: Loss at step 5073: 0.03684563934803009\n",
      "2017-11-11 07:48:58: Loss at step 5074: 0.03682190552353859\n",
      "2017-11-11 07:49:00: Loss at step 5075: 0.03670431673526764\n",
      "2017-11-11 07:49:02: Loss at step 5076: 0.03673689439892769\n",
      "2017-11-11 07:49:04: Loss at step 5077: 0.03675025701522827\n",
      "2017-11-11 07:49:06: Loss at step 5078: 0.036850787699222565\n",
      "2017-11-11 07:49:08: Loss at step 5079: 0.03677533566951752\n",
      "2017-11-11 07:49:10: Loss at step 5080: 0.036756452172994614\n",
      "2017-11-11 07:49:12: Loss at step 5081: 0.0367138534784317\n",
      "2017-11-11 07:49:15: Loss at step 5082: 0.03681287169456482\n",
      "2017-11-11 07:49:17: Loss at step 5083: 0.03677661716938019\n",
      "2017-11-11 07:49:19: Loss at step 5084: 0.03677782788872719\n",
      "2017-11-11 07:49:21: Loss at step 5085: 0.0367736741900444\n",
      "2017-11-11 07:49:23: Loss at step 5086: 0.03677834942936897\n",
      "2017-11-11 07:49:25: Loss at step 5087: 0.036806587129831314\n",
      "2017-11-11 07:49:27: Loss at step 5088: 0.03670935705304146\n",
      "2017-11-11 07:49:30: Loss at step 5089: 0.036701932549476624\n",
      "2017-11-11 07:49:32: Loss at step 5090: 0.036732371896505356\n",
      "2017-11-11 07:49:34: Loss at step 5091: 0.03674561530351639\n",
      "2017-11-11 07:49:36: Loss at step 5092: 0.03678007796406746\n",
      "2017-11-11 07:49:38: Loss at step 5093: 0.036882899701595306\n",
      "2017-11-11 07:49:40: Loss at step 5094: 0.0367274284362793\n",
      "2017-11-11 07:49:43: Loss at step 5095: 0.03680545464158058\n",
      "2017-11-11 07:49:45: Loss at step 5096: 0.03676455840468407\n",
      "2017-11-11 07:49:47: Loss at step 5097: 0.03680528327822685\n",
      "2017-11-11 07:49:49: Loss at step 5098: 0.03669578954577446\n",
      "2017-11-11 07:49:51: Loss at step 5099: 0.03663548082113266\n",
      "2017-11-11 07:49:54: Loss at step 5100: 0.036680035293102264\n",
      "2017-11-11 07:49:56: Loss at step 5101: 0.03674501180648804\n",
      "2017-11-11 07:49:58: Loss at step 5102: 0.03689168021082878\n",
      "2017-11-11 07:50:00: Loss at step 5103: 0.03677884489297867\n",
      "2017-11-11 07:50:02: Loss at step 5104: 0.036792706698179245\n",
      "2017-11-11 07:50:04: Loss at step 5105: 0.03675127401947975\n",
      "2017-11-11 07:50:07: Loss at step 5106: 0.03680509701371193\n",
      "2017-11-11 07:50:09: Loss at step 5107: 0.03678639978170395\n",
      "2017-11-11 07:50:11: Loss at step 5108: 0.0368649885058403\n",
      "2017-11-11 07:50:13: Loss at step 5109: 0.03685631603002548\n",
      "2017-11-11 07:50:15: Loss at step 5110: 0.036774348467588425\n",
      "2017-11-11 07:50:17: Loss at step 5111: 0.03679172322154045\n",
      "2017-11-11 07:50:20: Loss at step 5112: 0.03666526824235916\n",
      "2017-11-11 07:50:22: Loss at step 5113: 0.036827366799116135\n",
      "2017-11-11 07:50:24: Loss at step 5114: 0.0367591567337513\n",
      "2017-11-11 07:50:26: Loss at step 5115: 0.03679918870329857\n",
      "2017-11-11 07:50:28: Loss at step 5116: 0.036810778081417084\n",
      "2017-11-11 07:50:30: Loss at step 5117: 0.03667616844177246\n",
      "2017-11-11 07:50:33: Loss at step 5118: 0.03673024848103523\n",
      "2017-11-11 07:50:35: Loss at step 5119: 0.03676775097846985\n",
      "2017-11-11 07:50:37: Loss at step 5120: 0.03685936704277992\n",
      "2017-11-11 07:50:39: Loss at step 5121: 0.036793727427721024\n",
      "2017-11-11 07:50:41: Loss at step 5122: 0.0367271713912487\n",
      "2017-11-11 07:50:43: Loss at step 5123: 0.03678005933761597\n",
      "2017-11-11 07:50:45: Loss at step 5124: 0.03679034858942032\n",
      "2017-11-11 07:50:48: Loss at step 5125: 0.03677871823310852\n",
      "2017-11-11 07:50:50: Loss at step 5126: 0.03673788905143738\n",
      "2017-11-11 07:50:52: Loss at step 5127: 0.036760199815034866\n",
      "2017-11-11 07:50:54: Loss at step 5128: 0.036775246262550354\n",
      "2017-11-11 07:50:56: Loss at step 5129: 0.03683824464678764\n",
      "2017-11-11 07:50:58: Loss at step 5130: 0.03680604696273804\n",
      "2017-11-11 07:51:01: Loss at step 5131: 0.036714568734169006\n",
      "2017-11-11 07:51:03: Loss at step 5132: 0.036875203251838684\n",
      "2017-11-11 07:51:05: Loss at step 5133: 0.03685207664966583\n",
      "2017-11-11 07:51:07: Loss at step 5134: 0.036703236401081085\n",
      "2017-11-11 07:51:09: Loss at step 5135: 0.03676922619342804\n",
      "2017-11-11 07:51:11: Loss at step 5136: 0.03676412254571915\n",
      "2017-11-11 07:51:13: Loss at step 5137: 0.03681761026382446\n",
      "2017-11-11 07:51:16: Loss at step 5138: 0.036801502108573914\n",
      "2017-11-11 07:51:18: Loss at step 5139: 0.03677431493997574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:51:20: Loss at step 5140: 0.03676080331206322\n",
      "2017-11-11 07:51:22: Loss at step 5141: 0.03677816316485405\n",
      "2017-11-11 07:51:24: Loss at step 5142: 0.03675081208348274\n",
      "2017-11-11 07:51:26: Loss at step 5143: 0.03673573583364487\n",
      "2017-11-11 07:51:28: Loss at step 5144: 0.03682028129696846\n",
      "2017-11-11 07:51:31: Loss at step 5145: 0.03674256056547165\n",
      "2017-11-11 07:51:33: Loss at step 5146: 0.03673744574189186\n",
      "2017-11-11 07:51:35: Loss at step 5147: 0.03672629967331886\n",
      "2017-11-11 07:51:37: Loss at step 5148: 0.036629028618335724\n",
      "2017-11-11 07:51:39: Loss at step 5149: 0.03674600273370743\n",
      "2017-11-11 07:51:41: Loss at step 5150: 0.03682996705174446\n",
      "2017-11-11 07:51:43: Loss at step 5151: 0.03682280704379082\n",
      "2017-11-11 07:51:46: Loss at step 5152: 0.03673088550567627\n",
      "2017-11-11 07:51:48: Loss at step 5153: 0.036701176315546036\n",
      "2017-11-11 07:51:50: Loss at step 5154: 0.036726947873830795\n",
      "2017-11-11 07:51:52: Loss at step 5155: 0.03685373812913895\n",
      "2017-11-11 07:51:54: Loss at step 5156: 0.036842938512563705\n",
      "2017-11-11 07:51:56: Loss at step 5157: 0.036693401634693146\n",
      "2017-11-11 07:51:58: Loss at step 5158: 0.03678019344806671\n",
      "2017-11-11 07:52:01: Loss at step 5159: 0.03678928688168526\n",
      "2017-11-11 07:52:03: Loss at step 5160: 0.03687305748462677\n",
      "2017-11-11 07:52:05: Loss at step 5161: 0.036724723875522614\n",
      "2017-11-11 07:52:07: Loss at step 5162: 0.036746323108673096\n",
      "2017-11-11 07:52:09: Loss at step 5163: 0.03680472448468208\n",
      "2017-11-11 07:52:11: Loss at step 5164: 0.036757610738277435\n",
      "2017-11-11 07:52:13: Loss at step 5165: 0.03684238716959953\n",
      "2017-11-11 07:52:16: Loss at step 5166: 0.036731794476509094\n",
      "2017-11-11 07:52:18: Loss at step 5167: 0.03684006631374359\n",
      "2017-11-11 07:52:20: Loss at step 5168: 0.03673046827316284\n",
      "2017-11-11 07:52:22: Loss at step 5169: 0.036754705011844635\n",
      "2017-11-11 07:52:24: Loss at step 5170: 0.03680592402815819\n",
      "2017-11-11 07:52:26: Loss at step 5171: 0.03672144189476967\n",
      "2017-11-11 07:52:29: Loss at step 5172: 0.03675726801156998\n",
      "2017-11-11 07:52:31: Loss at step 5173: 0.036781128495931625\n",
      "2017-11-11 07:52:33: Loss at step 5174: 0.03682240471243858\n",
      "2017-11-11 07:52:35: Loss at step 5175: 0.036770258098840714\n",
      "2017-11-11 07:52:37: Loss at step 5176: 0.03677932918071747\n",
      "2017-11-11 07:52:39: Loss at step 5177: 0.0367584303021431\n",
      "2017-11-11 07:52:41: Loss at step 5178: 0.03668887913227081\n",
      "2017-11-11 07:52:44: Loss at step 5179: 0.0367378368973732\n",
      "2017-11-11 07:52:46: Loss at step 5180: 0.03674731403589249\n",
      "2017-11-11 07:52:48: Loss at step 5181: 0.0367777943611145\n",
      "2017-11-11 07:52:50: Loss at step 5182: 0.03681965172290802\n",
      "2017-11-11 07:52:52: Loss at step 5183: 0.03678678348660469\n",
      "2017-11-11 07:52:54: Loss at step 5184: 0.036837294697761536\n",
      "2017-11-11 07:52:56: Loss at step 5185: 0.036737389862537384\n",
      "2017-11-11 07:52:58: Loss at step 5186: 0.0368223562836647\n",
      "2017-11-11 07:53:01: Loss at step 5187: 0.036719661206007004\n",
      "2017-11-11 07:53:03: Loss at step 5188: 0.03678077459335327\n",
      "2017-11-11 07:53:05: Loss at step 5189: 0.03678738325834274\n",
      "2017-11-11 07:53:07: Loss at step 5190: 0.03678801283240318\n",
      "2017-11-11 07:53:09: Loss at step 5191: 0.03669733926653862\n",
      "2017-11-11 07:53:11: Loss at step 5192: 0.03677421435713768\n",
      "2017-11-11 07:53:13: Loss at step 5193: 0.03679007664322853\n",
      "2017-11-11 07:53:16: Loss at step 5194: 0.03674119710922241\n",
      "2017-11-11 07:53:18: Loss at step 5195: 0.036837611347436905\n",
      "2017-11-11 07:53:20: Loss at step 5196: 0.03675580397248268\n",
      "2017-11-11 07:53:22: Loss at step 5197: 0.03680470213294029\n",
      "2017-11-11 07:53:24: Loss at step 5198: 0.03679472580552101\n",
      "2017-11-11 07:53:26: Loss at step 5199: 0.03673947602510452\n",
      "2017-11-11 07:53:28: Loss at step 5200: 0.03674804046750069\n",
      "2017-11-11 07:53:31: Loss at step 5201: 0.03677479177713394\n",
      "2017-11-11 07:53:33: Loss at step 5202: 0.03671235591173172\n",
      "2017-11-11 07:53:35: Loss at step 5203: 0.036630649119615555\n",
      "2017-11-11 07:53:37: Loss at step 5204: 0.036679960787296295\n",
      "2017-11-11 07:53:39: Loss at step 5205: 0.036795783787965775\n",
      "2017-11-11 07:53:41: Loss at step 5206: 0.03672035411000252\n",
      "2017-11-11 07:53:43: Loss at step 5207: 0.036820124834775925\n",
      "2017-11-11 07:53:46: Loss at step 5208: 0.03673258423805237\n",
      "2017-11-11 07:53:48: Loss at step 5209: 0.036767736077308655\n",
      "2017-11-11 07:53:50: Loss at step 5210: 0.03682288900017738\n",
      "2017-11-11 07:53:52: Loss at step 5211: 0.03678444027900696\n",
      "2017-11-11 07:53:54: Loss at step 5212: 0.03679046034812927\n",
      "2017-11-11 07:53:56: Loss at step 5213: 0.03665817156434059\n",
      "2017-11-11 07:53:58: Loss at step 5214: 0.03671746701002121\n",
      "2017-11-11 07:54:01: Loss at step 5215: 0.03668820112943649\n",
      "2017-11-11 07:54:03: Loss at step 5216: 0.0366329699754715\n",
      "2017-11-11 07:54:05: Loss at step 5217: 0.036746446043252945\n",
      "2017-11-11 07:54:07: Loss at step 5218: 0.03681027516722679\n",
      "2017-11-11 07:54:09: Loss at step 5219: 0.03678829222917557\n",
      "2017-11-11 07:54:11: Loss at step 5220: 0.03683020547032356\n",
      "2017-11-11 07:54:13: Loss at step 5221: 0.036766886711120605\n",
      "2017-11-11 07:54:16: Loss at step 5222: 0.03675388917326927\n",
      "2017-11-11 07:54:18: Loss at step 5223: 0.036767251789569855\n",
      "2017-11-11 07:54:20: Loss at step 5224: 0.03674020618200302\n",
      "2017-11-11 07:54:22: Loss at step 5225: 0.0367463119328022\n",
      "2017-11-11 07:54:24: Loss at step 5226: 0.036774568259716034\n",
      "2017-11-11 07:54:26: Loss at step 5227: 0.03674112632870674\n",
      "2017-11-11 07:54:29: Loss at step 5228: 0.03679198771715164\n",
      "2017-11-11 07:54:31: Loss at step 5229: 0.03683701902627945\n",
      "2017-11-11 07:54:33: Loss at step 5230: 0.036791328340768814\n",
      "2017-11-11 07:54:35: Loss at step 5231: 0.0367782786488533\n",
      "2017-11-11 07:54:37: Loss at step 5232: 0.03676241636276245\n",
      "2017-11-11 07:54:39: Loss at step 5233: 0.03668997809290886\n",
      "2017-11-11 07:54:41: Loss at step 5234: 0.03679346665740013\n",
      "2017-11-11 07:54:44: Loss at step 5235: 0.03670653700828552\n",
      "2017-11-11 07:54:46: Loss at step 5236: 0.036785926669836044\n",
      "2017-11-11 07:54:48: Loss at step 5237: 0.03674128279089928\n",
      "2017-11-11 07:54:50: Loss at step 5238: 0.036740049719810486\n",
      "2017-11-11 07:54:52: Loss at step 5239: 0.03668208047747612\n",
      "2017-11-11 07:54:54: Loss at step 5240: 0.036758776754140854\n",
      "2017-11-11 07:54:56: Loss at step 5241: 0.03678639978170395\n",
      "2017-11-11 07:54:59: Loss at step 5242: 0.036723699420690536\n",
      "2017-11-11 07:55:01: Loss at step 5243: 0.03673157840967178\n",
      "2017-11-11 07:55:03: Loss at step 5244: 0.03671225532889366\n",
      "2017-11-11 07:55:05: Loss at step 5245: 0.036789778620004654\n",
      "2017-11-11 07:55:07: Loss at step 5246: 0.03680208697915077\n",
      "2017-11-11 07:55:09: Loss at step 5247: 0.03667474538087845\n",
      "2017-11-11 07:55:11: Loss at step 5248: 0.036746807396411896\n",
      "2017-11-11 07:55:14: Loss at step 5249: 0.036741454154253006\n",
      "2017-11-11 07:55:16: Loss at step 5250: 0.036823730915784836\n",
      "2017-11-11 07:55:18: Loss at step 5251: 0.0367286391556263\n",
      "2017-11-11 07:55:20: Loss at step 5252: 0.036740925163030624\n",
      "2017-11-11 07:55:22: Loss at step 5253: 0.036825016140937805\n",
      "2017-11-11 07:55:24: Loss at step 5254: 0.03672533482313156\n",
      "2017-11-11 07:55:26: Loss at step 5255: 0.03678300604224205\n",
      "2017-11-11 07:55:29: Loss at step 5256: 0.03673410043120384\n",
      "2017-11-11 07:55:31: Loss at step 5257: 0.03671795874834061\n",
      "2017-11-11 07:55:33: Loss at step 5258: 0.036723144352436066\n",
      "2017-11-11 07:55:35: Loss at step 5259: 0.03677015379071236\n",
      "2017-11-11 07:55:37: Loss at step 5260: 0.03673291578888893\n",
      "2017-11-11 07:55:39: Loss at step 5261: 0.03672051802277565\n",
      "2017-11-11 07:55:41: Loss at step 5262: 0.03679068386554718\n",
      "2017-11-11 07:55:44: Loss at step 5263: 0.03670056164264679\n",
      "2017-11-11 07:55:46: Loss at step 5264: 0.03686020150780678\n",
      "2017-11-11 07:55:48: Loss at step 5265: 0.03666955977678299\n",
      "2017-11-11 07:55:50: Loss at step 5266: 0.036760203540325165\n",
      "2017-11-11 07:55:52: Loss at step 5267: 0.03677933290600777\n",
      "2017-11-11 07:55:54: Loss at step 5268: 0.03674605116248131\n",
      "2017-11-11 07:55:56: Loss at step 5269: 0.0367596261203289\n",
      "2017-11-11 07:55:59: Loss at step 5270: 0.036706071346998215\n",
      "2017-11-11 07:56:01: Loss at step 5271: 0.036674827337265015\n",
      "2017-11-11 07:56:03: Loss at step 5272: 0.036813054233789444\n",
      "2017-11-11 07:56:05: Loss at step 5273: 0.0365871861577034\n",
      "2017-11-11 07:56:07: Loss at step 5274: 0.03678884357213974\n",
      "2017-11-11 07:56:10: Loss at step 5275: 0.0367046557366848\n",
      "2017-11-11 07:56:12: Loss at step 5276: 0.036798182874917984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 07:56:14: Loss at step 5277: 0.03683433309197426\n",
      "2017-11-11 07:56:16: Loss at step 5278: 0.03682652488350868\n",
      "2017-11-11 07:56:18: Loss at step 5279: 0.03677448257803917\n",
      "2017-11-11 07:56:20: Loss at step 5280: 0.03676701709628105\n",
      "2017-11-11 07:56:22: Loss at step 5281: 0.03684748709201813\n",
      "2017-11-11 07:56:25: Loss at step 5282: 0.03672650083899498\n",
      "2017-11-11 07:56:27: Loss at step 5283: 0.03678348660469055\n",
      "2017-11-11 07:56:29: Loss at step 5284: 0.03675515577197075\n",
      "2017-11-11 07:56:31: Loss at step 5285: 0.03686431050300598\n",
      "2017-11-11 07:56:33: Loss at step 5286: 0.03676925227046013\n",
      "2017-11-11 07:56:35: Loss at step 5287: 0.03671221435070038\n",
      "2017-11-11 07:56:38: Loss at step 5288: 0.03674933314323425\n",
      "2017-11-11 07:56:40: Loss at step 5289: 0.036709416657686234\n",
      "2017-11-11 07:56:42: Loss at step 5290: 0.036714378744363785\n",
      "2017-11-11 07:56:44: Loss at step 5291: 0.03668693080544472\n",
      "2017-11-11 07:56:46: Loss at step 5292: 0.03683823347091675\n",
      "2017-11-11 07:56:48: Loss at step 5293: 0.03671027719974518\n",
      "2017-11-11 07:56:50: Loss at step 5294: 0.03686247020959854\n",
      "2017-11-11 07:56:53: Loss at step 5295: 0.036905307322740555\n",
      "2017-11-11 07:56:55: Loss at step 5296: 0.036828868091106415\n",
      "2017-11-11 07:56:57: Loss at step 5297: 0.036809131503105164\n",
      "2017-11-11 07:56:59: Loss at step 5298: 0.03680337592959404\n",
      "2017-11-11 07:57:01: Loss at step 5299: 0.03683356195688248\n",
      "2017-11-11 07:57:03: Loss at step 5300: 0.03688674792647362\n",
      "2017-11-11 07:57:05: Loss at step 5301: 0.036859795451164246\n",
      "2017-11-11 07:57:08: Loss at step 5302: 0.036807168275117874\n",
      "2017-11-11 07:57:10: Loss at step 5303: 0.03676529973745346\n",
      "2017-11-11 07:57:12: Loss at step 5304: 0.036811668425798416\n",
      "2017-11-11 07:57:14: Loss at step 5305: 0.03678901493549347\n",
      "2017-11-11 07:57:16: Loss at step 5306: 0.036862559616565704\n",
      "2017-11-11 07:57:18: Loss at step 5307: 0.03682905435562134\n",
      "2017-11-11 07:57:20: Loss at step 5308: 0.03691459819674492\n",
      "2017-11-11 07:57:23: Loss at step 5309: 0.03679467737674713\n",
      "2017-11-11 07:57:25: Loss at step 5310: 0.03683741018176079\n",
      "2017-11-11 07:57:27: Loss at step 5311: 0.036798540502786636\n",
      "2017-11-11 07:57:29: Loss at step 5312: 0.03673071414232254\n",
      "2017-11-11 07:57:31: Loss at step 5313: 0.03680189326405525\n",
      "2017-11-11 07:57:33: Loss at step 5314: 0.03672581911087036\n",
      "2017-11-11 07:57:35: Loss at step 5315: 0.03676062822341919\n",
      "2017-11-11 07:57:38: Loss at step 5316: 0.036721933633089066\n",
      "2017-11-11 07:57:40: Loss at step 5317: 0.036704905331134796\n",
      "2017-11-11 07:57:42: Loss at step 5318: 0.036835551261901855\n",
      "2017-11-11 07:57:44: Loss at step 5319: 0.03674422949552536\n",
      "2017-11-11 07:57:46: Loss at step 5320: 0.03684733808040619\n",
      "2017-11-11 07:57:48: Loss at step 5321: 0.03679998964071274\n",
      "2017-11-11 07:57:50: Loss at step 5322: 0.036905501037836075\n",
      "2017-11-11 07:57:52: Loss at step 5323: 0.03677667677402496\n",
      "2017-11-11 07:57:55: Loss at step 5324: 0.03668184205889702\n",
      "2017-11-11 07:57:57: Loss at step 5325: 0.03678826987743378\n",
      "2017-11-11 07:57:59: Loss at step 5326: 0.03682888299226761\n",
      "2017-11-11 07:58:01: Loss at step 5327: 0.03680668771266937\n",
      "2017-11-11 07:58:03: Loss at step 5328: 0.03682740405201912\n",
      "2017-11-11 07:58:05: Loss at step 5329: 0.03679581731557846\n",
      "2017-11-11 07:58:07: Loss at step 5330: 0.03682710602879524\n",
      "2017-11-11 07:58:10: Loss at step 5331: 0.03680355101823807\n",
      "2017-11-11 07:58:12: Loss at step 5332: 0.036814335733652115\n",
      "2017-11-11 07:58:14: Loss at step 5333: 0.036782748997211456\n",
      "2017-11-11 07:58:16: Loss at step 5334: 0.03681540489196777\n",
      "2017-11-11 07:58:18: Loss at step 5335: 0.03671739622950554\n",
      "2017-11-11 07:58:20: Loss at step 5336: 0.03674297407269478\n",
      "2017-11-11 07:58:22: Loss at step 5337: 0.03678933531045914\n",
      "2017-11-11 07:58:25: Loss at step 5338: 0.03679101541638374\n",
      "2017-11-11 07:58:27: Loss at step 5339: 0.036832649260759354\n",
      "2017-11-11 07:58:29: Loss at step 5340: 0.036784447729587555\n",
      "2017-11-11 07:58:31: Loss at step 5341: 0.036716166883707047\n",
      "2017-11-11 07:58:33: Loss at step 5342: 0.03684326633810997\n",
      "2017-11-11 07:58:35: Loss at step 5343: 0.03682093322277069\n",
      "2017-11-11 07:58:37: Loss at step 5344: 0.036851417273283005\n",
      "2017-11-11 07:58:40: Loss at step 5345: 0.03676304593682289\n",
      "2017-11-11 07:58:42: Loss at step 5346: 0.03681571036577225\n",
      "2017-11-11 07:58:44: Loss at step 5347: 0.03673115745186806\n",
      "2017-11-11 07:58:46: Loss at step 5348: 0.03673990070819855\n",
      "2017-11-11 07:58:48: Loss at step 5349: 0.036801233887672424\n",
      "2017-11-11 07:58:50: Loss at step 5350: 0.036795295774936676\n",
      "2017-11-11 07:58:52: Loss at step 5351: 0.03675414249300957\n",
      "2017-11-11 07:58:55: Loss at step 5352: 0.03672649711370468\n",
      "2017-11-11 07:58:57: Loss at step 5353: 0.03672146424651146\n",
      "2017-11-11 07:58:59: Loss at step 5354: 0.036794982850551605\n",
      "2017-11-11 07:59:01: Loss at step 5355: 0.03674481064081192\n",
      "2017-11-11 07:59:03: Loss at step 5356: 0.036884404718875885\n",
      "2017-11-11 07:59:05: Loss at step 5357: 0.03682009130716324\n",
      "2017-11-11 07:59:07: Loss at step 5358: 0.03680087998509407\n",
      "2017-11-11 07:59:09: Loss at step 5359: 0.03679465502500534\n",
      "2017-11-11 07:59:12: Loss at step 5360: 0.0368197076022625\n",
      "2017-11-11 07:59:14: Loss at step 5361: 0.03673180565237999\n",
      "2017-11-11 07:59:16: Loss at step 5362: 0.036759257316589355\n",
      "2017-11-11 07:59:18: Loss at step 5363: 0.03679662570357323\n",
      "2017-11-11 07:59:20: Loss at step 5364: 0.03683863952755928\n",
      "2017-11-11 07:59:22: Loss at step 5365: 0.036849647760391235\n",
      "2017-11-11 07:59:24: Loss at step 5366: 0.03678043186664581\n",
      "2017-11-11 07:59:27: Loss at step 5367: 0.036820173263549805\n",
      "2017-11-11 07:59:29: Loss at step 5368: 0.03680432587862015\n",
      "2017-11-11 07:59:31: Loss at step 5369: 0.03670678660273552\n",
      "2017-11-11 07:59:33: Loss at step 5370: 0.03677838295698166\n",
      "2017-11-11 07:59:35: Loss at step 5371: 0.03680823743343353\n",
      "2017-11-11 07:59:37: Loss at step 5372: 0.03677165508270264\n",
      "2017-11-11 07:59:39: Loss at step 5373: 0.03684067726135254\n",
      "2017-11-11 07:59:42: Loss at step 5374: 0.036793503910303116\n",
      "2017-11-11 07:59:44: Loss at step 5375: 0.036730267107486725\n",
      "2017-11-11 07:59:46: Loss at step 5376: 0.03683202341198921\n",
      "2017-11-11 07:59:48: Loss at step 5377: 0.03679719939827919\n",
      "2017-11-11 07:59:50: Loss at step 5378: 0.0368865467607975\n",
      "2017-11-11 07:59:52: Loss at step 5379: 0.03679286316037178\n",
      "2017-11-11 07:59:54: Loss at step 5380: 0.036799389868974686\n",
      "2017-11-11 07:59:57: Loss at step 5381: 0.03684346377849579\n",
      "2017-11-11 07:59:59: Loss at step 5382: 0.03677287697792053\n",
      "2017-11-11 08:00:01: Loss at step 5383: 0.03683941811323166\n",
      "2017-11-11 08:00:03: Loss at step 5384: 0.03672851249575615\n",
      "2017-11-11 08:00:05: Loss at step 5385: 0.03685488551855087\n",
      "2017-11-11 08:00:07: Loss at step 5386: 0.03673338145017624\n",
      "2017-11-11 08:00:09: Loss at step 5387: 0.03674064576625824\n",
      "2017-11-11 08:00:12: Loss at step 5388: 0.03679120913147926\n",
      "2017-11-11 08:00:14: Loss at step 5389: 0.03680430352687836\n",
      "2017-11-11 08:00:16: Loss at step 5390: 0.03671708703041077\n",
      "2017-11-11 08:00:18: Loss at step 5391: 0.03674980625510216\n",
      "2017-11-11 08:00:20: Loss at step 5392: 0.03675062954425812\n",
      "2017-11-11 08:00:22: Loss at step 5393: 0.03678347170352936\n",
      "2017-11-11 08:00:24: Loss at step 5394: 0.036658111959695816\n",
      "2017-11-11 08:00:27: Loss at step 5395: 0.03673000633716583\n",
      "2017-11-11 08:00:29: Loss at step 5396: 0.03678110986948013\n",
      "2017-11-11 08:00:31: Loss at step 5397: 0.03674248605966568\n",
      "2017-11-11 08:00:33: Loss at step 5398: 0.03682790696620941\n",
      "2017-11-11 08:00:35: Loss at step 5399: 0.036798786371946335\n",
      "2017-11-11 08:00:38: Loss at step 5400: 0.036714691668748856\n",
      "2017-11-11 08:00:40: Loss at step 5401: 0.036777179688215256\n",
      "2017-11-11 08:00:42: Loss at step 5402: 0.03672010824084282\n",
      "2017-11-11 08:00:44: Loss at step 5403: 0.03684462979435921\n",
      "2017-11-11 08:00:46: Loss at step 5404: 0.03671201691031456\n",
      "2017-11-11 08:00:48: Loss at step 5405: 0.0367247574031353\n",
      "2017-11-11 08:00:50: Loss at step 5406: 0.03679122403264046\n",
      "2017-11-11 08:00:52: Loss at step 5407: 0.036821071058511734\n",
      "2017-11-11 08:00:55: Loss at step 5408: 0.03669092431664467\n",
      "2017-11-11 08:00:57: Loss at step 5409: 0.036759667098522186\n",
      "2017-11-11 08:00:59: Loss at step 5410: 0.03669776767492294\n",
      "2017-11-11 08:01:01: Loss at step 5411: 0.0367671400308609\n",
      "2017-11-11 08:01:03: Loss at step 5412: 0.0368623211979866\n",
      "2017-11-11 08:01:05: Loss at step 5413: 0.036832649260759354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 08:01:07: Loss at step 5414: 0.03678181767463684\n",
      "2017-11-11 08:01:10: Loss at step 5415: 0.03680938854813576\n",
      "2017-11-11 08:01:12: Loss at step 5416: 0.03675444796681404\n",
      "2017-11-11 08:01:14: Loss at step 5417: 0.03679803013801575\n",
      "2017-11-11 08:01:16: Loss at step 5418: 0.03680841624736786\n",
      "2017-11-11 08:01:18: Loss at step 5419: 0.03678342327475548\n",
      "2017-11-11 08:01:21: Loss at step 5420: 0.0367349237203598\n",
      "2017-11-11 08:01:23: Loss at step 5421: 0.036759618669748306\n",
      "2017-11-11 08:01:25: Loss at step 5422: 0.036769043654203415\n",
      "2017-11-11 08:01:27: Loss at step 5423: 0.03674799203872681\n",
      "2017-11-11 08:01:29: Loss at step 5424: 0.03675438091158867\n",
      "2017-11-11 08:01:32: Loss at step 5425: 0.03683612868189812\n",
      "2017-11-11 08:01:34: Loss at step 5426: 0.036764197051525116\n",
      "2017-11-11 08:01:36: Loss at step 5427: 0.03680302947759628\n",
      "2017-11-11 08:01:38: Loss at step 5428: 0.036851417273283005\n",
      "2017-11-11 08:01:40: Loss at step 5429: 0.03684091195464134\n",
      "2017-11-11 08:01:43: Loss at step 5430: 0.03673240914940834\n",
      "2017-11-11 08:01:45: Loss at step 5431: 0.03678251430392265\n",
      "2017-11-11 08:01:47: Loss at step 5432: 0.03677034750580788\n",
      "2017-11-11 08:01:49: Loss at step 5433: 0.036713652312755585\n",
      "2017-11-11 08:01:51: Loss at step 5434: 0.0368177630007267\n",
      "2017-11-11 08:01:53: Loss at step 5435: 0.03688018023967743\n",
      "2017-11-11 08:01:56: Loss at step 5436: 0.03679916262626648\n",
      "2017-11-11 08:01:58: Loss at step 5437: 0.036873120814561844\n",
      "2017-11-11 08:02:00: Loss at step 5438: 0.03674539923667908\n",
      "2017-11-11 08:02:02: Loss at step 5439: 0.03684953600168228\n",
      "2017-11-11 08:02:04: Loss at step 5440: 0.036802686750888824\n",
      "2017-11-11 08:02:07: Loss at step 5441: 0.036806464195251465\n",
      "2017-11-11 08:02:09: Loss at step 5442: 0.03679322823882103\n",
      "2017-11-11 08:02:11: Loss at step 5443: 0.036789849400520325\n",
      "2017-11-11 08:02:13: Loss at step 5444: 0.03685294836759567\n",
      "2017-11-11 08:02:15: Loss at step 5445: 0.03677147999405861\n",
      "2017-11-11 08:02:18: Loss at step 5446: 0.03684045374393463\n",
      "2017-11-11 08:02:20: Loss at step 5447: 0.036825187504291534\n",
      "2017-11-11 08:02:22: Loss at step 5448: 0.03682193532586098\n",
      "2017-11-11 08:02:24: Loss at step 5449: 0.03676871210336685\n",
      "2017-11-11 08:02:26: Loss at step 5450: 0.03668951243162155\n",
      "2017-11-11 08:02:29: Loss at step 5451: 0.03670115768909454\n",
      "2017-11-11 08:02:31: Loss at step 5452: 0.03668282926082611\n",
      "2017-11-11 08:02:33: Loss at step 5453: 0.036776915192604065\n",
      "2017-11-11 08:02:35: Loss at step 5454: 0.03675911948084831\n",
      "2017-11-11 08:02:37: Loss at step 5455: 0.036791954189538956\n",
      "2017-11-11 08:02:39: Loss at step 5456: 0.03685496747493744\n",
      "2017-11-11 08:02:41: Loss at step 5457: 0.03686942905187607\n",
      "2017-11-11 08:02:43: Loss at step 5458: 0.036801740527153015\n",
      "2017-11-11 08:02:46: Loss at step 5459: 0.036783382296562195\n",
      "2017-11-11 08:02:48: Loss at step 5460: 0.03677473962306976\n",
      "2017-11-11 08:02:50: Loss at step 5461: 0.036767810583114624\n",
      "2017-11-11 08:02:52: Loss at step 5462: 0.03667641058564186\n",
      "2017-11-11 08:02:54: Loss at step 5463: 0.036746274679899216\n",
      "2017-11-11 08:02:56: Loss at step 5464: 0.036900587379932404\n",
      "2017-11-11 08:02:58: Loss at step 5465: 0.036805398762226105\n",
      "2017-11-11 08:03:01: Loss at step 5466: 0.03667810186743736\n",
      "2017-11-11 08:03:03: Loss at step 5467: 0.03682009130716324\n",
      "2017-11-11 08:03:05: Loss at step 5468: 0.036862727254629135\n",
      "2017-11-11 08:03:07: Loss at step 5469: 0.03678509593009949\n",
      "2017-11-11 08:03:09: Loss at step 5470: 0.036743003875017166\n",
      "2017-11-11 08:03:11: Loss at step 5471: 0.03680036962032318\n",
      "2017-11-11 08:03:13: Loss at step 5472: 0.03676421940326691\n",
      "2017-11-11 08:03:16: Loss at step 5473: 0.036771226674318314\n",
      "2017-11-11 08:03:18: Loss at step 5474: 0.03672516345977783\n",
      "2017-11-11 08:03:20: Loss at step 5475: 0.036771826446056366\n",
      "2017-11-11 08:03:22: Loss at step 5476: 0.03682094067335129\n",
      "2017-11-11 08:03:24: Loss at step 5477: 0.03665436431765556\n",
      "2017-11-11 08:03:26: Loss at step 5478: 0.03674391657114029\n",
      "2017-11-11 08:03:28: Loss at step 5479: 0.03674278408288956\n",
      "2017-11-11 08:03:31: Loss at step 5480: 0.036776103079319\n",
      "2017-11-11 08:03:33: Loss at step 5481: 0.036791130900382996\n",
      "2017-11-11 08:03:35: Loss at step 5482: 0.036675166338682175\n",
      "2017-11-11 08:03:37: Loss at step 5483: 0.0367836132645607\n",
      "2017-11-11 08:03:39: Loss at step 5484: 0.03681066259741783\n",
      "2017-11-11 08:03:41: Loss at step 5485: 0.03680785745382309\n",
      "2017-11-11 08:03:43: Loss at step 5486: 0.03669634461402893\n",
      "2017-11-11 08:03:45: Loss at step 5487: 0.03684172406792641\n",
      "2017-11-11 08:03:48: Loss at step 5488: 0.0367451012134552\n",
      "2017-11-11 08:03:50: Loss at step 5489: 0.036748386919498444\n",
      "2017-11-11 08:03:52: Loss at step 5490: 0.03676531836390495\n",
      "2017-11-11 08:03:54: Loss at step 5491: 0.03680223226547241\n",
      "2017-11-11 08:03:56: Loss at step 5492: 0.03680728003382683\n",
      "2017-11-11 08:03:58: Loss at step 5493: 0.036858879029750824\n",
      "2017-11-11 08:04:00: Loss at step 5494: 0.03675009310245514\n",
      "2017-11-11 08:04:03: Loss at step 5495: 0.03675050288438797\n",
      "2017-11-11 08:04:05: Loss at step 5496: 0.036755647510290146\n",
      "2017-11-11 08:04:07: Loss at step 5497: 0.03673773631453514\n",
      "2017-11-11 08:04:09: Loss at step 5498: 0.03675134479999542\n",
      "2017-11-11 08:04:11: Loss at step 5499: 0.03672624006867409\n",
      "2017-11-11 08:04:13: Loss at step 5500: 0.036775946617126465\n",
      "2017-11-11 08:04:15: Loss at step 5501: 0.03684525191783905\n",
      "2017-11-11 08:04:18: Loss at step 5502: 0.036772578954696655\n",
      "2017-11-11 08:04:20: Loss at step 5503: 0.0367368645966053\n",
      "2017-11-11 08:04:22: Loss at step 5504: 0.03689400479197502\n",
      "2017-11-11 08:04:24: Loss at step 5505: 0.036831408739089966\n",
      "2017-11-11 08:04:26: Loss at step 5506: 0.03676464408636093\n",
      "2017-11-11 08:04:28: Loss at step 5507: 0.03673043102025986\n",
      "2017-11-11 08:04:31: Loss at step 5508: 0.03677539527416229\n",
      "2017-11-11 08:04:33: Loss at step 5509: 0.03682822734117508\n",
      "2017-11-11 08:04:35: Loss at step 5510: 0.03687387704849243\n",
      "2017-11-11 08:04:37: Loss at step 5511: 0.036759499460458755\n",
      "2017-11-11 08:04:39: Loss at step 5512: 0.036861058324575424\n",
      "2017-11-11 08:04:41: Loss at step 5513: 0.0367218554019928\n",
      "2017-11-11 08:04:43: Loss at step 5514: 0.03687397018074989\n",
      "2017-11-11 08:04:45: Loss at step 5515: 0.03677546977996826\n",
      "2017-11-11 08:04:48: Loss at step 5516: 0.03685682266950607\n",
      "2017-11-11 08:04:50: Loss at step 5517: 0.03677918016910553\n",
      "2017-11-11 08:04:52: Loss at step 5518: 0.03686782345175743\n",
      "2017-11-11 08:04:54: Loss at step 5519: 0.036762550473213196\n",
      "2017-11-11 08:04:56: Loss at step 5520: 0.03673391044139862\n",
      "2017-11-11 08:04:58: Loss at step 5521: 0.03675520047545433\n",
      "2017-11-11 08:05:00: Loss at step 5522: 0.03680802136659622\n",
      "2017-11-11 08:05:03: Loss at step 5523: 0.036775246262550354\n",
      "2017-11-11 08:05:05: Loss at step 5524: 0.03669963404536247\n",
      "2017-11-11 08:05:07: Loss at step 5525: 0.036697011440992355\n",
      "2017-11-11 08:05:09: Loss at step 5526: 0.036700185388326645\n",
      "2017-11-11 08:05:11: Loss at step 5527: 0.03682711347937584\n",
      "2017-11-11 08:05:13: Loss at step 5528: 0.0367186963558197\n",
      "2017-11-11 08:05:15: Loss at step 5529: 0.03676830604672432\n",
      "2017-11-11 08:05:18: Loss at step 5530: 0.03672187030315399\n",
      "2017-11-11 08:05:20: Loss at step 5531: 0.036831747740507126\n",
      "2017-11-11 08:05:22: Loss at step 5532: 0.03680478408932686\n",
      "2017-11-11 08:05:24: Loss at step 5533: 0.036837439984083176\n",
      "2017-11-11 08:05:26: Loss at step 5534: 0.03680429607629776\n",
      "2017-11-11 08:05:28: Loss at step 5535: 0.036749467253685\n",
      "2017-11-11 08:05:30: Loss at step 5536: 0.036749567836523056\n",
      "2017-11-11 08:05:33: Loss at step 5537: 0.036686211824417114\n",
      "2017-11-11 08:05:35: Loss at step 5538: 0.036769382655620575\n",
      "2017-11-11 08:05:37: Loss at step 5539: 0.0366760790348053\n",
      "2017-11-11 08:05:39: Loss at step 5540: 0.03666241466999054\n",
      "2017-11-11 08:05:41: Loss at step 5541: 0.03676001727581024\n",
      "2017-11-11 08:05:43: Loss at step 5542: 0.03682524338364601\n",
      "2017-11-11 08:05:46: Loss at step 5543: 0.0367741584777832\n",
      "2017-11-11 08:05:48: Loss at step 5544: 0.036746565252542496\n",
      "2017-11-11 08:05:50: Loss at step 5545: 0.03680209815502167\n",
      "2017-11-11 08:05:52: Loss at step 5546: 0.036762822419404984\n",
      "2017-11-11 08:05:54: Loss at step 5547: 0.0368107333779335\n",
      "2017-11-11 08:05:56: Loss at step 5548: 0.03675464913249016\n",
      "2017-11-11 08:05:58: Loss at step 5549: 0.03678453341126442\n",
      "2017-11-11 08:06:01: Loss at step 5550: 0.036677099764347076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 08:06:03: Loss at step 5551: 0.036704663187265396\n",
      "2017-11-11 08:06:05: Loss at step 5552: 0.036715950816869736\n",
      "2017-11-11 08:06:07: Loss at step 5553: 0.036708857864141464\n",
      "2017-11-11 08:06:09: Loss at step 5554: 0.03671044111251831\n",
      "2017-11-11 08:06:11: Loss at step 5555: 0.03673554211854935\n",
      "2017-11-11 08:06:13: Loss at step 5556: 0.03674839437007904\n",
      "2017-11-11 08:06:16: Loss at step 5557: 0.0367605984210968\n",
      "2017-11-11 08:06:18: Loss at step 5558: 0.036808136850595474\n",
      "2017-11-11 08:06:20: Loss at step 5559: 0.036758940666913986\n",
      "2017-11-11 08:06:22: Loss at step 5560: 0.03677215427160263\n",
      "2017-11-11 08:06:24: Loss at step 5561: 0.036749131977558136\n",
      "2017-11-11 08:06:26: Loss at step 5562: 0.036733854562044144\n",
      "2017-11-11 08:06:29: Loss at step 5563: 0.03674313426017761\n",
      "2017-11-11 08:06:31: Loss at step 5564: 0.03674943372607231\n",
      "2017-11-11 08:06:33: Loss at step 5565: 0.03677045926451683\n",
      "2017-11-11 08:06:35: Loss at step 5566: 0.036753296852111816\n",
      "2017-11-11 08:06:37: Loss at step 5567: 0.03674415498971939\n",
      "2017-11-11 08:06:39: Loss at step 5568: 0.03677783161401749\n",
      "2017-11-11 08:06:41: Loss at step 5569: 0.03675990551710129\n",
      "2017-11-11 08:06:44: Loss at step 5570: 0.03664136305451393\n",
      "2017-11-11 08:06:46: Loss at step 5571: 0.03676920011639595\n",
      "2017-11-11 08:06:48: Loss at step 5572: 0.03672562912106514\n",
      "2017-11-11 08:06:50: Loss at step 5573: 0.03670201450586319\n",
      "2017-11-11 08:06:52: Loss at step 5574: 0.03677643835544586\n",
      "2017-11-11 08:06:54: Loss at step 5575: 0.036802440881729126\n",
      "2017-11-11 08:06:57: Loss at step 5576: 0.036653321236371994\n",
      "2017-11-11 08:06:59: Loss at step 5577: 0.03670521825551987\n",
      "2017-11-11 08:07:01: Loss at step 5578: 0.03678479790687561\n",
      "2017-11-11 08:07:03: Loss at step 5579: 0.03667005896568298\n",
      "2017-11-11 08:07:05: Loss at step 5580: 0.03668710216879845\n",
      "2017-11-11 08:07:07: Loss at step 5581: 0.03671763837337494\n",
      "2017-11-11 08:07:10: Loss at step 5582: 0.036726441234350204\n",
      "2017-11-11 08:07:12: Loss at step 5583: 0.036724720150232315\n",
      "2017-11-11 08:07:14: Loss at step 5584: 0.036777064204216\n",
      "2017-11-11 08:07:16: Loss at step 5585: 0.03673578053712845\n",
      "2017-11-11 08:07:18: Loss at step 5586: 0.036807335913181305\n",
      "2017-11-11 08:07:20: Loss at step 5587: 0.03670385852456093\n",
      "2017-11-11 08:07:22: Loss at step 5588: 0.036799103021621704\n",
      "2017-11-11 08:07:25: Loss at step 5589: 0.036803968250751495\n",
      "2017-11-11 08:07:27: Loss at step 5590: 0.03676148131489754\n",
      "2017-11-11 08:07:29: Loss at step 5591: 0.03671599552035332\n",
      "2017-11-11 08:07:31: Loss at step 5592: 0.03679654747247696\n",
      "2017-11-11 08:07:33: Loss at step 5593: 0.03683725371956825\n",
      "2017-11-11 08:07:35: Loss at step 5594: 0.0367487296462059\n",
      "2017-11-11 08:07:38: Loss at step 5595: 0.03670863062143326\n",
      "2017-11-11 08:07:40: Loss at step 5596: 0.03676340728998184\n",
      "2017-11-11 08:07:42: Loss at step 5597: 0.03676125407218933\n",
      "2017-11-11 08:07:44: Loss at step 5598: 0.03684568032622337\n",
      "2017-11-11 08:07:46: Loss at step 5599: 0.03666790947318077\n",
      "2017-11-11 08:07:48: Loss at step 5600: 0.036727793514728546\n",
      "2017-11-11 08:07:51: Loss at step 5601: 0.03674826771020889\n",
      "2017-11-11 08:07:53: Loss at step 5602: 0.03675707429647446\n",
      "2017-11-11 08:07:55: Loss at step 5603: 0.03673906251788139\n",
      "2017-11-11 08:07:57: Loss at step 5604: 0.036763038486242294\n",
      "2017-11-11 08:07:59: Loss at step 5605: 0.03678445890545845\n",
      "2017-11-11 08:08:01: Loss at step 5606: 0.03668863698840141\n",
      "2017-11-11 08:08:04: Loss at step 5607: 0.036807987838983536\n",
      "2017-11-11 08:08:06: Loss at step 5608: 0.03683707118034363\n",
      "2017-11-11 08:08:08: Loss at step 5609: 0.036740776151418686\n",
      "2017-11-11 08:08:10: Loss at step 5610: 0.03681020811200142\n",
      "2017-11-11 08:08:12: Loss at step 5611: 0.03680935502052307\n",
      "2017-11-11 08:08:14: Loss at step 5612: 0.03685489669442177\n",
      "2017-11-11 08:08:16: Loss at step 5613: 0.03678383678197861\n",
      "2017-11-11 08:08:19: Loss at step 5614: 0.036765098571777344\n",
      "2017-11-11 08:08:21: Loss at step 5615: 0.03676613047719002\n",
      "2017-11-11 08:08:23: Loss at step 5616: 0.03675909712910652\n",
      "2017-11-11 08:08:25: Loss at step 5617: 0.03684287145733833\n",
      "2017-11-11 08:08:27: Loss at step 5618: 0.0368240587413311\n",
      "2017-11-11 08:08:30: Loss at step 5619: 0.036876924335956573\n",
      "2017-11-11 08:08:32: Loss at step 5620: 0.03677757456898689\n",
      "2017-11-11 08:08:34: Loss at step 5621: 0.03671211376786232\n",
      "2017-11-11 08:08:36: Loss at step 5622: 0.03685671091079712\n",
      "2017-11-11 08:08:38: Loss at step 5623: 0.03672581911087036\n",
      "2017-11-11 08:08:40: Loss at step 5624: 0.036830224096775055\n",
      "2017-11-11 08:08:43: Loss at step 5625: 0.03678128495812416\n",
      "2017-11-11 08:08:45: Loss at step 5626: 0.036819230765104294\n",
      "2017-11-11 08:08:47: Loss at step 5627: 0.036821674555540085\n",
      "2017-11-11 08:08:49: Loss at step 5628: 0.036779798567295074\n",
      "2017-11-11 08:08:51: Loss at step 5629: 0.03683501482009888\n",
      "2017-11-11 08:08:53: Loss at step 5630: 0.03680667653679848\n",
      "2017-11-11 08:08:55: Loss at step 5631: 0.03677698224782944\n",
      "2017-11-11 08:08:58: Loss at step 5632: 0.03678010776638985\n",
      "2017-11-11 08:09:00: Loss at step 5633: 0.03682740405201912\n",
      "2017-11-11 08:09:02: Loss at step 5634: 0.03679313883185387\n",
      "2017-11-11 08:09:04: Loss at step 5635: 0.036708470433950424\n",
      "2017-11-11 08:09:06: Loss at step 5636: 0.03682165592908859\n",
      "2017-11-11 08:09:08: Loss at step 5637: 0.036759596318006516\n",
      "2017-11-11 08:09:11: Loss at step 5638: 0.03687768429517746\n",
      "2017-11-11 08:09:13: Loss at step 5639: 0.03687041997909546\n",
      "2017-11-11 08:09:15: Loss at step 5640: 0.03678826615214348\n",
      "2017-11-11 08:09:17: Loss at step 5641: 0.03680019453167915\n",
      "2017-11-11 08:09:19: Loss at step 5642: 0.03680286929011345\n",
      "2017-11-11 08:09:22: Loss at step 5643: 0.03689732402563095\n",
      "2017-11-11 08:09:24: Loss at step 5644: 0.03677026927471161\n",
      "2017-11-11 08:09:26: Loss at step 5645: 0.036832116544246674\n",
      "2017-11-11 08:09:28: Loss at step 5646: 0.03678639978170395\n",
      "2017-11-11 08:09:31: Loss at step 5647: 0.036773983389139175\n",
      "2017-11-11 08:09:33: Loss at step 5648: 0.036797016859054565\n",
      "2017-11-11 08:09:35: Loss at step 5649: 0.03678516671061516\n",
      "2017-11-11 08:09:37: Loss at step 5650: 0.03688030317425728\n",
      "2017-11-11 08:09:39: Loss at step 5651: 0.036763180047273636\n",
      "2017-11-11 08:09:41: Loss at step 5652: 0.03681254759430885\n",
      "2017-11-11 08:09:44: Loss at step 5653: 0.03679566830396652\n",
      "2017-11-11 08:09:46: Loss at step 5654: 0.036805473268032074\n",
      "2017-11-11 08:09:48: Loss at step 5655: 0.03677114099264145\n",
      "2017-11-11 08:09:50: Loss at step 5656: 0.036826685070991516\n",
      "2017-11-11 08:09:52: Loss at step 5657: 0.036870237439870834\n",
      "2017-11-11 08:09:54: Loss at step 5658: 0.03683779016137123\n",
      "2017-11-11 08:09:57: Loss at step 5659: 0.036821525543928146\n",
      "2017-11-11 08:09:59: Loss at step 5660: 0.03680982068181038\n",
      "2017-11-11 08:10:01: Loss at step 5661: 0.036773502826690674\n",
      "2017-11-11 08:10:03: Loss at step 5662: 0.03670340031385422\n",
      "2017-11-11 08:10:05: Loss at step 5663: 0.03673935681581497\n",
      "2017-11-11 08:10:08: Loss at step 5664: 0.036801017820835114\n",
      "2017-11-11 08:10:10: Loss at step 5665: 0.03679271042346954\n",
      "2017-11-11 08:10:12: Loss at step 5666: 0.03678647801280022\n",
      "2017-11-11 08:10:14: Loss at step 5667: 0.03672993928194046\n",
      "2017-11-11 08:10:16: Loss at step 5668: 0.03678256645798683\n",
      "2017-11-11 08:10:19: Loss at step 5669: 0.036807116121053696\n",
      "2017-11-11 08:10:21: Loss at step 5670: 0.03680858388543129\n",
      "2017-11-11 08:10:23: Loss at step 5671: 0.036772601306438446\n",
      "2017-11-11 08:10:25: Loss at step 5672: 0.0368739515542984\n",
      "2017-11-11 08:10:28: Loss at step 5673: 0.03678349405527115\n",
      "2017-11-11 08:10:30: Loss at step 5674: 0.03684007748961449\n",
      "2017-11-11 08:10:32: Loss at step 5675: 0.03689979389309883\n",
      "2017-11-11 08:10:34: Loss at step 5676: 0.03681020066142082\n",
      "2017-11-11 08:10:36: Loss at step 5677: 0.03683831915259361\n",
      "2017-11-11 08:10:39: Loss at step 5678: 0.0368368923664093\n",
      "2017-11-11 08:10:41: Loss at step 5679: 0.036797896027565\n",
      "2017-11-11 08:10:43: Loss at step 5680: 0.036860961467027664\n",
      "2017-11-11 08:10:45: Loss at step 5681: 0.03686593100428581\n",
      "2017-11-11 08:10:47: Loss at step 5682: 0.03683322295546532\n",
      "2017-11-11 08:10:49: Loss at step 5683: 0.036869652569293976\n",
      "2017-11-11 08:10:52: Loss at step 5684: 0.03678327053785324\n",
      "2017-11-11 08:10:54: Loss at step 5685: 0.03686157986521721\n",
      "2017-11-11 08:10:56: Loss at step 5686: 0.03683876991271973\n",
      "2017-11-11 08:10:58: Loss at step 5687: 0.036707356572151184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 08:11:00: Loss at step 5688: 0.03686915710568428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4ebaaf5d30d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3501\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}: Loss at step {1}: {2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{0}/training_data/x_batch-{1}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-be9295578f83>\u001b[0m in \u001b[0;36mnext_training_batch\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnext_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mboards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardPartialMineCounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodeCountsOneHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidGuesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(3501,1000001):\n",
    "    x_batch, y_batch, _ = next_training_batch(10000)\n",
    "    loss = model.train_on_batch(np.array(x_batch), np.array(y_batch))\n",
    "    print('{0}: Loss at step {1}: {2}'.format(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), iteration, loss))\n",
    "    np.save('{0}/training_data/x_batch-{1}.npy'.format(savePath, iteration), np.array(x_batch))\n",
    "    np.save('{0}/training_data/y_batch-{1}.npy'.format(savePath, iteration), np.array(y_batch))\n",
    "    if iteration % 500 == 0:\n",
    "        model.save('{0}/model-{1}.h5'.format(savePath, iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_batch(n):\n",
    "    x_batch = np.load('{0}/training_data/x_batch-{1}.npy'.format(savePath, n))\n",
    "    y_batch = np.load('{0}/training_data/y_batch-{1}.npy'.format(savePath, n))\n",
    "    return (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-11 08:13:21: Loss at step 5689: 0.03686288744211197\n",
      "2017-11-11 08:13:22: Loss at step 5690: 0.036672044545412064\n",
      "2017-11-11 08:13:23: Loss at step 5691: 0.0370478555560112\n",
      "2017-11-11 08:13:23: Loss at step 5692: 0.03694919869303703\n",
      "2017-11-11 08:13:24: Loss at step 5693: 0.03681319206953049\n",
      "2017-11-11 08:13:24: Loss at step 5694: 0.03692269325256348\n",
      "2017-11-11 08:13:25: Loss at step 5695: 0.03690475970506668\n",
      "2017-11-11 08:13:25: Loss at step 5696: 0.03696698695421219\n",
      "2017-11-11 08:13:26: Loss at step 5697: 0.03687051311135292\n",
      "2017-11-11 08:13:26: Loss at step 5698: 0.03680562227964401\n",
      "2017-11-11 08:13:27: Loss at step 5699: 0.03691377490758896\n",
      "2017-11-11 08:13:27: Loss at step 5700: 0.03687459975481033\n",
      "2017-11-11 08:13:28: Loss at step 5701: 0.03696953132748604\n",
      "2017-11-11 08:13:28: Loss at step 5702: 0.0369834341108799\n",
      "2017-11-11 08:13:29: Loss at step 5703: 0.036900438368320465\n",
      "2017-11-11 08:13:29: Loss at step 5704: 0.0368473194539547\n",
      "2017-11-11 08:13:30: Loss at step 5705: 0.0369408018887043\n",
      "2017-11-11 08:13:30: Loss at step 5706: 0.0369722880423069\n",
      "2017-11-11 08:13:31: Loss at step 5707: 0.0368988998234272\n",
      "2017-11-11 08:13:31: Loss at step 5708: 0.03693779930472374\n",
      "2017-11-11 08:13:32: Loss at step 5709: 0.03689481317996979\n",
      "2017-11-11 08:13:32: Loss at step 5710: 0.036865755915641785\n",
      "2017-11-11 08:13:33: Loss at step 5711: 0.036788392812013626\n",
      "2017-11-11 08:13:33: Loss at step 5712: 0.036989327520132065\n",
      "2017-11-11 08:13:34: Loss at step 5713: 0.03696873039007187\n",
      "2017-11-11 08:13:34: Loss at step 5714: 0.03688075393438339\n",
      "2017-11-11 08:13:35: Loss at step 5715: 0.036872681230306625\n",
      "2017-11-11 08:13:35: Loss at step 5716: 0.03690873086452484\n",
      "2017-11-11 08:13:36: Loss at step 5717: 0.03687874227762222\n",
      "2017-11-11 08:13:36: Loss at step 5718: 0.036763016134500504\n",
      "2017-11-11 08:13:37: Loss at step 5719: 0.03694196045398712\n",
      "2017-11-11 08:13:37: Loss at step 5720: 0.03689658269286156\n",
      "2017-11-11 08:13:38: Loss at step 5721: 0.03692753240466118\n",
      "2017-11-11 08:13:38: Loss at step 5722: 0.03694750741124153\n",
      "2017-11-11 08:13:39: Loss at step 5723: 0.0367942750453949\n",
      "2017-11-11 08:13:39: Loss at step 5724: 0.03683285415172577\n",
      "2017-11-11 08:13:40: Loss at step 5725: 0.03687180578708649\n",
      "2017-11-11 08:13:40: Loss at step 5726: 0.03678714111447334\n",
      "2017-11-11 08:13:41: Loss at step 5727: 0.0368400439620018\n",
      "2017-11-11 08:13:41: Loss at step 5728: 0.036968812346458435\n",
      "2017-11-11 08:13:42: Loss at step 5729: 0.03682686761021614\n",
      "2017-11-11 08:13:42: Loss at step 5730: 0.03696451336145401\n",
      "2017-11-11 08:13:43: Loss at step 5731: 0.036890577524900436\n",
      "2017-11-11 08:13:43: Loss at step 5732: 0.03677411004900932\n",
      "2017-11-11 08:13:44: Loss at step 5733: 0.03677870333194733\n",
      "2017-11-11 08:13:44: Loss at step 5734: 0.036796145141124725\n",
      "2017-11-11 08:13:45: Loss at step 5735: 0.0367467999458313\n",
      "2017-11-11 08:13:45: Loss at step 5736: 0.03672822192311287\n",
      "2017-11-11 08:13:46: Loss at step 5737: 0.03672946244478226\n",
      "2017-11-11 08:13:46: Loss at step 5738: 0.036853037774562836\n",
      "2017-11-11 08:13:47: Loss at step 5739: 0.03676074743270874\n",
      "2017-11-11 08:13:47: Loss at step 5740: 0.03677909076213837\n",
      "2017-11-11 08:13:48: Loss at step 5741: 0.03675871342420578\n",
      "2017-11-11 08:13:48: Loss at step 5742: 0.03683329001069069\n",
      "2017-11-11 08:13:49: Loss at step 5743: 0.03680935502052307\n",
      "2017-11-11 08:13:49: Loss at step 5744: 0.036747224628925323\n",
      "2017-11-11 08:13:50: Loss at step 5745: 0.03675027936697006\n",
      "2017-11-11 08:13:50: Loss at step 5746: 0.036728717386722565\n",
      "2017-11-11 08:13:51: Loss at step 5747: 0.036776188760995865\n",
      "2017-11-11 08:13:51: Loss at step 5748: 0.03673359006643295\n",
      "2017-11-11 08:13:52: Loss at step 5749: 0.03678261116147041\n",
      "2017-11-11 08:13:52: Loss at step 5750: 0.036727193742990494\n",
      "2017-11-11 08:13:53: Loss at step 5751: 0.03668581694364548\n",
      "2017-11-11 08:13:53: Loss at step 5752: 0.03679009899497032\n",
      "2017-11-11 08:13:54: Loss at step 5753: 0.03674862161278725\n",
      "2017-11-11 08:13:54: Loss at step 5754: 0.036770280450582504\n",
      "2017-11-11 08:13:55: Loss at step 5755: 0.03669526427984238\n",
      "2017-11-11 08:13:55: Loss at step 5756: 0.03679676353931427\n",
      "2017-11-11 08:13:56: Loss at step 5757: 0.036756232380867004\n",
      "2017-11-11 08:13:56: Loss at step 5758: 0.036727339029312134\n",
      "2017-11-11 08:13:57: Loss at step 5759: 0.03674883022904396\n",
      "2017-11-11 08:13:57: Loss at step 5760: 0.03661199286580086\n",
      "2017-11-11 08:13:58: Loss at step 5761: 0.03677501529455185\n",
      "2017-11-11 08:13:58: Loss at step 5762: 0.03680412471294403\n",
      "2017-11-11 08:13:59: Loss at step 5763: 0.036786921322345734\n",
      "2017-11-11 08:13:59: Loss at step 5764: 0.03670552000403404\n",
      "2017-11-11 08:14:00: Loss at step 5765: 0.03673046827316284\n",
      "2017-11-11 08:14:00: Loss at step 5766: 0.036733631044626236\n",
      "2017-11-11 08:14:01: Loss at step 5767: 0.036702509969472885\n",
      "2017-11-11 08:14:01: Loss at step 5768: 0.03675076737999916\n",
      "2017-11-11 08:14:02: Loss at step 5769: 0.036805056035518646\n",
      "2017-11-11 08:14:02: Loss at step 5770: 0.036702711135149\n",
      "2017-11-11 08:14:03: Loss at step 5771: 0.03667079284787178\n",
      "2017-11-11 08:14:03: Loss at step 5772: 0.03671273589134216\n",
      "2017-11-11 08:14:04: Loss at step 5773: 0.0366617813706398\n",
      "2017-11-11 08:14:04: Loss at step 5774: 0.0366227887570858\n",
      "2017-11-11 08:14:05: Loss at step 5775: 0.036647576838731766\n",
      "2017-11-11 08:14:05: Loss at step 5776: 0.0367274172604084\n",
      "2017-11-11 08:14:06: Loss at step 5777: 0.03673883527517319\n",
      "2017-11-11 08:14:06: Loss at step 5778: 0.03681967034935951\n",
      "2017-11-11 08:14:07: Loss at step 5779: 0.03676968440413475\n",
      "2017-11-11 08:14:07: Loss at step 5780: 0.03683271259069443\n",
      "2017-11-11 08:14:08: Loss at step 5781: 0.036760929971933365\n",
      "2017-11-11 08:14:08: Loss at step 5782: 0.03677571564912796\n",
      "2017-11-11 08:14:09: Loss at step 5783: 0.0366896316409111\n",
      "2017-11-11 08:14:09: Loss at step 5784: 0.03673014044761658\n",
      "2017-11-11 08:14:10: Loss at step 5785: 0.03670518845319748\n",
      "2017-11-11 08:14:10: Loss at step 5786: 0.036602120846509933\n",
      "2017-11-11 08:14:11: Loss at step 5787: 0.03673321381211281\n",
      "2017-11-11 08:14:11: Loss at step 5788: 0.03684303164482117\n",
      "2017-11-11 08:14:12: Loss at step 5789: 0.036897897720336914\n",
      "2017-11-11 08:14:12: Loss at step 5790: 0.03678872063755989\n",
      "2017-11-11 08:14:13: Loss at step 5791: 0.03676421567797661\n",
      "2017-11-11 08:14:13: Loss at step 5792: 0.03685058653354645\n",
      "2017-11-11 08:14:13: Loss at step 5793: 0.036755841225385666\n",
      "2017-11-11 08:14:14: Loss at step 5794: 0.03676069527864456\n",
      "2017-11-11 08:14:14: Loss at step 5795: 0.036805473268032074\n",
      "2017-11-11 08:14:15: Loss at step 5796: 0.03683672472834587\n",
      "2017-11-11 08:14:15: Loss at step 5797: 0.03672697767615318\n",
      "2017-11-11 08:14:16: Loss at step 5798: 0.03670239448547363\n",
      "2017-11-11 08:14:16: Loss at step 5799: 0.03685688227415085\n",
      "2017-11-11 08:14:17: Loss at step 5800: 0.036792926490306854\n",
      "2017-11-11 08:14:17: Loss at step 5801: 0.03669513761997223\n",
      "2017-11-11 08:14:18: Loss at step 5802: 0.03669116646051407\n",
      "2017-11-11 08:14:18: Loss at step 5803: 0.03662542998790741\n",
      "2017-11-11 08:14:19: Loss at step 5804: 0.036675166338682175\n",
      "2017-11-11 08:14:19: Loss at step 5805: 0.03661338612437248\n",
      "2017-11-11 08:14:20: Loss at step 5806: 0.03669653832912445\n",
      "2017-11-11 08:14:20: Loss at step 5807: 0.03673478588461876\n",
      "2017-11-11 08:14:21: Loss at step 5808: 0.03666853904724121\n",
      "2017-11-11 08:14:21: Loss at step 5809: 0.03670434653759003\n",
      "2017-11-11 08:14:22: Loss at step 5810: 0.03664695471525192\n",
      "2017-11-11 08:14:22: Loss at step 5811: 0.03661893308162689\n",
      "2017-11-11 08:14:23: Loss at step 5812: 0.03658832237124443\n",
      "2017-11-11 08:14:23: Loss at step 5813: 0.03670290485024452\n",
      "2017-11-11 08:14:24: Loss at step 5814: 0.03681284934282303\n",
      "2017-11-11 08:14:24: Loss at step 5815: 0.03671060875058174\n",
      "2017-11-11 08:14:25: Loss at step 5816: 0.03667139261960983\n",
      "2017-11-11 08:14:25: Loss at step 5817: 0.03665926679968834\n",
      "2017-11-11 08:14:26: Loss at step 5818: 0.0368136465549469\n",
      "2017-11-11 08:14:27: Loss at step 5819: 0.03678645193576813\n",
      "2017-11-11 08:14:27: Loss at step 5820: 0.03679266199469566\n",
      "2017-11-11 08:14:27: Loss at step 5821: 0.036832090467214584\n",
      "2017-11-11 08:14:28: Loss at step 5822: 0.03680577501654625\n",
      "2017-11-11 08:14:28: Loss at step 5823: 0.03676941618323326\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(5689,1000001):\n",
    "    x_batch, y_batch = load_training_batch(iteration % 5689)\n",
    "    loss = model.train_on_batch(np.array(x_batch), np.array(y_batch))\n",
    "    print('{0}: Loss at step {1}: {2}'.format(dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), iteration, loss))\n",
    "    if iteration % 500 == 0:\n",
    "        model.save('{0}/model-{1}.h5'.format(savePath, iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors for batch size of  70000\n",
      "9352\n"
     ]
    }
   ],
   "source": [
    "# Run a test\n",
    "batchSize = 70000\n",
    "x_batch, y_batch, _ = next_training_batch(batchSize)\n",
    "\n",
    "predictions = model.predict(np.array(x_batch), batch_size = batchSize)\n",
    "bestSquares = [pred.argmax() for pred in predictions]\n",
    "unfrees = [(ys == 0).astype(int) for ys in y_batch]\n",
    "frees = [unfrees[i][bestSquares[i]] for i in range(batchSize)]\n",
    "print(\"Number of errors for batch size of \", batchSize)\n",
    "print(sum(frees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "900*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6300"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "900*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
