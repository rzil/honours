{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier for guessing minesweeper board position and whether it has a mine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for simulation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random as r\n",
    "import datetime as dt\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = (8,8)\n",
    "mineProbability = 0.16      # Probability that a square contain a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clears a square on the minesweeper board.\n",
    "# If it had a mine, return true\n",
    "# Otherwise if it has no adjacent mines, recursively run on adjacent squares\n",
    "# Return false\n",
    "def clearSquare(board,adjacency,row,col):\n",
    "    rows,cols = dimensions\n",
    "    if board[row,col] == 1:\n",
    "        return True\n",
    "    if adjacency[row,col] >= 0:\n",
    "        return False\n",
    "    n = 0\n",
    "    for r in range(row-1,row+2):\n",
    "        for c in range(col-1,col+2):\n",
    "            if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                n += board[r,c]\n",
    "    adjacency[row,col] = n\n",
    "    if n == 0:\n",
    "        for r in range(row-1,row+2):\n",
    "            for c in range(col-1,col+2):\n",
    "                if 0 <= r and r < rows and 0 <= c and c < cols:\n",
    "                    clearSquare(board,adjacency,r,c)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a mine board and gives a mine count with mines removed, and other random squares removed\n",
    "def boardPartialMineCounts(board):\n",
    "    clearProbability = r.uniform(0.05,0.5)\n",
    "    result = np.full(dimensions,-1)\n",
    "    for index, x in np.random.permutation(list(np.ndenumerate(board))):\n",
    "        row,col = index\n",
    "        if not(x) and result[row,col] == -1 and r.uniform(0,1) < clearProbability:\n",
    "            clearSquare(board,result,row,col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a random training batch of size n\n",
    "def randomBoard(i):\n",
    "    return(np.random.random(dimensions) < mineProbability)\n",
    "\n",
    "def encodeCountsOneHot(counts):\n",
    "    countsOneHot = np.zeros((counts.size,10))\n",
    "    countsOneHot[np.arange(counts.size), counts.flatten() + 1] = 1\n",
    "    return(countsOneHot.flatten())\n",
    "\n",
    "def validGuesses(boardAndCounts):\n",
    "    board,counts = boardAndCounts\n",
    "    validGuesses = np.append(((counts == -1).astype(int) - board).flatten().astype(float),\n",
    "        board.flatten().astype(float))\n",
    "    validGuessesSum = sum(validGuesses)\n",
    "    if validGuessesSum > 0:\n",
    "        return(validGuesses / validGuessesSum)\n",
    "    else:\n",
    "        return(np.zeros(board.size*2))\n",
    "\n",
    "try:\n",
    "    cpus = mp.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2   # arbitrary default\n",
    "\n",
    "pool = mp.Pool(processes=cpus)\n",
    "\n",
    "def next_training_batch(n):\n",
    "    boards = pool.map(randomBoard, range(n))\n",
    "    counts = pool.map(boardPartialMineCounts, boards)\n",
    "    batch_xs = pool.map(encodeCountsOneHot, counts)\n",
    "    batch_ys = pool.map(validGuesses, zip(boards,counts))\n",
    "    return(batch_xs, batch_ys, boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = dimensions\n",
    "size = rows*cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=size, input_dim=size*10))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((rows, cols, 1)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same'))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((4*4*32,)))\n",
    "\n",
    "#model.add(tf.keras.layers.Dense(units=1024))\n",
    "#model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=size*2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='poisson',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = '/media/ruben/BigDisk/tensorflow/tensorflow-logs/tf.Mines8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:30:22: Loss and accuracy at step 0: 0.0457546, 0.004\n",
      "2017-11-10 14:30:24: Loss and accuracy at step 1: 0.0457211, 0.0307\n",
      "2017-11-10 14:30:26: Loss and accuracy at step 2: 0.0456961, 0.0994\n",
      "2017-11-10 14:30:28: Loss and accuracy at step 3: 0.0456614, 0.1845\n",
      "2017-11-10 14:30:31: Loss and accuracy at step 4: 0.0456281, 0.2248\n",
      "2017-11-10 14:30:33: Loss and accuracy at step 5: 0.0455972, 0.2615\n",
      "2017-11-10 14:30:35: Loss and accuracy at step 6: 0.0455677, 0.2972\n",
      "2017-11-10 14:30:37: Loss and accuracy at step 7: 0.0455334, 0.3123\n",
      "2017-11-10 14:30:39: Loss and accuracy at step 8: 0.0455119, 0.3297\n",
      "2017-11-10 14:30:41: Loss and accuracy at step 9: 0.0454891, 0.3391\n",
      "2017-11-10 14:30:43: Loss and accuracy at step 10: 0.0454549, 0.3418\n",
      "2017-11-10 14:30:45: Loss and accuracy at step 11: 0.0454475, 0.3338\n",
      "2017-11-10 14:30:47: Loss and accuracy at step 12: 0.0454402, 0.3356\n",
      "2017-11-10 14:30:49: Loss and accuracy at step 13: 0.0454124, 0.3241\n",
      "2017-11-10 14:30:51: Loss and accuracy at step 14: 0.0453931, 0.3089\n",
      "2017-11-10 14:30:54: Loss and accuracy at step 15: 0.045362, 0.29\n",
      "2017-11-10 14:30:56: Loss and accuracy at step 16: 0.0453327, 0.2524\n",
      "2017-11-10 14:30:58: Loss and accuracy at step 17: 0.045298, 0.2166\n",
      "2017-11-10 14:31:00: Loss and accuracy at step 18: 0.0452589, 0.191\n",
      "2017-11-10 14:31:02: Loss and accuracy at step 19: 0.0452337, 0.1451\n",
      "2017-11-10 14:31:04: Loss and accuracy at step 20: 0.0451877, 0.1194\n",
      "2017-11-10 14:31:06: Loss and accuracy at step 21: 0.0451495, 0.1007\n",
      "2017-11-10 14:31:08: Loss and accuracy at step 22: 0.0451029, 0.0868\n",
      "2017-11-10 14:31:10: Loss and accuracy at step 23: 0.0450636, 0.0831\n",
      "2017-11-10 14:31:12: Loss and accuracy at step 24: 0.045007, 0.08\n",
      "2017-11-10 14:31:14: Loss and accuracy at step 25: 0.0449527, 0.0797\n",
      "2017-11-10 14:31:16: Loss and accuracy at step 26: 0.0448861, 0.0883\n",
      "2017-11-10 14:31:18: Loss and accuracy at step 27: 0.0448024, 0.0932\n",
      "2017-11-10 14:31:20: Loss and accuracy at step 28: 0.0447348, 0.1005\n",
      "2017-11-10 14:31:23: Loss and accuracy at step 29: 0.0446581, 0.1129\n",
      "2017-11-10 14:31:25: Loss and accuracy at step 30: 0.04458, 0.1185\n",
      "2017-11-10 14:31:27: Loss and accuracy at step 31: 0.0444828, 0.1055\n",
      "2017-11-10 14:31:29: Loss and accuracy at step 32: 0.0443808, 0.0923\n",
      "2017-11-10 14:31:31: Loss and accuracy at step 33: 0.0442882, 0.0701\n",
      "2017-11-10 14:31:33: Loss and accuracy at step 34: 0.0442028, 0.0591\n",
      "2017-11-10 14:31:35: Loss and accuracy at step 35: 0.0441037, 0.0576\n",
      "2017-11-10 14:31:37: Loss and accuracy at step 36: 0.0440007, 0.0545\n",
      "2017-11-10 14:31:39: Loss and accuracy at step 37: 0.0438813, 0.0541\n",
      "2017-11-10 14:31:41: Loss and accuracy at step 38: 0.043791, 0.0498\n",
      "2017-11-10 14:31:43: Loss and accuracy at step 39: 0.0436969, 0.0511\n",
      "2017-11-10 14:31:45: Loss and accuracy at step 40: 0.0435785, 0.0537\n",
      "2017-11-10 14:31:47: Loss and accuracy at step 41: 0.0434772, 0.0585\n",
      "2017-11-10 14:31:49: Loss and accuracy at step 42: 0.0434451, 0.0525\n",
      "2017-11-10 14:31:51: Loss and accuracy at step 43: 0.043359, 0.0471\n",
      "2017-11-10 14:31:53: Loss and accuracy at step 44: 0.0432833, 0.0467\n",
      "2017-11-10 14:31:55: Loss and accuracy at step 45: 0.043156, 0.0425\n",
      "2017-11-10 14:31:57: Loss and accuracy at step 46: 0.0431308, 0.0423\n",
      "2017-11-10 14:31:59: Loss and accuracy at step 47: 0.0430429, 0.0375\n",
      "2017-11-10 14:32:01: Loss and accuracy at step 48: 0.0429841, 0.0474\n",
      "2017-11-10 14:32:03: Loss and accuracy at step 49: 0.0429224, 0.053\n",
      "2017-11-10 14:32:05: Loss and accuracy at step 50: 0.0428602, 0.0603\n",
      "2017-11-10 14:32:08: Loss and accuracy at step 51: 0.0427839, 0.0618\n",
      "2017-11-10 14:32:10: Loss and accuracy at step 52: 0.0426925, 0.0539\n",
      "2017-11-10 14:32:12: Loss and accuracy at step 53: 0.0426295, 0.0494\n",
      "2017-11-10 14:32:14: Loss and accuracy at step 54: 0.042543, 0.041\n",
      "2017-11-10 14:32:16: Loss and accuracy at step 55: 0.0424483, 0.0473\n",
      "2017-11-10 14:32:18: Loss and accuracy at step 56: 0.0424167, 0.0559\n",
      "2017-11-10 14:32:20: Loss and accuracy at step 57: 0.0423053, 0.0655\n",
      "2017-11-10 14:32:22: Loss and accuracy at step 58: 0.0422732, 0.0583\n",
      "2017-11-10 14:32:24: Loss and accuracy at step 59: 0.0421222, 0.0497\n",
      "2017-11-10 14:32:26: Loss and accuracy at step 60: 0.0421036, 0.0415\n",
      "2017-11-10 14:32:28: Loss and accuracy at step 61: 0.0420974, 0.0431\n",
      "2017-11-10 14:32:30: Loss and accuracy at step 62: 0.0419573, 0.0505\n",
      "2017-11-10 14:32:32: Loss and accuracy at step 63: 0.0419187, 0.0606\n",
      "2017-11-10 14:32:34: Loss and accuracy at step 64: 0.0418999, 0.0563\n",
      "2017-11-10 14:32:37: Loss and accuracy at step 65: 0.0417807, 0.047\n",
      "2017-11-10 14:32:39: Loss and accuracy at step 66: 0.0417315, 0.0402\n",
      "2017-11-10 14:32:41: Loss and accuracy at step 67: 0.0416466, 0.0353\n",
      "2017-11-10 14:32:43: Loss and accuracy at step 68: 0.0416497, 0.0393\n",
      "2017-11-10 14:32:45: Loss and accuracy at step 69: 0.0415719, 0.047\n",
      "2017-11-10 14:32:47: Loss and accuracy at step 70: 0.0415553, 0.0514\n",
      "2017-11-10 14:32:49: Loss and accuracy at step 71: 0.0414598, 0.0469\n",
      "2017-11-10 14:32:51: Loss and accuracy at step 72: 0.0414402, 0.0388\n",
      "2017-11-10 14:32:53: Loss and accuracy at step 73: 0.0413806, 0.0367\n",
      "2017-11-10 14:32:55: Loss and accuracy at step 74: 0.0413448, 0.0422\n",
      "2017-11-10 14:32:57: Loss and accuracy at step 75: 0.0412607, 0.035\n",
      "2017-11-10 14:32:59: Loss and accuracy at step 76: 0.041123, 0.0396\n",
      "2017-11-10 14:33:01: Loss and accuracy at step 77: 0.0410912, 0.0457\n",
      "2017-11-10 14:33:03: Loss and accuracy at step 78: 0.041121, 0.0429\n",
      "2017-11-10 14:33:05: Loss and accuracy at step 79: 0.0410573, 0.0414\n",
      "2017-11-10 14:33:08: Loss and accuracy at step 80: 0.0409621, 0.036\n",
      "2017-11-10 14:33:10: Loss and accuracy at step 81: 0.0409132, 0.0348\n",
      "2017-11-10 14:33:12: Loss and accuracy at step 82: 0.0409253, 0.0352\n",
      "2017-11-10 14:33:14: Loss and accuracy at step 83: 0.040901, 0.0383\n",
      "2017-11-10 14:33:16: Loss and accuracy at step 84: 0.0408276, 0.0313\n",
      "2017-11-10 14:33:18: Loss and accuracy at step 85: 0.0407889, 0.0322\n",
      "2017-11-10 14:33:20: Loss and accuracy at step 86: 0.0406892, 0.038\n",
      "2017-11-10 14:33:22: Loss and accuracy at step 87: 0.0406637, 0.0375\n",
      "2017-11-10 14:33:24: Loss and accuracy at step 88: 0.0405733, 0.0375\n",
      "2017-11-10 14:33:26: Loss and accuracy at step 89: 0.0405029, 0.0383\n",
      "2017-11-10 14:33:28: Loss and accuracy at step 90: 0.0404769, 0.0339\n",
      "2017-11-10 14:33:30: Loss and accuracy at step 91: 0.0404881, 0.033\n",
      "2017-11-10 14:33:32: Loss and accuracy at step 92: 0.0404794, 0.0319\n",
      "2017-11-10 14:33:34: Loss and accuracy at step 93: 0.0403559, 0.0364\n",
      "2017-11-10 14:33:36: Loss and accuracy at step 94: 0.040385, 0.045\n",
      "2017-11-10 14:33:39: Loss and accuracy at step 95: 0.0403012, 0.0382\n",
      "2017-11-10 14:33:41: Loss and accuracy at step 96: 0.0402322, 0.0285\n",
      "2017-11-10 14:33:43: Loss and accuracy at step 97: 0.04031, 0.0369\n",
      "2017-11-10 14:33:45: Loss and accuracy at step 98: 0.0401984, 0.0414\n",
      "2017-11-10 14:33:47: Loss and accuracy at step 99: 0.0402487, 0.0326\n",
      "2017-11-10 14:33:49: Loss and accuracy at step 100: 0.0400455, 0.0219\n",
      "2017-11-10 14:33:51: Loss and accuracy at step 101: 0.0400378, 0.0411\n",
      "2017-11-10 14:33:53: Loss and accuracy at step 102: 0.0400408, 0.038\n",
      "2017-11-10 14:33:55: Loss and accuracy at step 103: 0.0400845, 0.0263\n",
      "2017-11-10 14:33:57: Loss and accuracy at step 104: 0.0399874, 0.0333\n",
      "2017-11-10 14:33:59: Loss and accuracy at step 105: 0.0399566, 0.0397\n",
      "2017-11-10 14:34:01: Loss and accuracy at step 106: 0.0398734, 0.0343\n",
      "2017-11-10 14:34:03: Loss and accuracy at step 107: 0.03977, 0.0283\n",
      "2017-11-10 14:34:06: Loss and accuracy at step 108: 0.0397759, 0.036\n",
      "2017-11-10 14:34:08: Loss and accuracy at step 109: 0.0398256, 0.0355\n",
      "2017-11-10 14:34:10: Loss and accuracy at step 110: 0.0398502, 0.0311\n",
      "2017-11-10 14:34:12: Loss and accuracy at step 111: 0.0398218, 0.0304\n",
      "2017-11-10 14:34:14: Loss and accuracy at step 112: 0.0398121, 0.0311\n",
      "2017-11-10 14:34:16: Loss and accuracy at step 113: 0.0396905, 0.0429\n",
      "2017-11-10 14:34:18: Loss and accuracy at step 114: 0.039738, 0.0273\n",
      "2017-11-10 14:34:20: Loss and accuracy at step 115: 0.0396499, 0.0209\n",
      "2017-11-10 14:34:22: Loss and accuracy at step 116: 0.0395753, 0.0405\n",
      "2017-11-10 14:34:24: Loss and accuracy at step 117: 0.0396968, 0.0315\n",
      "2017-11-10 14:34:26: Loss and accuracy at step 118: 0.0396405, 0.0228\n",
      "2017-11-10 14:34:28: Loss and accuracy at step 119: 0.0395105, 0.0358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:34:31: Loss and accuracy at step 120: 0.0394767, 0.0315\n",
      "2017-11-10 14:34:33: Loss and accuracy at step 121: 0.0394613, 0.0235\n",
      "2017-11-10 14:34:35: Loss and accuracy at step 122: 0.0394703, 0.0229\n",
      "2017-11-10 14:34:37: Loss and accuracy at step 123: 0.0394517, 0.0328\n",
      "2017-11-10 14:34:39: Loss and accuracy at step 124: 0.0394302, 0.0283\n",
      "2017-11-10 14:34:41: Loss and accuracy at step 125: 0.0393682, 0.0206\n",
      "2017-11-10 14:34:43: Loss and accuracy at step 126: 0.0393593, 0.0264\n",
      "2017-11-10 14:34:45: Loss and accuracy at step 127: 0.0393721, 0.0347\n",
      "2017-11-10 14:34:47: Loss and accuracy at step 128: 0.0392923, 0.0223\n",
      "2017-11-10 14:34:49: Loss and accuracy at step 129: 0.0392085, 0.0277\n",
      "2017-11-10 14:34:51: Loss and accuracy at step 130: 0.0393568, 0.0348\n",
      "2017-11-10 14:34:53: Loss and accuracy at step 131: 0.0393104, 0.0259\n",
      "2017-11-10 14:34:55: Loss and accuracy at step 132: 0.0392658, 0.0206\n",
      "2017-11-10 14:34:57: Loss and accuracy at step 133: 0.0392168, 0.034\n",
      "2017-11-10 14:34:59: Loss and accuracy at step 134: 0.0391897, 0.0218\n",
      "2017-11-10 14:35:02: Loss and accuracy at step 135: 0.0391388, 0.0244\n",
      "2017-11-10 14:35:04: Loss and accuracy at step 136: 0.0390363, 0.0254\n",
      "2017-11-10 14:35:06: Loss and accuracy at step 137: 0.0390527, 0.0249\n",
      "2017-11-10 14:35:08: Loss and accuracy at step 138: 0.0390857, 0.0252\n",
      "2017-11-10 14:35:10: Loss and accuracy at step 139: 0.0390605, 0.0259\n",
      "2017-11-10 14:35:12: Loss and accuracy at step 140: 0.0389914, 0.0256\n",
      "2017-11-10 14:35:14: Loss and accuracy at step 141: 0.0390881, 0.0202\n",
      "2017-11-10 14:35:16: Loss and accuracy at step 142: 0.0391064, 0.0234\n",
      "2017-11-10 14:35:18: Loss and accuracy at step 143: 0.0390752, 0.0225\n",
      "2017-11-10 14:35:20: Loss and accuracy at step 144: 0.0389786, 0.0257\n",
      "2017-11-10 14:35:22: Loss and accuracy at step 145: 0.0389338, 0.0267\n",
      "2017-11-10 14:35:24: Loss and accuracy at step 146: 0.0389419, 0.0229\n",
      "2017-11-10 14:35:26: Loss and accuracy at step 147: 0.0389198, 0.0216\n",
      "2017-11-10 14:35:28: Loss and accuracy at step 148: 0.0389031, 0.0273\n",
      "2017-11-10 14:35:31: Loss and accuracy at step 149: 0.0388116, 0.0219\n",
      "2017-11-10 14:35:33: Loss and accuracy at step 150: 0.0389085, 0.0281\n",
      "2017-11-10 14:35:35: Loss and accuracy at step 151: 0.0389164, 0.0212\n",
      "2017-11-10 14:35:37: Loss and accuracy at step 152: 0.0388338, 0.0232\n",
      "2017-11-10 14:35:39: Loss and accuracy at step 153: 0.0388014, 0.034\n",
      "2017-11-10 14:35:41: Loss and accuracy at step 154: 0.0388386, 0.0187\n",
      "2017-11-10 14:35:43: Loss and accuracy at step 155: 0.0388865, 0.0261\n",
      "2017-11-10 14:35:45: Loss and accuracy at step 156: 0.0388308, 0.0217\n",
      "2017-11-10 14:35:47: Loss and accuracy at step 157: 0.0388088, 0.0237\n",
      "2017-11-10 14:35:49: Loss and accuracy at step 158: 0.0387308, 0.0323\n",
      "2017-11-10 14:35:51: Loss and accuracy at step 159: 0.0386972, 0.0186\n",
      "2017-11-10 14:35:53: Loss and accuracy at step 160: 0.0387766, 0.0252\n",
      "2017-11-10 14:35:55: Loss and accuracy at step 161: 0.0387435, 0.0269\n",
      "2017-11-10 14:35:58: Loss and accuracy at step 162: 0.0387471, 0.0236\n",
      "2017-11-10 14:36:00: Loss and accuracy at step 163: 0.0387497, 0.0259\n",
      "2017-11-10 14:36:02: Loss and accuracy at step 164: 0.0387355, 0.0251\n",
      "2017-11-10 14:36:04: Loss and accuracy at step 165: 0.0386459, 0.024\n",
      "2017-11-10 14:36:06: Loss and accuracy at step 166: 0.0386731, 0.0226\n",
      "2017-11-10 14:36:08: Loss and accuracy at step 167: 0.0386578, 0.0186\n",
      "2017-11-10 14:36:10: Loss and accuracy at step 168: 0.0386562, 0.0271\n",
      "2017-11-10 14:36:12: Loss and accuracy at step 169: 0.0385785, 0.0261\n",
      "2017-11-10 14:36:14: Loss and accuracy at step 170: 0.0385727, 0.0184\n",
      "2017-11-10 14:36:16: Loss and accuracy at step 171: 0.038566, 0.0283\n",
      "2017-11-10 14:36:18: Loss and accuracy at step 172: 0.0385227, 0.0202\n",
      "2017-11-10 14:36:21: Loss and accuracy at step 173: 0.0385499, 0.021\n",
      "2017-11-10 14:36:23: Loss and accuracy at step 174: 0.0386115, 0.0388\n",
      "2017-11-10 14:36:25: Loss and accuracy at step 175: 0.0385563, 0.0219\n",
      "2017-11-10 14:36:27: Loss and accuracy at step 176: 0.0385809, 0.0217\n",
      "2017-11-10 14:36:29: Loss and accuracy at step 177: 0.0385214, 0.0323\n",
      "2017-11-10 14:36:31: Loss and accuracy at step 178: 0.0385321, 0.0179\n",
      "2017-11-10 14:36:33: Loss and accuracy at step 179: 0.0384399, 0.024\n",
      "2017-11-10 14:36:35: Loss and accuracy at step 180: 0.0384099, 0.0294\n",
      "2017-11-10 14:36:37: Loss and accuracy at step 181: 0.0384365, 0.0206\n",
      "2017-11-10 14:36:39: Loss and accuracy at step 182: 0.0384598, 0.0208\n",
      "2017-11-10 14:36:41: Loss and accuracy at step 183: 0.0384779, 0.0241\n",
      "2017-11-10 14:36:43: Loss and accuracy at step 184: 0.0385476, 0.0191\n",
      "2017-11-10 14:36:46: Loss and accuracy at step 185: 0.0384406, 0.0264\n",
      "2017-11-10 14:36:48: Loss and accuracy at step 186: 0.0384767, 0.0193\n",
      "2017-11-10 14:36:50: Loss and accuracy at step 187: 0.0384309, 0.0226\n",
      "2017-11-10 14:36:52: Loss and accuracy at step 188: 0.0383894, 0.0321\n",
      "2017-11-10 14:36:54: Loss and accuracy at step 189: 0.0384578, 0.0176\n",
      "2017-11-10 14:36:56: Loss and accuracy at step 190: 0.0384025, 0.0271\n",
      "2017-11-10 14:36:58: Loss and accuracy at step 191: 0.0385234, 0.0256\n",
      "2017-11-10 14:37:00: Loss and accuracy at step 192: 0.0384392, 0.0202\n",
      "2017-11-10 14:37:02: Loss and accuracy at step 193: 0.0383439, 0.0183\n",
      "2017-11-10 14:37:04: Loss and accuracy at step 194: 0.0383317, 0.0267\n",
      "2017-11-10 14:37:06: Loss and accuracy at step 195: 0.0383192, 0.0209\n",
      "2017-11-10 14:37:08: Loss and accuracy at step 196: 0.0382991, 0.0214\n",
      "2017-11-10 14:37:10: Loss and accuracy at step 197: 0.0383072, 0.0223\n",
      "2017-11-10 14:37:13: Loss and accuracy at step 198: 0.0383304, 0.025\n",
      "2017-11-10 14:37:15: Loss and accuracy at step 199: 0.0383681, 0.0292\n",
      "2017-11-10 14:37:17: Loss and accuracy at step 200: 0.0383907, 0.0171\n",
      "2017-11-10 14:37:19: Loss and accuracy at step 201: 0.0383006, 0.0258\n",
      "2017-11-10 14:37:21: Loss and accuracy at step 202: 0.0383316, 0.0294\n",
      "2017-11-10 14:37:23: Loss and accuracy at step 203: 0.0382491, 0.017\n",
      "2017-11-10 14:37:25: Loss and accuracy at step 204: 0.0383457, 0.0212\n",
      "2017-11-10 14:37:27: Loss and accuracy at step 205: 0.0383588, 0.0295\n",
      "2017-11-10 14:37:29: Loss and accuracy at step 206: 0.0382735, 0.0183\n",
      "2017-11-10 14:37:31: Loss and accuracy at step 207: 0.0382312, 0.0228\n",
      "2017-11-10 14:37:33: Loss and accuracy at step 208: 0.0382279, 0.0292\n",
      "2017-11-10 14:37:36: Loss and accuracy at step 209: 0.0382457, 0.0174\n",
      "2017-11-10 14:37:38: Loss and accuracy at step 210: 0.0383225, 0.0288\n",
      "2017-11-10 14:37:40: Loss and accuracy at step 211: 0.0383227, 0.0236\n",
      "2017-11-10 14:37:42: Loss and accuracy at step 212: 0.0382368, 0.0193\n",
      "2017-11-10 14:37:44: Loss and accuracy at step 213: 0.0382992, 0.0229\n",
      "2017-11-10 14:37:46: Loss and accuracy at step 214: 0.0382815, 0.0195\n",
      "2017-11-10 14:37:48: Loss and accuracy at step 215: 0.0382341, 0.0204\n",
      "2017-11-10 14:37:50: Loss and accuracy at step 216: 0.0381823, 0.0248\n",
      "2017-11-10 14:37:52: Loss and accuracy at step 217: 0.0383082, 0.0203\n",
      "2017-11-10 14:37:54: Loss and accuracy at step 218: 0.0383533, 0.0193\n",
      "2017-11-10 14:37:56: Loss and accuracy at step 219: 0.0382482, 0.0274\n",
      "2017-11-10 14:37:58: Loss and accuracy at step 220: 0.0382678, 0.0146\n",
      "2017-11-10 14:38:00: Loss and accuracy at step 221: 0.0382078, 0.0205\n",
      "2017-11-10 14:38:02: Loss and accuracy at step 222: 0.0382604, 0.0227\n",
      "2017-11-10 14:38:05: Loss and accuracy at step 223: 0.0382282, 0.017\n",
      "2017-11-10 14:38:07: Loss and accuracy at step 224: 0.0381613, 0.025\n",
      "2017-11-10 14:38:09: Loss and accuracy at step 225: 0.0382425, 0.0208\n",
      "2017-11-10 14:38:11: Loss and accuracy at step 226: 0.038191, 0.0212\n",
      "2017-11-10 14:38:13: Loss and accuracy at step 227: 0.0382288, 0.0209\n",
      "2017-11-10 14:38:15: Loss and accuracy at step 228: 0.0382059, 0.025\n",
      "2017-11-10 14:38:17: Loss and accuracy at step 229: 0.0382086, 0.0225\n",
      "2017-11-10 14:38:19: Loss and accuracy at step 230: 0.0381658, 0.0168\n",
      "2017-11-10 14:38:21: Loss and accuracy at step 231: 0.0381622, 0.0279\n",
      "2017-11-10 14:38:23: Loss and accuracy at step 232: 0.0382208, 0.0189\n",
      "2017-11-10 14:38:25: Loss and accuracy at step 233: 0.0381635, 0.0209\n",
      "2017-11-10 14:38:28: Loss and accuracy at step 234: 0.0381412, 0.0248\n",
      "2017-11-10 14:38:30: Loss and accuracy at step 235: 0.0380442, 0.0228\n",
      "2017-11-10 14:38:32: Loss and accuracy at step 236: 0.0381956, 0.0163\n",
      "2017-11-10 14:38:34: Loss and accuracy at step 237: 0.0381349, 0.0273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:38:36: Loss and accuracy at step 238: 0.0381651, 0.0218\n",
      "2017-11-10 14:38:38: Loss and accuracy at step 239: 0.0380896, 0.0158\n",
      "2017-11-10 14:38:40: Loss and accuracy at step 240: 0.0380644, 0.0285\n",
      "2017-11-10 14:38:42: Loss and accuracy at step 241: 0.0380941, 0.0183\n",
      "2017-11-10 14:38:44: Loss and accuracy at step 242: 0.038111, 0.0243\n",
      "2017-11-10 14:38:46: Loss and accuracy at step 243: 0.0381304, 0.0209\n",
      "2017-11-10 14:38:48: Loss and accuracy at step 244: 0.0380736, 0.0203\n",
      "2017-11-10 14:38:50: Loss and accuracy at step 245: 0.0379929, 0.0271\n",
      "2017-11-10 14:38:53: Loss and accuracy at step 246: 0.0380739, 0.0164\n",
      "2017-11-10 14:38:55: Loss and accuracy at step 247: 0.0380343, 0.0167\n",
      "2017-11-10 14:38:57: Loss and accuracy at step 248: 0.0380299, 0.029\n",
      "2017-11-10 14:38:59: Loss and accuracy at step 249: 0.0380985, 0.0179\n",
      "2017-11-10 14:39:01: Loss and accuracy at step 250: 0.0380822, 0.0174\n",
      "2017-11-10 14:39:03: Loss and accuracy at step 251: 0.0380748, 0.0303\n",
      "2017-11-10 14:39:05: Loss and accuracy at step 252: 0.0381117, 0.0203\n",
      "2017-11-10 14:39:07: Loss and accuracy at step 253: 0.038087, 0.02\n",
      "2017-11-10 14:39:09: Loss and accuracy at step 254: 0.0380618, 0.0304\n",
      "2017-11-10 14:39:11: Loss and accuracy at step 255: 0.0380842, 0.0161\n",
      "2017-11-10 14:39:13: Loss and accuracy at step 256: 0.0380026, 0.0238\n",
      "2017-11-10 14:39:15: Loss and accuracy at step 257: 0.0380012, 0.0276\n",
      "2017-11-10 14:39:18: Loss and accuracy at step 258: 0.0379997, 0.0152\n",
      "2017-11-10 14:39:20: Loss and accuracy at step 259: 0.0380519, 0.0281\n",
      "2017-11-10 14:39:22: Loss and accuracy at step 260: 0.0379397, 0.0242\n",
      "2017-11-10 14:39:24: Loss and accuracy at step 261: 0.0380119, 0.0182\n",
      "2017-11-10 14:39:26: Loss and accuracy at step 262: 0.0379088, 0.0227\n",
      "2017-11-10 14:39:28: Loss and accuracy at step 263: 0.0380216, 0.0215\n",
      "2017-11-10 14:39:30: Loss and accuracy at step 264: 0.0379784, 0.0212\n",
      "2017-11-10 14:39:32: Loss and accuracy at step 265: 0.0379453, 0.0217\n",
      "2017-11-10 14:39:34: Loss and accuracy at step 266: 0.0379826, 0.0154\n",
      "2017-11-10 14:39:36: Loss and accuracy at step 267: 0.0380452, 0.0244\n",
      "2017-11-10 14:39:38: Loss and accuracy at step 268: 0.038, 0.0196\n",
      "2017-11-10 14:39:41: Loss and accuracy at step 269: 0.0380233, 0.016\n",
      "2017-11-10 14:39:43: Loss and accuracy at step 270: 0.0379873, 0.028\n",
      "2017-11-10 14:39:45: Loss and accuracy at step 271: 0.0380243, 0.0175\n",
      "2017-11-10 14:39:47: Loss and accuracy at step 272: 0.0379338, 0.0232\n",
      "2017-11-10 14:39:49: Loss and accuracy at step 273: 0.0379503, 0.0275\n",
      "2017-11-10 14:39:51: Loss and accuracy at step 274: 0.0379511, 0.0162\n",
      "2017-11-10 14:39:53: Loss and accuracy at step 275: 0.0379136, 0.021\n",
      "2017-11-10 14:39:55: Loss and accuracy at step 276: 0.037977, 0.0302\n",
      "2017-11-10 14:39:57: Loss and accuracy at step 277: 0.0379187, 0.0162\n",
      "2017-11-10 14:39:59: Loss and accuracy at step 278: 0.0378996, 0.0213\n",
      "2017-11-10 14:40:01: Loss and accuracy at step 279: 0.0379162, 0.0249\n",
      "2017-11-10 14:40:03: Loss and accuracy at step 280: 0.0378211, 0.0215\n",
      "2017-11-10 14:40:06: Loss and accuracy at step 281: 0.0378475, 0.0203\n",
      "2017-11-10 14:40:08: Loss and accuracy at step 282: 0.0378157, 0.0226\n",
      "2017-11-10 14:40:10: Loss and accuracy at step 283: 0.0379477, 0.0157\n",
      "2017-11-10 14:40:12: Loss and accuracy at step 284: 0.0379923, 0.0196\n",
      "2017-11-10 14:40:14: Loss and accuracy at step 285: 0.0378707, 0.0307\n",
      "2017-11-10 14:40:16: Loss and accuracy at step 286: 0.0378468, 0.0199\n",
      "2017-11-10 14:40:18: Loss and accuracy at step 287: 0.0379364, 0.0152\n",
      "2017-11-10 14:40:20: Loss and accuracy at step 288: 0.0379255, 0.036\n",
      "2017-11-10 14:40:22: Loss and accuracy at step 289: 0.0378653, 0.0163\n",
      "2017-11-10 14:40:24: Loss and accuracy at step 290: 0.0378404, 0.0188\n",
      "2017-11-10 14:40:27: Loss and accuracy at step 291: 0.0378598, 0.0269\n",
      "2017-11-10 14:40:29: Loss and accuracy at step 292: 0.0378364, 0.0194\n",
      "2017-11-10 14:40:31: Loss and accuracy at step 293: 0.0378926, 0.0198\n",
      "2017-11-10 14:40:33: Loss and accuracy at step 294: 0.0378987, 0.0269\n",
      "2017-11-10 14:40:35: Loss and accuracy at step 295: 0.0378807, 0.0186\n",
      "2017-11-10 14:40:37: Loss and accuracy at step 296: 0.0378799, 0.0209\n",
      "2017-11-10 14:40:39: Loss and accuracy at step 297: 0.0379111, 0.0211\n",
      "2017-11-10 14:40:41: Loss and accuracy at step 298: 0.0378939, 0.0192\n",
      "2017-11-10 14:40:43: Loss and accuracy at step 299: 0.0377572, 0.0245\n",
      "2017-11-10 14:40:45: Loss and accuracy at step 300: 0.0378557, 0.0206\n",
      "2017-11-10 14:40:47: Loss and accuracy at step 301: 0.0379195, 0.0164\n",
      "2017-11-10 14:40:49: Loss and accuracy at step 302: 0.0378461, 0.028\n",
      "2017-11-10 14:40:52: Loss and accuracy at step 303: 0.0378297, 0.0146\n",
      "2017-11-10 14:40:54: Loss and accuracy at step 304: 0.0378506, 0.0227\n",
      "2017-11-10 14:40:56: Loss and accuracy at step 305: 0.0378755, 0.0303\n",
      "2017-11-10 14:40:58: Loss and accuracy at step 306: 0.0378227, 0.0129\n",
      "2017-11-10 14:41:00: Loss and accuracy at step 307: 0.0378357, 0.0258\n",
      "2017-11-10 14:41:02: Loss and accuracy at step 308: 0.0378787, 0.0203\n",
      "2017-11-10 14:41:04: Loss and accuracy at step 309: 0.0378974, 0.0232\n",
      "2017-11-10 14:41:06: Loss and accuracy at step 310: 0.0378636, 0.0141\n",
      "2017-11-10 14:41:08: Loss and accuracy at step 311: 0.0378147, 0.0236\n",
      "2017-11-10 14:41:10: Loss and accuracy at step 312: 0.037808, 0.0304\n",
      "2017-11-10 14:41:12: Loss and accuracy at step 313: 0.037807, 0.0086\n",
      "2017-11-10 14:41:14: Loss and accuracy at step 314: 0.0378858, 0.0239\n",
      "2017-11-10 14:41:17: Loss and accuracy at step 315: 0.0377669, 0.027\n",
      "2017-11-10 14:41:19: Loss and accuracy at step 316: 0.0377374, 0.0164\n",
      "2017-11-10 14:41:21: Loss and accuracy at step 317: 0.0377845, 0.0181\n",
      "2017-11-10 14:41:23: Loss and accuracy at step 318: 0.0376665, 0.0364\n",
      "2017-11-10 14:41:25: Loss and accuracy at step 319: 0.0378405, 0.0175\n",
      "2017-11-10 14:41:27: Loss and accuracy at step 320: 0.0378306, 0.0207\n",
      "2017-11-10 14:41:29: Loss and accuracy at step 321: 0.0379222, 0.0307\n",
      "2017-11-10 14:41:31: Loss and accuracy at step 322: 0.0378102, 0.014\n",
      "2017-11-10 14:41:33: Loss and accuracy at step 323: 0.0378086, 0.0271\n",
      "2017-11-10 14:41:35: Loss and accuracy at step 324: 0.0379094, 0.0238\n",
      "2017-11-10 14:41:37: Loss and accuracy at step 325: 0.0378442, 0.0153\n",
      "2017-11-10 14:41:40: Loss and accuracy at step 326: 0.0377903, 0.0191\n",
      "2017-11-10 14:41:42: Loss and accuracy at step 327: 0.0377878, 0.0233\n",
      "2017-11-10 14:41:44: Loss and accuracy at step 328: 0.0377536, 0.0248\n",
      "2017-11-10 14:41:46: Loss and accuracy at step 329: 0.037801, 0.0137\n",
      "2017-11-10 14:41:48: Loss and accuracy at step 330: 0.0377514, 0.0246\n",
      "2017-11-10 14:41:50: Loss and accuracy at step 331: 0.0377319, 0.0291\n",
      "2017-11-10 14:41:52: Loss and accuracy at step 332: 0.0379539, 0.0122\n",
      "2017-11-10 14:41:54: Loss and accuracy at step 333: 0.0377921, 0.0186\n",
      "2017-11-10 14:41:56: Loss and accuracy at step 334: 0.0377456, 0.0265\n",
      "2017-11-10 14:41:58: Loss and accuracy at step 335: 0.0377138, 0.022\n",
      "2017-11-10 14:42:00: Loss and accuracy at step 336: 0.0377868, 0.0208\n",
      "2017-11-10 14:42:02: Loss and accuracy at step 337: 0.0377574, 0.018\n",
      "2017-11-10 14:42:04: Loss and accuracy at step 338: 0.0377938, 0.0295\n",
      "2017-11-10 14:42:07: Loss and accuracy at step 339: 0.037731, 0.0122\n",
      "2017-11-10 14:42:09: Loss and accuracy at step 340: 0.0377478, 0.0237\n",
      "2017-11-10 14:42:11: Loss and accuracy at step 341: 0.0377572, 0.027\n",
      "2017-11-10 14:42:13: Loss and accuracy at step 342: 0.037709, 0.0175\n",
      "2017-11-10 14:42:15: Loss and accuracy at step 343: 0.037725, 0.0214\n",
      "2017-11-10 14:42:17: Loss and accuracy at step 344: 0.0377259, 0.0252\n",
      "2017-11-10 14:42:19: Loss and accuracy at step 345: 0.0376944, 0.025\n",
      "2017-11-10 14:42:21: Loss and accuracy at step 346: 0.0376984, 0.0153\n",
      "2017-11-10 14:42:23: Loss and accuracy at step 347: 0.0377373, 0.0299\n",
      "2017-11-10 14:42:26: Loss and accuracy at step 348: 0.0376239, 0.0177\n",
      "2017-11-10 14:42:28: Loss and accuracy at step 349: 0.0377317, 0.0175\n",
      "2017-11-10 14:42:30: Loss and accuracy at step 350: 0.0377891, 0.0197\n",
      "2017-11-10 14:42:32: Loss and accuracy at step 351: 0.0376305, 0.0201\n",
      "2017-11-10 14:42:34: Loss and accuracy at step 352: 0.0376609, 0.0234\n",
      "2017-11-10 14:42:36: Loss and accuracy at step 353: 0.037783, 0.0153\n",
      "2017-11-10 14:42:38: Loss and accuracy at step 354: 0.0377651, 0.0211\n",
      "2017-11-10 14:42:40: Loss and accuracy at step 355: 0.0379015, 0.0183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:42:42: Loss and accuracy at step 356: 0.0376803, 0.021\n",
      "2017-11-10 14:42:44: Loss and accuracy at step 357: 0.0377147, 0.0223\n",
      "2017-11-10 14:42:46: Loss and accuracy at step 358: 0.0376877, 0.0132\n",
      "2017-11-10 14:42:48: Loss and accuracy at step 359: 0.0377075, 0.024\n",
      "2017-11-10 14:42:51: Loss and accuracy at step 360: 0.0377018, 0.0195\n",
      "2017-11-10 14:42:53: Loss and accuracy at step 361: 0.0375855, 0.014\n",
      "2017-11-10 14:42:55: Loss and accuracy at step 362: 0.0377411, 0.0251\n",
      "2017-11-10 14:42:57: Loss and accuracy at step 363: 0.0377246, 0.0164\n",
      "2017-11-10 14:42:59: Loss and accuracy at step 364: 0.0376231, 0.0165\n",
      "2017-11-10 14:43:01: Loss and accuracy at step 365: 0.0375867, 0.0127\n",
      "2017-11-10 14:43:03: Loss and accuracy at step 366: 0.0376023, 0.0246\n",
      "2017-11-10 14:43:05: Loss and accuracy at step 367: 0.0376761, 0.0243\n",
      "2017-11-10 14:43:07: Loss and accuracy at step 368: 0.0377555, 0.0107\n",
      "2017-11-10 14:43:09: Loss and accuracy at step 369: 0.0377771, 0.0266\n",
      "2017-11-10 14:43:11: Loss and accuracy at step 370: 0.0377802, 0.015\n",
      "2017-11-10 14:43:13: Loss and accuracy at step 371: 0.0376574, 0.0191\n",
      "2017-11-10 14:43:16: Loss and accuracy at step 372: 0.0376512, 0.0204\n",
      "2017-11-10 14:43:18: Loss and accuracy at step 373: 0.037716, 0.0138\n",
      "2017-11-10 14:43:20: Loss and accuracy at step 374: 0.0375621, 0.0223\n",
      "2017-11-10 14:43:22: Loss and accuracy at step 375: 0.0377484, 0.014\n",
      "2017-11-10 14:43:24: Loss and accuracy at step 376: 0.0376585, 0.0199\n",
      "2017-11-10 14:43:26: Loss and accuracy at step 377: 0.0376401, 0.0257\n",
      "2017-11-10 14:43:28: Loss and accuracy at step 378: 0.037755, 0.0153\n",
      "2017-11-10 14:43:30: Loss and accuracy at step 379: 0.0377343, 0.021\n",
      "2017-11-10 14:43:32: Loss and accuracy at step 380: 0.0376284, 0.023\n",
      "2017-11-10 14:43:34: Loss and accuracy at step 381: 0.0377076, 0.0161\n",
      "2017-11-10 14:43:36: Loss and accuracy at step 382: 0.0376829, 0.0241\n",
      "2017-11-10 14:43:38: Loss and accuracy at step 383: 0.037646, 0.017\n",
      "2017-11-10 14:43:40: Loss and accuracy at step 384: 0.0376332, 0.0189\n",
      "2017-11-10 14:43:43: Loss and accuracy at step 385: 0.0376805, 0.0142\n",
      "2017-11-10 14:43:45: Loss and accuracy at step 386: 0.0376501, 0.0253\n",
      "2017-11-10 14:43:47: Loss and accuracy at step 387: 0.0377514, 0.0166\n",
      "2017-11-10 14:43:49: Loss and accuracy at step 388: 0.0376029, 0.0228\n",
      "2017-11-10 14:43:51: Loss and accuracy at step 389: 0.0377754, 0.0199\n",
      "2017-11-10 14:43:53: Loss and accuracy at step 390: 0.0377002, 0.0163\n",
      "2017-11-10 14:43:55: Loss and accuracy at step 391: 0.0376859, 0.0266\n",
      "2017-11-10 14:43:57: Loss and accuracy at step 392: 0.0376109, 0.0124\n",
      "2017-11-10 14:43:59: Loss and accuracy at step 393: 0.0375185, 0.0189\n",
      "2017-11-10 14:44:01: Loss and accuracy at step 394: 0.0375492, 0.0312\n",
      "2017-11-10 14:44:03: Loss and accuracy at step 395: 0.037609, 0.0126\n",
      "2017-11-10 14:44:05: Loss and accuracy at step 396: 0.0375814, 0.0198\n",
      "2017-11-10 14:44:08: Loss and accuracy at step 397: 0.0376485, 0.0249\n",
      "2017-11-10 14:44:10: Loss and accuracy at step 398: 0.037624, 0.0253\n",
      "2017-11-10 14:44:12: Loss and accuracy at step 399: 0.0375545, 0.0164\n",
      "2017-11-10 14:44:14: Loss and accuracy at step 400: 0.0376548, 0.0186\n",
      "2017-11-10 14:44:16: Loss and accuracy at step 401: 0.0375906, 0.0259\n",
      "2017-11-10 14:44:18: Loss and accuracy at step 402: 0.0376856, 0.0137\n",
      "2017-11-10 14:44:20: Loss and accuracy at step 403: 0.0376247, 0.0212\n",
      "2017-11-10 14:44:22: Loss and accuracy at step 404: 0.0375708, 0.0152\n",
      "2017-11-10 14:44:24: Loss and accuracy at step 405: 0.0376033, 0.023\n",
      "2017-11-10 14:44:26: Loss and accuracy at step 406: 0.0376435, 0.0175\n",
      "2017-11-10 14:44:29: Loss and accuracy at step 407: 0.0376557, 0.0153\n",
      "2017-11-10 14:44:31: Loss and accuracy at step 408: 0.0377637, 0.0299\n",
      "2017-11-10 14:44:33: Loss and accuracy at step 409: 0.0375864, 0.0125\n",
      "2017-11-10 14:44:35: Loss and accuracy at step 410: 0.0375294, 0.0257\n",
      "2017-11-10 14:44:37: Loss and accuracy at step 411: 0.0375204, 0.0217\n",
      "2017-11-10 14:44:39: Loss and accuracy at step 412: 0.0375844, 0.0133\n",
      "2017-11-10 14:44:41: Loss and accuracy at step 413: 0.0376838, 0.0256\n",
      "2017-11-10 14:44:43: Loss and accuracy at step 414: 0.0375587, 0.0134\n",
      "2017-11-10 14:44:45: Loss and accuracy at step 415: 0.0376417, 0.0132\n",
      "2017-11-10 14:44:47: Loss and accuracy at step 416: 0.0375162, 0.0191\n",
      "2017-11-10 14:44:49: Loss and accuracy at step 417: 0.0377021, 0.0214\n",
      "2017-11-10 14:44:51: Loss and accuracy at step 418: 0.0376373, 0.0188\n",
      "2017-11-10 14:44:54: Loss and accuracy at step 419: 0.0375374, 0.0175\n",
      "2017-11-10 14:44:56: Loss and accuracy at step 420: 0.0375735, 0.0197\n",
      "2017-11-10 14:44:58: Loss and accuracy at step 421: 0.0376061, 0.0148\n",
      "2017-11-10 14:45:00: Loss and accuracy at step 422: 0.0376249, 0.0285\n",
      "2017-11-10 14:45:02: Loss and accuracy at step 423: 0.0376355, 0.0133\n",
      "2017-11-10 14:45:04: Loss and accuracy at step 424: 0.037664, 0.0194\n",
      "2017-11-10 14:45:06: Loss and accuracy at step 425: 0.0376477, 0.0253\n",
      "2017-11-10 14:45:08: Loss and accuracy at step 426: 0.0375682, 0.0121\n",
      "2017-11-10 14:45:10: Loss and accuracy at step 427: 0.0375685, 0.0226\n",
      "2017-11-10 14:45:12: Loss and accuracy at step 428: 0.0375877, 0.0144\n",
      "2017-11-10 14:45:14: Loss and accuracy at step 429: 0.0375983, 0.0204\n",
      "2017-11-10 14:45:16: Loss and accuracy at step 430: 0.0375892, 0.0197\n",
      "2017-11-10 14:45:19: Loss and accuracy at step 431: 0.0375704, 0.013\n",
      "2017-11-10 14:45:21: Loss and accuracy at step 432: 0.0376634, 0.0225\n",
      "2017-11-10 14:45:23: Loss and accuracy at step 433: 0.0375515, 0.0186\n",
      "2017-11-10 14:45:25: Loss and accuracy at step 434: 0.0376094, 0.0126\n",
      "2017-11-10 14:45:27: Loss and accuracy at step 435: 0.0376418, 0.0278\n",
      "2017-11-10 14:45:29: Loss and accuracy at step 436: 0.0376114, 0.011\n",
      "2017-11-10 14:45:31: Loss and accuracy at step 437: 0.0375599, 0.0245\n",
      "2017-11-10 14:45:33: Loss and accuracy at step 438: 0.0375322, 0.0221\n",
      "2017-11-10 14:45:35: Loss and accuracy at step 439: 0.037554, 0.0138\n",
      "2017-11-10 14:45:37: Loss and accuracy at step 440: 0.0375324, 0.0272\n",
      "2017-11-10 14:45:39: Loss and accuracy at step 441: 0.0375115, 0.0147\n",
      "2017-11-10 14:45:41: Loss and accuracy at step 442: 0.0374792, 0.024\n",
      "2017-11-10 14:45:44: Loss and accuracy at step 443: 0.037533, 0.0181\n",
      "2017-11-10 14:45:46: Loss and accuracy at step 444: 0.0375713, 0.0165\n",
      "2017-11-10 14:45:48: Loss and accuracy at step 445: 0.0375751, 0.017\n",
      "2017-11-10 14:45:50: Loss and accuracy at step 446: 0.0374724, 0.0191\n",
      "2017-11-10 14:45:52: Loss and accuracy at step 447: 0.0376437, 0.0221\n",
      "2017-11-10 14:45:54: Loss and accuracy at step 448: 0.0375253, 0.0168\n",
      "2017-11-10 14:45:56: Loss and accuracy at step 449: 0.0376194, 0.0173\n",
      "2017-11-10 14:45:58: Loss and accuracy at step 450: 0.037517, 0.0241\n",
      "2017-11-10 14:46:00: Loss and accuracy at step 451: 0.0375509, 0.0102\n",
      "2017-11-10 14:46:02: Loss and accuracy at step 452: 0.0375683, 0.0302\n",
      "2017-11-10 14:46:04: Loss and accuracy at step 453: 0.0375448, 0.0165\n",
      "2017-11-10 14:46:06: Loss and accuracy at step 454: 0.037555, 0.0103\n",
      "2017-11-10 14:46:09: Loss and accuracy at step 455: 0.037498, 0.0316\n",
      "2017-11-10 14:46:11: Loss and accuracy at step 456: 0.0375263, 0.0151\n",
      "2017-11-10 14:46:13: Loss and accuracy at step 457: 0.0374691, 0.0155\n",
      "2017-11-10 14:46:15: Loss and accuracy at step 458: 0.0375622, 0.0242\n",
      "2017-11-10 14:46:17: Loss and accuracy at step 459: 0.0375727, 0.0171\n",
      "2017-11-10 14:46:19: Loss and accuracy at step 460: 0.0374698, 0.0194\n",
      "2017-11-10 14:46:21: Loss and accuracy at step 461: 0.0374876, 0.0147\n",
      "2017-11-10 14:46:23: Loss and accuracy at step 462: 0.0375286, 0.0235\n",
      "2017-11-10 14:46:25: Loss and accuracy at step 463: 0.0374178, 0.0202\n",
      "2017-11-10 14:46:28: Loss and accuracy at step 464: 0.0374591, 0.0134\n",
      "2017-11-10 14:46:30: Loss and accuracy at step 465: 0.0374264, 0.0248\n",
      "2017-11-10 14:46:32: Loss and accuracy at step 466: 0.037499, 0.0134\n",
      "2017-11-10 14:46:34: Loss and accuracy at step 467: 0.0375474, 0.0208\n",
      "2017-11-10 14:46:36: Loss and accuracy at step 468: 0.0375334, 0.0181\n",
      "2017-11-10 14:46:38: Loss and accuracy at step 469: 0.037504, 0.015\n",
      "2017-11-10 14:46:40: Loss and accuracy at step 470: 0.0375067, 0.0215\n",
      "2017-11-10 14:46:42: Loss and accuracy at step 471: 0.03751, 0.0122\n",
      "2017-11-10 14:46:44: Loss and accuracy at step 472: 0.0375148, 0.0219\n",
      "2017-11-10 14:46:46: Loss and accuracy at step 473: 0.0374726, 0.0145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:46:48: Loss and accuracy at step 474: 0.0374174, 0.0183\n",
      "2017-11-10 14:46:50: Loss and accuracy at step 475: 0.0374606, 0.0243\n",
      "2017-11-10 14:46:53: Loss and accuracy at step 476: 0.0374728, 0.0106\n",
      "2017-11-10 14:46:55: Loss and accuracy at step 477: 0.037494, 0.0221\n",
      "2017-11-10 14:46:57: Loss and accuracy at step 478: 0.0374981, 0.0232\n",
      "2017-11-10 14:46:59: Loss and accuracy at step 479: 0.0375274, 0.0131\n",
      "2017-11-10 14:47:01: Loss and accuracy at step 480: 0.0375019, 0.0196\n",
      "2017-11-10 14:47:03: Loss and accuracy at step 481: 0.0375662, 0.0217\n",
      "2017-11-10 14:47:05: Loss and accuracy at step 482: 0.0375107, 0.0157\n",
      "2017-11-10 14:47:07: Loss and accuracy at step 483: 0.0374838, 0.0116\n",
      "2017-11-10 14:47:09: Loss and accuracy at step 484: 0.037439, 0.024\n",
      "2017-11-10 14:47:11: Loss and accuracy at step 485: 0.0373736, 0.0202\n",
      "2017-11-10 14:47:13: Loss and accuracy at step 486: 0.0374414, 0.0138\n",
      "2017-11-10 14:47:16: Loss and accuracy at step 487: 0.0373282, 0.0175\n",
      "2017-11-10 14:47:18: Loss and accuracy at step 488: 0.0373839, 0.0193\n",
      "2017-11-10 14:47:20: Loss and accuracy at step 489: 0.0374599, 0.0158\n",
      "2017-11-10 14:47:22: Loss and accuracy at step 490: 0.0374427, 0.0154\n",
      "2017-11-10 14:47:24: Loss and accuracy at step 491: 0.0373982, 0.0224\n",
      "2017-11-10 14:47:26: Loss and accuracy at step 492: 0.037461, 0.0163\n",
      "2017-11-10 14:47:28: Loss and accuracy at step 493: 0.037425, 0.0169\n",
      "2017-11-10 14:47:30: Loss and accuracy at step 494: 0.0375143, 0.0193\n",
      "2017-11-10 14:47:32: Loss and accuracy at step 495: 0.0375078, 0.0125\n",
      "2017-11-10 14:47:34: Loss and accuracy at step 496: 0.0374766, 0.0206\n",
      "2017-11-10 14:47:36: Loss and accuracy at step 497: 0.0374557, 0.0228\n",
      "2017-11-10 14:47:39: Loss and accuracy at step 498: 0.0374796, 0.0132\n",
      "2017-11-10 14:47:41: Loss and accuracy at step 499: 0.0374232, 0.0189\n",
      "2017-11-10 14:47:43: Loss and accuracy at step 500: 0.0374848, 0.0215\n",
      "2017-11-10 14:47:45: Loss and accuracy at step 501: 0.0374216, 0.0135\n",
      "2017-11-10 14:47:47: Loss and accuracy at step 502: 0.0374494, 0.0166\n",
      "2017-11-10 14:47:49: Loss and accuracy at step 503: 0.0375272, 0.031\n",
      "2017-11-10 14:47:51: Loss and accuracy at step 504: 0.0374498, 0.0098\n",
      "2017-11-10 14:47:53: Loss and accuracy at step 505: 0.0374401, 0.0133\n",
      "2017-11-10 14:47:55: Loss and accuracy at step 506: 0.0374474, 0.0264\n",
      "2017-11-10 14:47:57: Loss and accuracy at step 507: 0.0375016, 0.0154\n",
      "2017-11-10 14:47:59: Loss and accuracy at step 508: 0.0374285, 0.0158\n",
      "2017-11-10 14:48:01: Loss and accuracy at step 509: 0.0374434, 0.0267\n",
      "2017-11-10 14:48:04: Loss and accuracy at step 510: 0.0373846, 0.0141\n",
      "2017-11-10 14:48:06: Loss and accuracy at step 511: 0.0374021, 0.0158\n",
      "2017-11-10 14:48:08: Loss and accuracy at step 512: 0.0375289, 0.0267\n",
      "2017-11-10 14:48:10: Loss and accuracy at step 513: 0.0375534, 0.0147\n",
      "2017-11-10 14:48:12: Loss and accuracy at step 514: 0.0374767, 0.0159\n",
      "2017-11-10 14:48:14: Loss and accuracy at step 515: 0.0374641, 0.0308\n",
      "2017-11-10 14:48:16: Loss and accuracy at step 516: 0.0374418, 0.0136\n",
      "2017-11-10 14:48:18: Loss and accuracy at step 517: 0.0374996, 0.0154\n",
      "2017-11-10 14:48:20: Loss and accuracy at step 518: 0.0374419, 0.0224\n",
      "2017-11-10 14:48:23: Loss and accuracy at step 519: 0.0375122, 0.0139\n",
      "2017-11-10 14:48:25: Loss and accuracy at step 520: 0.0374469, 0.0167\n",
      "2017-11-10 14:48:27: Loss and accuracy at step 521: 0.0374425, 0.0199\n",
      "2017-11-10 14:48:29: Loss and accuracy at step 522: 0.0374079, 0.0126\n",
      "2017-11-10 14:48:31: Loss and accuracy at step 523: 0.0374447, 0.0269\n",
      "2017-11-10 14:48:33: Loss and accuracy at step 524: 0.037419, 0.0138\n",
      "2017-11-10 14:48:35: Loss and accuracy at step 525: 0.0375096, 0.0147\n",
      "2017-11-10 14:48:37: Loss and accuracy at step 526: 0.0374985, 0.0224\n",
      "2017-11-10 14:48:39: Loss and accuracy at step 527: 0.037411, 0.014\n",
      "2017-11-10 14:48:41: Loss and accuracy at step 528: 0.0374222, 0.0193\n",
      "2017-11-10 14:48:43: Loss and accuracy at step 529: 0.037479, 0.0137\n",
      "2017-11-10 14:48:45: Loss and accuracy at step 530: 0.0374678, 0.0208\n",
      "2017-11-10 14:48:48: Loss and accuracy at step 531: 0.0375038, 0.0151\n",
      "2017-11-10 14:48:50: Loss and accuracy at step 532: 0.0375004, 0.0138\n",
      "2017-11-10 14:48:52: Loss and accuracy at step 533: 0.0375252, 0.0234\n",
      "2017-11-10 14:48:54: Loss and accuracy at step 534: 0.0374084, 0.0132\n",
      "2017-11-10 14:48:56: Loss and accuracy at step 535: 0.0374483, 0.0179\n",
      "2017-11-10 14:48:58: Loss and accuracy at step 536: 0.0374394, 0.0266\n",
      "2017-11-10 14:49:00: Loss and accuracy at step 537: 0.0374349, 0.0147\n",
      "2017-11-10 14:49:02: Loss and accuracy at step 538: 0.0373916, 0.018\n",
      "2017-11-10 14:49:04: Loss and accuracy at step 539: 0.0374453, 0.0176\n",
      "2017-11-10 14:49:06: Loss and accuracy at step 540: 0.0374006, 0.0164\n",
      "2017-11-10 14:49:08: Loss and accuracy at step 541: 0.0374904, 0.0133\n",
      "2017-11-10 14:49:10: Loss and accuracy at step 542: 0.0374123, 0.0181\n",
      "2017-11-10 14:49:13: Loss and accuracy at step 543: 0.037463, 0.0223\n",
      "2017-11-10 14:49:15: Loss and accuracy at step 544: 0.0374047, 0.0115\n",
      "2017-11-10 14:49:17: Loss and accuracy at step 545: 0.037337, 0.0241\n",
      "2017-11-10 14:49:19: Loss and accuracy at step 546: 0.0373798, 0.0201\n",
      "2017-11-10 14:49:21: Loss and accuracy at step 547: 0.0374812, 0.0126\n",
      "2017-11-10 14:49:23: Loss and accuracy at step 548: 0.0373763, 0.0218\n",
      "2017-11-10 14:49:25: Loss and accuracy at step 549: 0.0374126, 0.016\n",
      "2017-11-10 14:49:27: Loss and accuracy at step 550: 0.0373897, 0.0177\n",
      "2017-11-10 14:49:29: Loss and accuracy at step 551: 0.0374175, 0.0131\n",
      "2017-11-10 14:49:31: Loss and accuracy at step 552: 0.037327, 0.0264\n",
      "2017-11-10 14:49:34: Loss and accuracy at step 553: 0.0373875, 0.0125\n",
      "2017-11-10 14:49:36: Loss and accuracy at step 554: 0.0374295, 0.0137\n",
      "2017-11-10 14:49:38: Loss and accuracy at step 555: 0.0374088, 0.0211\n",
      "2017-11-10 14:49:40: Loss and accuracy at step 556: 0.0373789, 0.0124\n",
      "2017-11-10 14:49:42: Loss and accuracy at step 557: 0.0373422, 0.0151\n",
      "2017-11-10 14:49:44: Loss and accuracy at step 558: 0.0373221, 0.019\n",
      "2017-11-10 14:49:46: Loss and accuracy at step 559: 0.037325, 0.0156\n",
      "2017-11-10 14:49:48: Loss and accuracy at step 560: 0.0374148, 0.0189\n",
      "2017-11-10 14:49:50: Loss and accuracy at step 561: 0.0374285, 0.0175\n",
      "2017-11-10 14:49:52: Loss and accuracy at step 562: 0.0373968, 0.0143\n",
      "2017-11-10 14:49:54: Loss and accuracy at step 563: 0.0374991, 0.0221\n",
      "2017-11-10 14:49:56: Loss and accuracy at step 564: 0.0374546, 0.0143\n",
      "2017-11-10 14:49:59: Loss and accuracy at step 565: 0.037295, 0.0147\n",
      "2017-11-10 14:50:01: Loss and accuracy at step 566: 0.0373602, 0.0174\n",
      "2017-11-10 14:50:03: Loss and accuracy at step 567: 0.0374281, 0.0177\n",
      "2017-11-10 14:50:05: Loss and accuracy at step 568: 0.0374311, 0.0148\n",
      "2017-11-10 14:50:07: Loss and accuracy at step 569: 0.0374395, 0.0161\n",
      "2017-11-10 14:50:09: Loss and accuracy at step 570: 0.0373967, 0.0141\n",
      "2017-11-10 14:50:11: Loss and accuracy at step 571: 0.0374361, 0.017\n",
      "2017-11-10 14:50:13: Loss and accuracy at step 572: 0.0374563, 0.016\n",
      "2017-11-10 14:50:15: Loss and accuracy at step 573: 0.0374348, 0.0095\n",
      "2017-11-10 14:50:18: Loss and accuracy at step 574: 0.0374682, 0.0224\n",
      "2017-11-10 14:50:20: Loss and accuracy at step 575: 0.0374838, 0.0166\n",
      "2017-11-10 14:50:22: Loss and accuracy at step 576: 0.0373789, 0.0139\n",
      "2017-11-10 14:50:24: Loss and accuracy at step 577: 0.0374084, 0.0186\n",
      "2017-11-10 14:50:26: Loss and accuracy at step 578: 0.0375106, 0.0135\n",
      "2017-11-10 14:50:28: Loss and accuracy at step 579: 0.0374436, 0.025\n",
      "2017-11-10 14:50:30: Loss and accuracy at step 580: 0.0374135, 0.0176\n",
      "2017-11-10 14:50:32: Loss and accuracy at step 581: 0.0374232, 0.014\n",
      "2017-11-10 14:50:34: Loss and accuracy at step 582: 0.0373968, 0.0208\n",
      "2017-11-10 14:50:36: Loss and accuracy at step 583: 0.0374591, 0.0224\n",
      "2017-11-10 14:50:38: Loss and accuracy at step 584: 0.037426, 0.0099\n",
      "2017-11-10 14:50:40: Loss and accuracy at step 585: 0.0374589, 0.0193\n",
      "2017-11-10 14:50:42: Loss and accuracy at step 586: 0.0373072, 0.0261\n",
      "2017-11-10 14:50:45: Loss and accuracy at step 587: 0.0373348, 0.0103\n",
      "2017-11-10 14:50:47: Loss and accuracy at step 588: 0.0373809, 0.0176\n",
      "2017-11-10 14:50:49: Loss and accuracy at step 589: 0.0373697, 0.0297\n",
      "2017-11-10 14:50:51: Loss and accuracy at step 590: 0.0374309, 0.0128\n",
      "2017-11-10 14:50:53: Loss and accuracy at step 591: 0.0373606, 0.0159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:50:55: Loss and accuracy at step 592: 0.0374113, 0.0304\n",
      "2017-11-10 14:50:57: Loss and accuracy at step 593: 0.0373835, 0.0136\n",
      "2017-11-10 14:50:59: Loss and accuracy at step 594: 0.0373543, 0.0151\n",
      "2017-11-10 14:51:01: Loss and accuracy at step 595: 0.0373458, 0.0271\n",
      "2017-11-10 14:51:03: Loss and accuracy at step 596: 0.0373398, 0.0112\n",
      "2017-11-10 14:51:05: Loss and accuracy at step 597: 0.0374499, 0.011\n",
      "2017-11-10 14:51:07: Loss and accuracy at step 598: 0.0374293, 0.0261\n",
      "2017-11-10 14:51:10: Loss and accuracy at step 599: 0.0374127, 0.0107\n",
      "2017-11-10 14:51:12: Loss and accuracy at step 600: 0.0374517, 0.0131\n",
      "2017-11-10 14:51:14: Loss and accuracy at step 601: 0.0374281, 0.0177\n",
      "2017-11-10 14:51:16: Loss and accuracy at step 602: 0.0373725, 0.0166\n",
      "2017-11-10 14:51:18: Loss and accuracy at step 603: 0.037469, 0.0181\n",
      "2017-11-10 14:51:20: Loss and accuracy at step 604: 0.0374295, 0.0113\n",
      "2017-11-10 14:51:22: Loss and accuracy at step 605: 0.0374887, 0.0233\n",
      "2017-11-10 14:51:24: Loss and accuracy at step 606: 0.037359, 0.0125\n",
      "2017-11-10 14:51:26: Loss and accuracy at step 607: 0.0373499, 0.0116\n",
      "2017-11-10 14:51:28: Loss and accuracy at step 608: 0.0374041, 0.0141\n",
      "2017-11-10 14:51:30: Loss and accuracy at step 609: 0.0373673, 0.0161\n",
      "2017-11-10 14:51:32: Loss and accuracy at step 610: 0.0373353, 0.0149\n",
      "2017-11-10 14:51:35: Loss and accuracy at step 611: 0.0373366, 0.0157\n",
      "2017-11-10 14:51:37: Loss and accuracy at step 612: 0.0374019, 0.0161\n",
      "2017-11-10 14:51:39: Loss and accuracy at step 613: 0.0374006, 0.0177\n",
      "2017-11-10 14:51:41: Loss and accuracy at step 614: 0.037311, 0.0161\n",
      "2017-11-10 14:51:43: Loss and accuracy at step 615: 0.0373269, 0.0158\n",
      "2017-11-10 14:51:45: Loss and accuracy at step 616: 0.0373651, 0.0153\n",
      "2017-11-10 14:51:47: Loss and accuracy at step 617: 0.0372816, 0.0221\n",
      "2017-11-10 14:51:49: Loss and accuracy at step 618: 0.0373128, 0.0109\n",
      "2017-11-10 14:51:51: Loss and accuracy at step 619: 0.037242, 0.0156\n",
      "2017-11-10 14:51:53: Loss and accuracy at step 620: 0.0373236, 0.0196\n",
      "2017-11-10 14:51:55: Loss and accuracy at step 621: 0.0373493, 0.0143\n",
      "2017-11-10 14:51:57: Loss and accuracy at step 622: 0.0373049, 0.0187\n",
      "2017-11-10 14:52:00: Loss and accuracy at step 623: 0.0373773, 0.019\n",
      "2017-11-10 14:52:02: Loss and accuracy at step 624: 0.0372518, 0.01\n",
      "2017-11-10 14:52:04: Loss and accuracy at step 625: 0.0372586, 0.0253\n",
      "2017-11-10 14:52:06: Loss and accuracy at step 626: 0.0373531, 0.0165\n",
      "2017-11-10 14:52:08: Loss and accuracy at step 627: 0.0372923, 0.0149\n",
      "2017-11-10 14:52:10: Loss and accuracy at step 628: 0.0372632, 0.0213\n",
      "2017-11-10 14:52:12: Loss and accuracy at step 629: 0.037285, 0.0203\n",
      "2017-11-10 14:52:14: Loss and accuracy at step 630: 0.0373742, 0.0135\n",
      "2017-11-10 14:52:17: Loss and accuracy at step 631: 0.0373005, 0.0251\n",
      "2017-11-10 14:52:19: Loss and accuracy at step 632: 0.0372539, 0.0135\n",
      "2017-11-10 14:52:21: Loss and accuracy at step 633: 0.0373627, 0.0157\n",
      "2017-11-10 14:52:23: Loss and accuracy at step 634: 0.0373675, 0.0247\n",
      "2017-11-10 14:52:25: Loss and accuracy at step 635: 0.0373587, 0.0125\n",
      "2017-11-10 14:52:27: Loss and accuracy at step 636: 0.0373968, 0.0184\n",
      "2017-11-10 14:52:29: Loss and accuracy at step 637: 0.0373552, 0.0242\n",
      "2017-11-10 14:52:31: Loss and accuracy at step 638: 0.037293, 0.0105\n",
      "2017-11-10 14:52:33: Loss and accuracy at step 639: 0.0373898, 0.0147\n",
      "2017-11-10 14:52:35: Loss and accuracy at step 640: 0.0373941, 0.0247\n",
      "2017-11-10 14:52:37: Loss and accuracy at step 641: 0.0373948, 0.0149\n",
      "2017-11-10 14:52:39: Loss and accuracy at step 642: 0.0373972, 0.0103\n",
      "2017-11-10 14:52:42: Loss and accuracy at step 643: 0.0374296, 0.0323\n",
      "2017-11-10 14:52:44: Loss and accuracy at step 644: 0.037377, 0.0106\n",
      "2017-11-10 14:52:46: Loss and accuracy at step 645: 0.0374275, 0.0131\n",
      "2017-11-10 14:52:48: Loss and accuracy at step 646: 0.0373906, 0.0367\n",
      "2017-11-10 14:52:50: Loss and accuracy at step 647: 0.037375, 0.013\n",
      "2017-11-10 14:52:52: Loss and accuracy at step 648: 0.0373522, 0.0114\n",
      "2017-11-10 14:52:54: Loss and accuracy at step 649: 0.0372978, 0.0691\n",
      "2017-11-10 14:52:56: Loss and accuracy at step 650: 0.0373484, 0.0098\n",
      "2017-11-10 14:52:58: Loss and accuracy at step 651: 0.0374115, 0.012\n",
      "2017-11-10 14:53:00: Loss and accuracy at step 652: 0.0372653, 0.0593\n",
      "2017-11-10 14:53:02: Loss and accuracy at step 653: 0.0372902, 0.0077\n",
      "2017-11-10 14:53:04: Loss and accuracy at step 654: 0.0372585, 0.0175\n",
      "2017-11-10 14:53:07: Loss and accuracy at step 655: 0.0372996, 0.042\n",
      "2017-11-10 14:53:09: Loss and accuracy at step 656: 0.0372285, 0.0108\n",
      "2017-11-10 14:53:11: Loss and accuracy at step 657: 0.0373664, 0.018\n",
      "2017-11-10 14:53:13: Loss and accuracy at step 658: 0.0371895, 0.0214\n",
      "2017-11-10 14:53:15: Loss and accuracy at step 659: 0.0374047, 0.0196\n",
      "2017-11-10 14:53:17: Loss and accuracy at step 660: 0.0373359, 0.0117\n",
      "2017-11-10 14:53:19: Loss and accuracy at step 661: 0.0372934, 0.0154\n",
      "2017-11-10 14:53:21: Loss and accuracy at step 662: 0.0372561, 0.0197\n",
      "2017-11-10 14:53:23: Loss and accuracy at step 663: 0.0372752, 0.016\n",
      "2017-11-10 14:53:25: Loss and accuracy at step 664: 0.0373331, 0.0135\n",
      "2017-11-10 14:53:27: Loss and accuracy at step 665: 0.0372785, 0.016\n",
      "2017-11-10 14:53:30: Loss and accuracy at step 666: 0.0372017, 0.0195\n",
      "2017-11-10 14:53:32: Loss and accuracy at step 667: 0.0373188, 0.0166\n",
      "2017-11-10 14:53:34: Loss and accuracy at step 668: 0.0373821, 0.0118\n",
      "2017-11-10 14:53:36: Loss and accuracy at step 669: 0.0373378, 0.0235\n",
      "2017-11-10 14:53:38: Loss and accuracy at step 670: 0.0373586, 0.0181\n",
      "2017-11-10 14:53:40: Loss and accuracy at step 671: 0.0373212, 0.0136\n",
      "2017-11-10 14:53:42: Loss and accuracy at step 672: 0.0373537, 0.0132\n",
      "2017-11-10 14:53:44: Loss and accuracy at step 673: 0.037439, 0.032\n",
      "2017-11-10 14:53:46: Loss and accuracy at step 674: 0.0373058, 0.0121\n",
      "2017-11-10 14:53:48: Loss and accuracy at step 675: 0.0372715, 0.0163\n",
      "2017-11-10 14:53:50: Loss and accuracy at step 676: 0.0373117, 0.0135\n",
      "2017-11-10 14:53:53: Loss and accuracy at step 677: 0.037368, 0.0199\n",
      "2017-11-10 14:53:55: Loss and accuracy at step 678: 0.037308, 0.0136\n",
      "2017-11-10 14:53:57: Loss and accuracy at step 679: 0.0372145, 0.0114\n",
      "2017-11-10 14:53:59: Loss and accuracy at step 680: 0.0372625, 0.0165\n",
      "2017-11-10 14:54:01: Loss and accuracy at step 681: 0.0372031, 0.0203\n",
      "2017-11-10 14:54:03: Loss and accuracy at step 682: 0.0372651, 0.0157\n",
      "2017-11-10 14:54:05: Loss and accuracy at step 683: 0.0371097, 0.0101\n",
      "2017-11-10 14:54:07: Loss and accuracy at step 684: 0.0373007, 0.0211\n",
      "2017-11-10 14:54:09: Loss and accuracy at step 685: 0.0372392, 0.0144\n",
      "2017-11-10 14:54:11: Loss and accuracy at step 686: 0.0372882, 0.0126\n",
      "2017-11-10 14:54:14: Loss and accuracy at step 687: 0.0373122, 0.0201\n",
      "2017-11-10 14:54:16: Loss and accuracy at step 688: 0.037343, 0.0116\n",
      "2017-11-10 14:54:18: Loss and accuracy at step 689: 0.037258, 0.0186\n",
      "2017-11-10 14:54:20: Loss and accuracy at step 690: 0.0372985, 0.0129\n",
      "2017-11-10 14:54:22: Loss and accuracy at step 691: 0.0372149, 0.0143\n",
      "2017-11-10 14:54:24: Loss and accuracy at step 692: 0.0372342, 0.0205\n",
      "2017-11-10 14:54:26: Loss and accuracy at step 693: 0.0372429, 0.0106\n",
      "2017-11-10 14:54:28: Loss and accuracy at step 694: 0.0373001, 0.0163\n",
      "2017-11-10 14:54:30: Loss and accuracy at step 695: 0.0372596, 0.0128\n",
      "2017-11-10 14:54:32: Loss and accuracy at step 696: 0.0372843, 0.0184\n",
      "2017-11-10 14:54:35: Loss and accuracy at step 697: 0.0372652, 0.0143\n",
      "2017-11-10 14:54:37: Loss and accuracy at step 698: 0.03721, 0.0098\n",
      "2017-11-10 14:54:39: Loss and accuracy at step 699: 0.0371716, 0.0197\n",
      "2017-11-10 14:54:41: Loss and accuracy at step 700: 0.0372644, 0.011\n",
      "2017-11-10 14:54:43: Loss and accuracy at step 701: 0.0373337, 0.0152\n",
      "2017-11-10 14:54:45: Loss and accuracy at step 702: 0.0372927, 0.0178\n",
      "2017-11-10 14:54:47: Loss and accuracy at step 703: 0.0373679, 0.0115\n",
      "2017-11-10 14:54:49: Loss and accuracy at step 704: 0.0373531, 0.0252\n",
      "2017-11-10 14:54:51: Loss and accuracy at step 705: 0.0372614, 0.0136\n",
      "2017-11-10 14:54:53: Loss and accuracy at step 706: 0.0373533, 0.0104\n",
      "2017-11-10 14:54:55: Loss and accuracy at step 707: 0.0372274, 0.0248\n",
      "2017-11-10 14:54:57: Loss and accuracy at step 708: 0.037352, 0.0132\n",
      "2017-11-10 14:55:00: Loss and accuracy at step 709: 0.0373452, 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:55:02: Loss and accuracy at step 710: 0.0373571, 0.011\n",
      "2017-11-10 14:55:04: Loss and accuracy at step 711: 0.0373119, 0.0195\n",
      "2017-11-10 14:55:06: Loss and accuracy at step 712: 0.0372738, 0.0114\n",
      "2017-11-10 14:55:08: Loss and accuracy at step 713: 0.037332, 0.0194\n",
      "2017-11-10 14:55:10: Loss and accuracy at step 714: 0.0373354, 0.0167\n",
      "2017-11-10 14:55:12: Loss and accuracy at step 715: 0.0372846, 0.0123\n",
      "2017-11-10 14:55:14: Loss and accuracy at step 716: 0.0373284, 0.016\n",
      "2017-11-10 14:55:16: Loss and accuracy at step 717: 0.0372619, 0.023\n",
      "2017-11-10 14:55:18: Loss and accuracy at step 718: 0.0371963, 0.0156\n",
      "2017-11-10 14:55:20: Loss and accuracy at step 719: 0.0372923, 0.0113\n",
      "2017-11-10 14:55:23: Loss and accuracy at step 720: 0.0372157, 0.0203\n",
      "2017-11-10 14:55:25: Loss and accuracy at step 721: 0.0372529, 0.0164\n",
      "2017-11-10 14:55:27: Loss and accuracy at step 722: 0.0371676, 0.0117\n",
      "2017-11-10 14:55:29: Loss and accuracy at step 723: 0.0372285, 0.013\n",
      "2017-11-10 14:55:31: Loss and accuracy at step 724: 0.0372907, 0.0189\n",
      "2017-11-10 14:55:33: Loss and accuracy at step 725: 0.0371652, 0.0162\n",
      "2017-11-10 14:55:35: Loss and accuracy at step 726: 0.037285, 0.0135\n",
      "2017-11-10 14:55:37: Loss and accuracy at step 727: 0.0372829, 0.0154\n",
      "2017-11-10 14:55:39: Loss and accuracy at step 728: 0.0372834, 0.0133\n",
      "2017-11-10 14:55:41: Loss and accuracy at step 729: 0.0373289, 0.0159\n",
      "2017-11-10 14:55:43: Loss and accuracy at step 730: 0.0373933, 0.0138\n",
      "2017-11-10 14:55:45: Loss and accuracy at step 731: 0.0373224, 0.0111\n",
      "2017-11-10 14:55:48: Loss and accuracy at step 732: 0.0372948, 0.017\n",
      "2017-11-10 14:55:50: Loss and accuracy at step 733: 0.0372881, 0.0132\n",
      "2017-11-10 14:55:52: Loss and accuracy at step 734: 0.0372343, 0.0129\n",
      "2017-11-10 14:55:54: Loss and accuracy at step 735: 0.0372644, 0.013\n",
      "2017-11-10 14:55:56: Loss and accuracy at step 736: 0.0373491, 0.0157\n",
      "2017-11-10 14:55:58: Loss and accuracy at step 737: 0.0372978, 0.0153\n",
      "2017-11-10 14:56:00: Loss and accuracy at step 738: 0.0373397, 0.0163\n",
      "2017-11-10 14:56:02: Loss and accuracy at step 739: 0.0371968, 0.0132\n",
      "2017-11-10 14:56:04: Loss and accuracy at step 740: 0.0372726, 0.0201\n",
      "2017-11-10 14:56:06: Loss and accuracy at step 741: 0.0372422, 0.0112\n",
      "2017-11-10 14:56:08: Loss and accuracy at step 742: 0.0371921, 0.0173\n",
      "2017-11-10 14:56:11: Loss and accuracy at step 743: 0.0372719, 0.0149\n",
      "2017-11-10 14:56:13: Loss and accuracy at step 744: 0.0372818, 0.0124\n",
      "2017-11-10 14:56:15: Loss and accuracy at step 745: 0.0372687, 0.0182\n",
      "2017-11-10 14:56:17: Loss and accuracy at step 746: 0.0372251, 0.0128\n",
      "2017-11-10 14:56:19: Loss and accuracy at step 747: 0.0372794, 0.0163\n",
      "2017-11-10 14:56:21: Loss and accuracy at step 748: 0.0371964, 0.0135\n",
      "2017-11-10 14:56:23: Loss and accuracy at step 749: 0.0372763, 0.018\n",
      "2017-11-10 14:56:25: Loss and accuracy at step 750: 0.0372913, 0.0119\n",
      "2017-11-10 14:56:27: Loss and accuracy at step 751: 0.0372766, 0.012\n",
      "2017-11-10 14:56:30: Loss and accuracy at step 752: 0.0372374, 0.0206\n",
      "2017-11-10 14:56:32: Loss and accuracy at step 753: 0.0373153, 0.0172\n",
      "2017-11-10 14:56:34: Loss and accuracy at step 754: 0.0372942, 0.0107\n",
      "2017-11-10 14:56:36: Loss and accuracy at step 755: 0.0372422, 0.0212\n",
      "2017-11-10 14:56:38: Loss and accuracy at step 756: 0.0371638, 0.0135\n",
      "2017-11-10 14:56:40: Loss and accuracy at step 757: 0.03733, 0.0098\n",
      "2017-11-10 14:56:42: Loss and accuracy at step 758: 0.0372582, 0.0222\n",
      "2017-11-10 14:56:44: Loss and accuracy at step 759: 0.0373098, 0.0172\n",
      "2017-11-10 14:56:46: Loss and accuracy at step 760: 0.0372556, 0.0122\n",
      "2017-11-10 14:56:48: Loss and accuracy at step 761: 0.0371744, 0.0119\n",
      "2017-11-10 14:56:50: Loss and accuracy at step 762: 0.0372481, 0.017\n",
      "2017-11-10 14:56:52: Loss and accuracy at step 763: 0.0373006, 0.0135\n",
      "2017-11-10 14:56:54: Loss and accuracy at step 764: 0.0372318, 0.0154\n",
      "2017-11-10 14:56:57: Loss and accuracy at step 765: 0.0372174, 0.0216\n",
      "2017-11-10 14:56:59: Loss and accuracy at step 766: 0.0372983, 0.0131\n",
      "2017-11-10 14:57:01: Loss and accuracy at step 767: 0.0372561, 0.0152\n",
      "2017-11-10 14:57:03: Loss and accuracy at step 768: 0.0372566, 0.0181\n",
      "2017-11-10 14:57:05: Loss and accuracy at step 769: 0.0372042, 0.013\n",
      "2017-11-10 14:57:07: Loss and accuracy at step 770: 0.0371809, 0.0124\n",
      "2017-11-10 14:57:09: Loss and accuracy at step 771: 0.037306, 0.0192\n",
      "2017-11-10 14:57:11: Loss and accuracy at step 772: 0.0373044, 0.0212\n",
      "2017-11-10 14:57:13: Loss and accuracy at step 773: 0.0373315, 0.0104\n",
      "2017-11-10 14:57:15: Loss and accuracy at step 774: 0.0373389, 0.0133\n",
      "2017-11-10 14:57:17: Loss and accuracy at step 775: 0.0372202, 0.0217\n",
      "2017-11-10 14:57:20: Loss and accuracy at step 776: 0.0372871, 0.0126\n",
      "2017-11-10 14:57:22: Loss and accuracy at step 777: 0.0372917, 0.0124\n",
      "2017-11-10 14:57:24: Loss and accuracy at step 778: 0.0372367, 0.0159\n",
      "2017-11-10 14:57:26: Loss and accuracy at step 779: 0.0372333, 0.0186\n",
      "2017-11-10 14:57:28: Loss and accuracy at step 780: 0.0373252, 0.0099\n",
      "2017-11-10 14:57:30: Loss and accuracy at step 781: 0.0373134, 0.0138\n",
      "2017-11-10 14:57:32: Loss and accuracy at step 782: 0.0372545, 0.018\n",
      "2017-11-10 14:57:34: Loss and accuracy at step 783: 0.0371773, 0.0139\n",
      "2017-11-10 14:57:36: Loss and accuracy at step 784: 0.037241, 0.0149\n",
      "2017-11-10 14:57:38: Loss and accuracy at step 785: 0.0372909, 0.011\n",
      "2017-11-10 14:57:40: Loss and accuracy at step 786: 0.037217, 0.0182\n",
      "2017-11-10 14:57:42: Loss and accuracy at step 787: 0.037311, 0.0124\n",
      "2017-11-10 14:57:45: Loss and accuracy at step 788: 0.0372148, 0.0131\n",
      "2017-11-10 14:57:47: Loss and accuracy at step 789: 0.037248, 0.0171\n",
      "2017-11-10 14:57:49: Loss and accuracy at step 790: 0.0373151, 0.0106\n",
      "2017-11-10 14:57:51: Loss and accuracy at step 791: 0.0373012, 0.0181\n",
      "2017-11-10 14:57:53: Loss and accuracy at step 792: 0.037263, 0.0102\n",
      "2017-11-10 14:57:55: Loss and accuracy at step 793: 0.0372971, 0.0138\n",
      "2017-11-10 14:57:57: Loss and accuracy at step 794: 0.0372347, 0.02\n",
      "2017-11-10 14:57:59: Loss and accuracy at step 795: 0.0372942, 0.0103\n",
      "2017-11-10 14:58:01: Loss and accuracy at step 796: 0.0372353, 0.012\n",
      "2017-11-10 14:58:03: Loss and accuracy at step 797: 0.0371529, 0.0131\n",
      "2017-11-10 14:58:05: Loss and accuracy at step 798: 0.0373195, 0.0163\n",
      "2017-11-10 14:58:07: Loss and accuracy at step 799: 0.0372878, 0.0157\n",
      "2017-11-10 14:58:10: Loss and accuracy at step 800: 0.0372103, 0.0126\n",
      "2017-11-10 14:58:12: Loss and accuracy at step 801: 0.0372775, 0.0174\n",
      "2017-11-10 14:58:14: Loss and accuracy at step 802: 0.037279, 0.0159\n",
      "2017-11-10 14:58:16: Loss and accuracy at step 803: 0.0372464, 0.0107\n",
      "2017-11-10 14:58:18: Loss and accuracy at step 804: 0.0372288, 0.0168\n",
      "2017-11-10 14:58:20: Loss and accuracy at step 805: 0.0371369, 0.0107\n",
      "2017-11-10 14:58:22: Loss and accuracy at step 806: 0.0372778, 0.016\n",
      "2017-11-10 14:58:24: Loss and accuracy at step 807: 0.0371915, 0.0134\n",
      "2017-11-10 14:58:26: Loss and accuracy at step 808: 0.0372557, 0.0143\n",
      "2017-11-10 14:58:29: Loss and accuracy at step 809: 0.0371583, 0.0156\n",
      "2017-11-10 14:58:31: Loss and accuracy at step 810: 0.0372711, 0.0143\n",
      "2017-11-10 14:58:33: Loss and accuracy at step 811: 0.037207, 0.0139\n",
      "2017-11-10 14:58:35: Loss and accuracy at step 812: 0.0372677, 0.0126\n",
      "2017-11-10 14:58:37: Loss and accuracy at step 813: 0.0372498, 0.0217\n",
      "2017-11-10 14:58:39: Loss and accuracy at step 814: 0.0372364, 0.0076\n",
      "2017-11-10 14:58:41: Loss and accuracy at step 815: 0.0371335, 0.0167\n",
      "2017-11-10 14:58:43: Loss and accuracy at step 816: 0.037161, 0.0205\n",
      "2017-11-10 14:58:45: Loss and accuracy at step 817: 0.0372509, 0.0111\n",
      "2017-11-10 14:58:47: Loss and accuracy at step 818: 0.0371878, 0.0138\n",
      "2017-11-10 14:58:49: Loss and accuracy at step 819: 0.0371621, 0.0126\n",
      "2017-11-10 14:58:52: Loss and accuracy at step 820: 0.0371908, 0.0195\n",
      "2017-11-10 14:58:54: Loss and accuracy at step 821: 0.0372593, 0.0099\n",
      "2017-11-10 14:58:56: Loss and accuracy at step 822: 0.0372337, 0.0103\n",
      "2017-11-10 14:58:58: Loss and accuracy at step 823: 0.0372234, 0.017\n",
      "2017-11-10 14:59:00: Loss and accuracy at step 824: 0.0372036, 0.0142\n",
      "2017-11-10 14:59:02: Loss and accuracy at step 825: 0.0371732, 0.015\n",
      "2017-11-10 14:59:04: Loss and accuracy at step 826: 0.0370732, 0.0144\n",
      "2017-11-10 14:59:06: Loss and accuracy at step 827: 0.0371996, 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 14:59:08: Loss and accuracy at step 828: 0.0372017, 0.0133\n",
      "2017-11-10 14:59:10: Loss and accuracy at step 829: 0.0371499, 0.0134\n",
      "2017-11-10 14:59:12: Loss and accuracy at step 830: 0.0372395, 0.0153\n",
      "2017-11-10 14:59:15: Loss and accuracy at step 831: 0.037147, 0.0157\n",
      "2017-11-10 14:59:17: Loss and accuracy at step 832: 0.037187, 0.0114\n",
      "2017-11-10 14:59:19: Loss and accuracy at step 833: 0.0373344, 0.0185\n",
      "2017-11-10 14:59:21: Loss and accuracy at step 834: 0.0372273, 0.011\n",
      "2017-11-10 14:59:23: Loss and accuracy at step 835: 0.0372426, 0.0122\n",
      "2017-11-10 14:59:25: Loss and accuracy at step 836: 0.0371814, 0.0198\n",
      "2017-11-10 14:59:27: Loss and accuracy at step 837: 0.037182, 0.0143\n",
      "2017-11-10 14:59:29: Loss and accuracy at step 838: 0.0371471, 0.0094\n",
      "2017-11-10 14:59:31: Loss and accuracy at step 839: 0.0371121, 0.026\n",
      "2017-11-10 14:59:33: Loss and accuracy at step 840: 0.0372681, 0.0113\n",
      "2017-11-10 14:59:35: Loss and accuracy at step 841: 0.0372461, 0.0184\n",
      "2017-11-10 14:59:37: Loss and accuracy at step 842: 0.0372848, 0.0126\n",
      "2017-11-10 14:59:40: Loss and accuracy at step 843: 0.0372499, 0.0135\n",
      "2017-11-10 14:59:42: Loss and accuracy at step 844: 0.0372706, 0.018\n",
      "2017-11-10 14:59:44: Loss and accuracy at step 845: 0.0371555, 0.0119\n",
      "2017-11-10 14:59:46: Loss and accuracy at step 846: 0.037243, 0.0194\n",
      "2017-11-10 14:59:48: Loss and accuracy at step 847: 0.0372497, 0.0153\n",
      "2017-11-10 14:59:50: Loss and accuracy at step 848: 0.0372521, 0.0132\n",
      "2017-11-10 14:59:52: Loss and accuracy at step 849: 0.0372302, 0.0195\n",
      "2017-11-10 14:59:54: Loss and accuracy at step 850: 0.0372212, 0.0129\n",
      "2017-11-10 14:59:56: Loss and accuracy at step 851: 0.0372832, 0.0196\n",
      "2017-11-10 14:59:58: Loss and accuracy at step 852: 0.0371994, 0.0136\n",
      "2017-11-10 15:00:00: Loss and accuracy at step 853: 0.0372509, 0.0108\n",
      "2017-11-10 15:00:03: Loss and accuracy at step 854: 0.0371617, 0.0204\n",
      "2017-11-10 15:00:05: Loss and accuracy at step 855: 0.0372694, 0.0074\n",
      "2017-11-10 15:00:07: Loss and accuracy at step 856: 0.0372298, 0.0138\n",
      "2017-11-10 15:00:09: Loss and accuracy at step 857: 0.037245, 0.0201\n",
      "2017-11-10 15:00:11: Loss and accuracy at step 858: 0.0371493, 0.0109\n",
      "2017-11-10 15:00:13: Loss and accuracy at step 859: 0.0373332, 0.0143\n",
      "2017-11-10 15:00:15: Loss and accuracy at step 860: 0.0371795, 0.0125\n",
      "2017-11-10 15:00:17: Loss and accuracy at step 861: 0.0371921, 0.0231\n",
      "2017-11-10 15:00:19: Loss and accuracy at step 862: 0.0372075, 0.0092\n",
      "2017-11-10 15:00:21: Loss and accuracy at step 863: 0.0371641, 0.012\n",
      "2017-11-10 15:00:24: Loss and accuracy at step 864: 0.0371877, 0.0306\n",
      "2017-11-10 15:00:26: Loss and accuracy at step 865: 0.0371703, 0.0092\n",
      "2017-11-10 15:00:28: Loss and accuracy at step 866: 0.0371852, 0.0148\n",
      "2017-11-10 15:00:30: Loss and accuracy at step 867: 0.0372039, 0.0256\n",
      "2017-11-10 15:00:32: Loss and accuracy at step 868: 0.0372173, 0.0108\n",
      "2017-11-10 15:00:34: Loss and accuracy at step 869: 0.03721, 0.0173\n",
      "2017-11-10 15:00:36: Loss and accuracy at step 870: 0.0372055, 0.0104\n",
      "2017-11-10 15:00:38: Loss and accuracy at step 871: 0.0372484, 0.0239\n",
      "2017-11-10 15:00:40: Loss and accuracy at step 872: 0.0371987, 0.012\n",
      "2017-11-10 15:00:42: Loss and accuracy at step 873: 0.0372564, 0.0102\n",
      "2017-11-10 15:00:45: Loss and accuracy at step 874: 0.0372551, 0.0302\n",
      "2017-11-10 15:00:47: Loss and accuracy at step 875: 0.03737, 0.0087\n",
      "2017-11-10 15:00:49: Loss and accuracy at step 876: 0.0372912, 0.0121\n",
      "2017-11-10 15:00:51: Loss and accuracy at step 877: 0.0372294, 0.0287\n",
      "2017-11-10 15:00:53: Loss and accuracy at step 878: 0.0372514, 0.0096\n",
      "2017-11-10 15:00:55: Loss and accuracy at step 879: 0.0372529, 0.0109\n",
      "2017-11-10 15:00:57: Loss and accuracy at step 880: 0.0371794, 0.0313\n",
      "2017-11-10 15:00:59: Loss and accuracy at step 881: 0.037148, 0.0089\n",
      "2017-11-10 15:01:01: Loss and accuracy at step 882: 0.0372036, 0.012\n",
      "2017-11-10 15:01:03: Loss and accuracy at step 883: 0.0371704, 0.019\n",
      "2017-11-10 15:01:05: Loss and accuracy at step 884: 0.0371545, 0.0141\n",
      "2017-11-10 15:01:07: Loss and accuracy at step 885: 0.0371321, 0.015\n",
      "2017-11-10 15:01:10: Loss and accuracy at step 886: 0.0370897, 0.0142\n",
      "2017-11-10 15:01:12: Loss and accuracy at step 887: 0.0372168, 0.0205\n",
      "2017-11-10 15:01:14: Loss and accuracy at step 888: 0.0371476, 0.0102\n",
      "2017-11-10 15:01:16: Loss and accuracy at step 889: 0.0371568, 0.0112\n",
      "2017-11-10 15:01:18: Loss and accuracy at step 890: 0.0372037, 0.0159\n",
      "2017-11-10 15:01:20: Loss and accuracy at step 891: 0.0371242, 0.0168\n",
      "2017-11-10 15:01:22: Loss and accuracy at step 892: 0.0372899, 0.0136\n",
      "2017-11-10 15:01:24: Loss and accuracy at step 893: 0.0372734, 0.0081\n",
      "2017-11-10 15:01:26: Loss and accuracy at step 894: 0.0372132, 0.0252\n",
      "2017-11-10 15:01:28: Loss and accuracy at step 895: 0.0372228, 0.0179\n",
      "2017-11-10 15:01:30: Loss and accuracy at step 896: 0.0372216, 0.0083\n",
      "2017-11-10 15:01:33: Loss and accuracy at step 897: 0.0372126, 0.0271\n",
      "2017-11-10 15:01:35: Loss and accuracy at step 898: 0.0372612, 0.0113\n",
      "2017-11-10 15:01:37: Loss and accuracy at step 899: 0.0371855, 0.0139\n",
      "2017-11-10 15:01:39: Loss and accuracy at step 900: 0.0372683, 0.0169\n",
      "2017-11-10 15:01:41: Loss and accuracy at step 901: 0.0372073, 0.0125\n",
      "2017-11-10 15:01:43: Loss and accuracy at step 902: 0.0372, 0.0197\n",
      "2017-11-10 15:01:45: Loss and accuracy at step 903: 0.0372034, 0.0112\n",
      "2017-11-10 15:01:47: Loss and accuracy at step 904: 0.0372626, 0.0144\n",
      "2017-11-10 15:01:49: Loss and accuracy at step 905: 0.0372553, 0.0142\n",
      "2017-11-10 15:01:51: Loss and accuracy at step 906: 0.0371892, 0.0092\n",
      "2017-11-10 15:01:53: Loss and accuracy at step 907: 0.0372386, 0.0209\n",
      "2017-11-10 15:01:55: Loss and accuracy at step 908: 0.0371624, 0.0103\n",
      "2017-11-10 15:01:58: Loss and accuracy at step 909: 0.0371166, 0.0091\n",
      "2017-11-10 15:02:00: Loss and accuracy at step 910: 0.037105, 0.0198\n",
      "2017-11-10 15:02:02: Loss and accuracy at step 911: 0.0371157, 0.0154\n",
      "2017-11-10 15:02:04: Loss and accuracy at step 912: 0.0372504, 0.0136\n",
      "2017-11-10 15:02:06: Loss and accuracy at step 913: 0.0371201, 0.0113\n",
      "2017-11-10 15:02:08: Loss and accuracy at step 914: 0.0371989, 0.0153\n",
      "2017-11-10 15:02:10: Loss and accuracy at step 915: 0.0371433, 0.0149\n",
      "2017-11-10 15:02:12: Loss and accuracy at step 916: 0.0371078, 0.0098\n",
      "2017-11-10 15:02:15: Loss and accuracy at step 917: 0.0370967, 0.0198\n",
      "2017-11-10 15:02:17: Loss and accuracy at step 918: 0.037086, 0.0173\n",
      "2017-11-10 15:02:19: Loss and accuracy at step 919: 0.0371032, 0.0077\n",
      "2017-11-10 15:02:21: Loss and accuracy at step 920: 0.0371789, 0.0198\n",
      "2017-11-10 15:02:23: Loss and accuracy at step 921: 0.0372118, 0.0143\n",
      "2017-11-10 15:02:25: Loss and accuracy at step 922: 0.037186, 0.0083\n",
      "2017-11-10 15:02:27: Loss and accuracy at step 923: 0.0372666, 0.0166\n",
      "2017-11-10 15:02:29: Loss and accuracy at step 924: 0.0372292, 0.0139\n",
      "2017-11-10 15:02:31: Loss and accuracy at step 925: 0.0372402, 0.013\n",
      "2017-11-10 15:02:33: Loss and accuracy at step 926: 0.0371904, 0.0123\n",
      "2017-11-10 15:02:35: Loss and accuracy at step 927: 0.0372319, 0.0147\n",
      "2017-11-10 15:02:37: Loss and accuracy at step 928: 0.0371613, 0.0139\n",
      "2017-11-10 15:02:40: Loss and accuracy at step 929: 0.0371956, 0.0103\n",
      "2017-11-10 15:02:42: Loss and accuracy at step 930: 0.0371518, 0.0143\n",
      "2017-11-10 15:02:44: Loss and accuracy at step 931: 0.0371368, 0.0177\n",
      "2017-11-10 15:02:46: Loss and accuracy at step 932: 0.0371152, 0.0082\n",
      "2017-11-10 15:02:48: Loss and accuracy at step 933: 0.0371349, 0.0192\n",
      "2017-11-10 15:02:50: Loss and accuracy at step 934: 0.0371267, 0.0151\n",
      "2017-11-10 15:02:52: Loss and accuracy at step 935: 0.037207, 0.0084\n",
      "2017-11-10 15:02:54: Loss and accuracy at step 936: 0.0372653, 0.0129\n",
      "2017-11-10 15:02:56: Loss and accuracy at step 937: 0.0372333, 0.0127\n",
      "2017-11-10 15:02:58: Loss and accuracy at step 938: 0.0372168, 0.0138\n",
      "2017-11-10 15:03:00: Loss and accuracy at step 939: 0.0372296, 0.013\n",
      "2017-11-10 15:03:03: Loss and accuracy at step 940: 0.0371755, 0.0104\n",
      "2017-11-10 15:03:05: Loss and accuracy at step 941: 0.0371187, 0.0173\n",
      "2017-11-10 15:03:07: Loss and accuracy at step 942: 0.0371361, 0.0099\n",
      "2017-11-10 15:03:09: Loss and accuracy at step 943: 0.0371504, 0.0124\n",
      "2017-11-10 15:03:11: Loss and accuracy at step 944: 0.0371215, 0.013\n",
      "2017-11-10 15:03:13: Loss and accuracy at step 945: 0.0371616, 0.0141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:03:15: Loss and accuracy at step 946: 0.0371412, 0.0105\n",
      "2017-11-10 15:03:17: Loss and accuracy at step 947: 0.0372052, 0.0121\n",
      "2017-11-10 15:03:19: Loss and accuracy at step 948: 0.0371174, 0.0171\n",
      "2017-11-10 15:03:21: Loss and accuracy at step 949: 0.0371721, 0.0114\n",
      "2017-11-10 15:03:23: Loss and accuracy at step 950: 0.0371753, 0.0087\n",
      "2017-11-10 15:03:25: Loss and accuracy at step 951: 0.0372259, 0.0206\n",
      "2017-11-10 15:03:27: Loss and accuracy at step 952: 0.0372764, 0.0134\n",
      "2017-11-10 15:03:30: Loss and accuracy at step 953: 0.0371912, 0.0087\n",
      "2017-11-10 15:03:32: Loss and accuracy at step 954: 0.0372009, 0.0168\n",
      "2017-11-10 15:03:34: Loss and accuracy at step 955: 0.0372219, 0.0121\n",
      "2017-11-10 15:03:36: Loss and accuracy at step 956: 0.0371945, 0.0127\n",
      "2017-11-10 15:03:38: Loss and accuracy at step 957: 0.0371909, 0.0136\n",
      "2017-11-10 15:03:40: Loss and accuracy at step 958: 0.037168, 0.012\n",
      "2017-11-10 15:03:42: Loss and accuracy at step 959: 0.0371183, 0.0131\n",
      "2017-11-10 15:03:44: Loss and accuracy at step 960: 0.0371439, 0.0137\n",
      "2017-11-10 15:03:46: Loss and accuracy at step 961: 0.0371914, 0.0094\n",
      "2017-11-10 15:03:48: Loss and accuracy at step 962: 0.0370796, 0.0171\n",
      "2017-11-10 15:03:50: Loss and accuracy at step 963: 0.0371737, 0.0153\n",
      "2017-11-10 15:03:53: Loss and accuracy at step 964: 0.0371711, 0.0124\n",
      "2017-11-10 15:03:55: Loss and accuracy at step 965: 0.0373176, 0.0102\n",
      "2017-11-10 15:03:57: Loss and accuracy at step 966: 0.0371985, 0.0118\n",
      "2017-11-10 15:03:59: Loss and accuracy at step 967: 0.0372929, 0.0123\n",
      "2017-11-10 15:04:01: Loss and accuracy at step 968: 0.0372325, 0.009\n",
      "2017-11-10 15:04:03: Loss and accuracy at step 969: 0.0370948, 0.015\n",
      "2017-11-10 15:04:05: Loss and accuracy at step 970: 0.0371753, 0.013\n",
      "2017-11-10 15:04:07: Loss and accuracy at step 971: 0.0371324, 0.0097\n",
      "2017-11-10 15:04:09: Loss and accuracy at step 972: 0.0372529, 0.0154\n",
      "2017-11-10 15:04:11: Loss and accuracy at step 973: 0.0371903, 0.0115\n",
      "2017-11-10 15:04:14: Loss and accuracy at step 974: 0.0371041, 0.012\n",
      "2017-11-10 15:04:16: Loss and accuracy at step 975: 0.0371124, 0.0138\n",
      "2017-11-10 15:04:18: Loss and accuracy at step 976: 0.0371338, 0.0152\n",
      "2017-11-10 15:04:20: Loss and accuracy at step 977: 0.0371155, 0.0146\n",
      "2017-11-10 15:04:22: Loss and accuracy at step 978: 0.0371525, 0.0091\n",
      "2017-11-10 15:04:24: Loss and accuracy at step 979: 0.0371215, 0.0123\n",
      "2017-11-10 15:04:26: Loss and accuracy at step 980: 0.0371864, 0.0252\n",
      "2017-11-10 15:04:28: Loss and accuracy at step 981: 0.0371597, 0.0079\n",
      "2017-11-10 15:04:30: Loss and accuracy at step 982: 0.0371588, 0.0118\n",
      "2017-11-10 15:04:32: Loss and accuracy at step 983: 0.0372663, 0.0218\n",
      "2017-11-10 15:04:34: Loss and accuracy at step 984: 0.0371277, 0.0109\n",
      "2017-11-10 15:04:37: Loss and accuracy at step 985: 0.0371565, 0.0133\n",
      "2017-11-10 15:04:39: Loss and accuracy at step 986: 0.0372104, 0.0163\n",
      "2017-11-10 15:04:41: Loss and accuracy at step 987: 0.0371508, 0.0133\n",
      "2017-11-10 15:04:43: Loss and accuracy at step 988: 0.0372221, 0.0126\n",
      "2017-11-10 15:04:45: Loss and accuracy at step 989: 0.0371372, 0.0133\n",
      "2017-11-10 15:04:47: Loss and accuracy at step 990: 0.0371522, 0.0132\n",
      "2017-11-10 15:04:49: Loss and accuracy at step 991: 0.0370822, 0.0161\n",
      "2017-11-10 15:04:51: Loss and accuracy at step 992: 0.0372401, 0.013\n",
      "2017-11-10 15:04:53: Loss and accuracy at step 993: 0.0371972, 0.0158\n",
      "2017-11-10 15:04:55: Loss and accuracy at step 994: 0.0372005, 0.0109\n",
      "2017-11-10 15:04:57: Loss and accuracy at step 995: 0.0371894, 0.0141\n",
      "2017-11-10 15:04:59: Loss and accuracy at step 996: 0.0370322, 0.0153\n",
      "2017-11-10 15:05:02: Loss and accuracy at step 997: 0.03705, 0.0107\n",
      "2017-11-10 15:05:04: Loss and accuracy at step 998: 0.0371621, 0.0129\n",
      "2017-11-10 15:05:06: Loss and accuracy at step 999: 0.0370798, 0.0192\n",
      "2017-11-10 15:05:08: Loss and accuracy at step 1000: 0.0371588, 0.0151\n",
      "2017-11-10 15:05:10: Loss and accuracy at step 1001: 0.037105, 0.0091\n",
      "2017-11-10 15:05:12: Loss and accuracy at step 1002: 0.0371273, 0.0161\n",
      "2017-11-10 15:05:14: Loss and accuracy at step 1003: 0.0371954, 0.0113\n",
      "2017-11-10 15:05:16: Loss and accuracy at step 1004: 0.0371462, 0.013\n",
      "2017-11-10 15:05:18: Loss and accuracy at step 1005: 0.0371572, 0.011\n",
      "2017-11-10 15:05:20: Loss and accuracy at step 1006: 0.0371149, 0.0162\n",
      "2017-11-10 15:05:22: Loss and accuracy at step 1007: 0.0370701, 0.0112\n",
      "2017-11-10 15:05:25: Loss and accuracy at step 1008: 0.0370437, 0.0101\n",
      "2017-11-10 15:05:27: Loss and accuracy at step 1009: 0.0371482, 0.0239\n",
      "2017-11-10 15:05:29: Loss and accuracy at step 1010: 0.0370511, 0.0082\n",
      "2017-11-10 15:05:31: Loss and accuracy at step 1011: 0.037141, 0.0162\n",
      "2017-11-10 15:05:33: Loss and accuracy at step 1012: 0.0371108, 0.0193\n",
      "2017-11-10 15:05:35: Loss and accuracy at step 1013: 0.0372244, 0.0066\n",
      "2017-11-10 15:05:37: Loss and accuracy at step 1014: 0.0371316, 0.0186\n",
      "2017-11-10 15:05:39: Loss and accuracy at step 1015: 0.0371309, 0.0172\n",
      "2017-11-10 15:05:41: Loss and accuracy at step 1016: 0.0370795, 0.0111\n",
      "2017-11-10 15:05:43: Loss and accuracy at step 1017: 0.0371093, 0.0175\n",
      "2017-11-10 15:05:45: Loss and accuracy at step 1018: 0.0371695, 0.009\n",
      "2017-11-10 15:05:48: Loss and accuracy at step 1019: 0.0371072, 0.0187\n",
      "2017-11-10 15:05:50: Loss and accuracy at step 1020: 0.0371803, 0.0084\n",
      "2017-11-10 15:05:52: Loss and accuracy at step 1021: 0.0371822, 0.0131\n",
      "2017-11-10 15:05:54: Loss and accuracy at step 1022: 0.0371926, 0.0126\n",
      "2017-11-10 15:05:56: Loss and accuracy at step 1023: 0.0371679, 0.0085\n",
      "2017-11-10 15:05:58: Loss and accuracy at step 1024: 0.0370936, 0.0225\n",
      "2017-11-10 15:06:00: Loss and accuracy at step 1025: 0.0370761, 0.0106\n",
      "2017-11-10 15:06:02: Loss and accuracy at step 1026: 0.0370816, 0.01\n",
      "2017-11-10 15:06:04: Loss and accuracy at step 1027: 0.037176, 0.0162\n",
      "2017-11-10 15:06:06: Loss and accuracy at step 1028: 0.0372462, 0.0146\n",
      "2017-11-10 15:06:08: Loss and accuracy at step 1029: 0.0371094, 0.0106\n",
      "2017-11-10 15:06:11: Loss and accuracy at step 1030: 0.0372074, 0.0118\n",
      "2017-11-10 15:06:13: Loss and accuracy at step 1031: 0.0370715, 0.0153\n",
      "2017-11-10 15:06:15: Loss and accuracy at step 1032: 0.0370882, 0.0127\n",
      "2017-11-10 15:06:17: Loss and accuracy at step 1033: 0.0371459, 0.0091\n",
      "2017-11-10 15:06:19: Loss and accuracy at step 1034: 0.037172, 0.0139\n",
      "2017-11-10 15:06:21: Loss and accuracy at step 1035: 0.037129, 0.0248\n",
      "2017-11-10 15:06:23: Loss and accuracy at step 1036: 0.0370975, 0.0069\n",
      "2017-11-10 15:06:25: Loss and accuracy at step 1037: 0.037136, 0.0112\n",
      "2017-11-10 15:06:27: Loss and accuracy at step 1038: 0.0371559, 0.0266\n",
      "2017-11-10 15:06:30: Loss and accuracy at step 1039: 0.0371922, 0.0072\n",
      "2017-11-10 15:06:32: Loss and accuracy at step 1040: 0.0372264, 0.0111\n",
      "2017-11-10 15:06:34: Loss and accuracy at step 1041: 0.0371571, 0.0221\n",
      "2017-11-10 15:06:36: Loss and accuracy at step 1042: 0.0371179, 0.0123\n",
      "2017-11-10 15:06:38: Loss and accuracy at step 1043: 0.0370457, 0.015\n",
      "2017-11-10 15:06:40: Loss and accuracy at step 1044: 0.0371006, 0.0136\n",
      "2017-11-10 15:06:42: Loss and accuracy at step 1045: 0.0370701, 0.0144\n",
      "2017-11-10 15:06:44: Loss and accuracy at step 1046: 0.0371396, 0.0157\n",
      "2017-11-10 15:06:46: Loss and accuracy at step 1047: 0.0371239, 0.0151\n",
      "2017-11-10 15:06:48: Loss and accuracy at step 1048: 0.0370908, 0.0122\n",
      "2017-11-10 15:06:50: Loss and accuracy at step 1049: 0.0371497, 0.0157\n",
      "2017-11-10 15:06:52: Loss and accuracy at step 1050: 0.0371848, 0.0124\n",
      "2017-11-10 15:06:55: Loss and accuracy at step 1051: 0.0371535, 0.0081\n",
      "2017-11-10 15:06:57: Loss and accuracy at step 1052: 0.0371199, 0.0158\n",
      "2017-11-10 15:06:59: Loss and accuracy at step 1053: 0.0371655, 0.0124\n",
      "2017-11-10 15:07:01: Loss and accuracy at step 1054: 0.037196, 0.0123\n",
      "2017-11-10 15:07:03: Loss and accuracy at step 1055: 0.0371909, 0.0117\n",
      "2017-11-10 15:07:05: Loss and accuracy at step 1056: 0.0371482, 0.0145\n",
      "2017-11-10 15:07:07: Loss and accuracy at step 1057: 0.0371604, 0.0112\n",
      "2017-11-10 15:07:09: Loss and accuracy at step 1058: 0.0371501, 0.0148\n",
      "2017-11-10 15:07:11: Loss and accuracy at step 1059: 0.0370917, 0.0163\n",
      "2017-11-10 15:07:13: Loss and accuracy at step 1060: 0.0371194, 0.0108\n",
      "2017-11-10 15:07:15: Loss and accuracy at step 1061: 0.037172, 0.0103\n",
      "2017-11-10 15:07:17: Loss and accuracy at step 1062: 0.0370716, 0.0184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:07:20: Loss and accuracy at step 1063: 0.0371038, 0.0142\n",
      "2017-11-10 15:07:22: Loss and accuracy at step 1064: 0.0372174, 0.0091\n",
      "2017-11-10 15:07:24: Loss and accuracy at step 1065: 0.0370734, 0.0129\n",
      "2017-11-10 15:07:26: Loss and accuracy at step 1066: 0.0371089, 0.0117\n",
      "2017-11-10 15:07:28: Loss and accuracy at step 1067: 0.0371584, 0.0143\n",
      "2017-11-10 15:07:30: Loss and accuracy at step 1068: 0.0372282, 0.0107\n",
      "2017-11-10 15:07:32: Loss and accuracy at step 1069: 0.0371572, 0.0099\n",
      "2017-11-10 15:07:34: Loss and accuracy at step 1070: 0.0372061, 0.0126\n",
      "2017-11-10 15:07:36: Loss and accuracy at step 1071: 0.0372376, 0.0118\n",
      "2017-11-10 15:07:38: Loss and accuracy at step 1072: 0.0371254, 0.008\n",
      "2017-11-10 15:07:40: Loss and accuracy at step 1073: 0.0371938, 0.0178\n",
      "2017-11-10 15:07:43: Loss and accuracy at step 1074: 0.0371164, 0.0108\n",
      "2017-11-10 15:07:45: Loss and accuracy at step 1075: 0.0371664, 0.0147\n",
      "2017-11-10 15:07:47: Loss and accuracy at step 1076: 0.0370649, 0.0094\n",
      "2017-11-10 15:07:49: Loss and accuracy at step 1077: 0.0370697, 0.0142\n",
      "2017-11-10 15:07:51: Loss and accuracy at step 1078: 0.0371688, 0.0117\n",
      "2017-11-10 15:07:53: Loss and accuracy at step 1079: 0.0370091, 0.0099\n",
      "2017-11-10 15:07:55: Loss and accuracy at step 1080: 0.0370606, 0.0147\n",
      "2017-11-10 15:07:57: Loss and accuracy at step 1081: 0.0370575, 0.0131\n",
      "2017-11-10 15:07:59: Loss and accuracy at step 1082: 0.0370378, 0.0094\n",
      "2017-11-10 15:08:01: Loss and accuracy at step 1083: 0.0371231, 0.0144\n",
      "2017-11-10 15:08:03: Loss and accuracy at step 1084: 0.0371155, 0.019\n",
      "2017-11-10 15:08:06: Loss and accuracy at step 1085: 0.0371325, 0.0103\n",
      "2017-11-10 15:08:08: Loss and accuracy at step 1086: 0.0370755, 0.0115\n",
      "2017-11-10 15:08:10: Loss and accuracy at step 1087: 0.037051, 0.021\n",
      "2017-11-10 15:08:12: Loss and accuracy at step 1088: 0.0371207, 0.0087\n",
      "2017-11-10 15:08:14: Loss and accuracy at step 1089: 0.0371271, 0.0124\n",
      "2017-11-10 15:08:16: Loss and accuracy at step 1090: 0.0370817, 0.0139\n",
      "2017-11-10 15:08:18: Loss and accuracy at step 1091: 0.0371267, 0.0093\n",
      "2017-11-10 15:08:20: Loss and accuracy at step 1092: 0.0371474, 0.0167\n",
      "2017-11-10 15:08:22: Loss and accuracy at step 1093: 0.0371591, 0.0088\n",
      "2017-11-10 15:08:24: Loss and accuracy at step 1094: 0.0371427, 0.0126\n",
      "2017-11-10 15:08:27: Loss and accuracy at step 1095: 0.0371272, 0.0169\n",
      "2017-11-10 15:08:29: Loss and accuracy at step 1096: 0.0371388, 0.0095\n",
      "2017-11-10 15:08:31: Loss and accuracy at step 1097: 0.0370963, 0.0135\n",
      "2017-11-10 15:08:33: Loss and accuracy at step 1098: 0.0371397, 0.0173\n",
      "2017-11-10 15:08:35: Loss and accuracy at step 1099: 0.0371797, 0.0111\n",
      "2017-11-10 15:08:37: Loss and accuracy at step 1100: 0.0371421, 0.0116\n",
      "2017-11-10 15:08:39: Loss and accuracy at step 1101: 0.0370988, 0.0191\n",
      "2017-11-10 15:08:41: Loss and accuracy at step 1102: 0.0371882, 0.0081\n",
      "2017-11-10 15:08:43: Loss and accuracy at step 1103: 0.0370978, 0.0118\n",
      "2017-11-10 15:08:45: Loss and accuracy at step 1104: 0.0372157, 0.0177\n",
      "2017-11-10 15:08:47: Loss and accuracy at step 1105: 0.037125, 0.01\n",
      "2017-11-10 15:08:49: Loss and accuracy at step 1106: 0.0371409, 0.0125\n",
      "2017-11-10 15:08:52: Loss and accuracy at step 1107: 0.0370832, 0.024\n",
      "2017-11-10 15:08:54: Loss and accuracy at step 1108: 0.0371232, 0.0091\n",
      "2017-11-10 15:08:56: Loss and accuracy at step 1109: 0.0370643, 0.0144\n",
      "2017-11-10 15:08:58: Loss and accuracy at step 1110: 0.0370949, 0.0129\n",
      "2017-11-10 15:09:00: Loss and accuracy at step 1111: 0.0370534, 0.0116\n",
      "2017-11-10 15:09:02: Loss and accuracy at step 1112: 0.0371304, 0.019\n",
      "2017-11-10 15:09:04: Loss and accuracy at step 1113: 0.0371168, 0.0099\n",
      "2017-11-10 15:09:06: Loss and accuracy at step 1114: 0.0371076, 0.0129\n",
      "2017-11-10 15:09:08: Loss and accuracy at step 1115: 0.0371406, 0.0136\n",
      "2017-11-10 15:09:10: Loss and accuracy at step 1116: 0.0370731, 0.0139\n",
      "2017-11-10 15:09:12: Loss and accuracy at step 1117: 0.0370511, 0.009\n",
      "2017-11-10 15:09:14: Loss and accuracy at step 1118: 0.0371072, 0.0158\n",
      "2017-11-10 15:09:17: Loss and accuracy at step 1119: 0.0370373, 0.0105\n",
      "2017-11-10 15:09:19: Loss and accuracy at step 1120: 0.0371061, 0.009\n",
      "2017-11-10 15:09:21: Loss and accuracy at step 1121: 0.0370981, 0.0179\n",
      "2017-11-10 15:09:23: Loss and accuracy at step 1122: 0.0371427, 0.0069\n",
      "2017-11-10 15:09:25: Loss and accuracy at step 1123: 0.0371561, 0.0143\n",
      "2017-11-10 15:09:27: Loss and accuracy at step 1124: 0.0371593, 0.0156\n",
      "2017-11-10 15:09:29: Loss and accuracy at step 1125: 0.0371156, 0.0083\n",
      "2017-11-10 15:09:31: Loss and accuracy at step 1126: 0.0372557, 0.0128\n",
      "2017-11-10 15:09:33: Loss and accuracy at step 1127: 0.0370612, 0.0158\n",
      "2017-11-10 15:09:35: Loss and accuracy at step 1128: 0.0371684, 0.0108\n",
      "2017-11-10 15:09:37: Loss and accuracy at step 1129: 0.0370776, 0.0118\n",
      "2017-11-10 15:09:40: Loss and accuracy at step 1130: 0.0370521, 0.0116\n",
      "2017-11-10 15:09:42: Loss and accuracy at step 1131: 0.0371511, 0.0134\n",
      "2017-11-10 15:09:44: Loss and accuracy at step 1132: 0.0371594, 0.0109\n",
      "2017-11-10 15:09:46: Loss and accuracy at step 1133: 0.0370944, 0.0105\n",
      "2017-11-10 15:09:48: Loss and accuracy at step 1134: 0.0371352, 0.0156\n",
      "2017-11-10 15:09:50: Loss and accuracy at step 1135: 0.0371214, 0.0103\n",
      "2017-11-10 15:09:52: Loss and accuracy at step 1136: 0.0371925, 0.0106\n",
      "2017-11-10 15:09:54: Loss and accuracy at step 1137: 0.0372461, 0.0133\n",
      "2017-11-10 15:09:56: Loss and accuracy at step 1138: 0.0371548, 0.0145\n",
      "2017-11-10 15:09:58: Loss and accuracy at step 1139: 0.0371672, 0.0131\n",
      "2017-11-10 15:10:00: Loss and accuracy at step 1140: 0.0371032, 0.0115\n",
      "2017-11-10 15:10:02: Loss and accuracy at step 1141: 0.0371774, 0.0163\n",
      "2017-11-10 15:10:05: Loss and accuracy at step 1142: 0.0371186, 0.0102\n",
      "2017-11-10 15:10:07: Loss and accuracy at step 1143: 0.0371574, 0.0132\n",
      "2017-11-10 15:10:09: Loss and accuracy at step 1144: 0.0371115, 0.0158\n",
      "2017-11-10 15:10:11: Loss and accuracy at step 1145: 0.0370735, 0.0088\n",
      "2017-11-10 15:10:13: Loss and accuracy at step 1146: 0.0371565, 0.0193\n",
      "2017-11-10 15:10:15: Loss and accuracy at step 1147: 0.0370976, 0.0104\n",
      "2017-11-10 15:10:17: Loss and accuracy at step 1148: 0.0370241, 0.0105\n",
      "2017-11-10 15:10:19: Loss and accuracy at step 1149: 0.0370732, 0.0179\n",
      "2017-11-10 15:10:21: Loss and accuracy at step 1150: 0.0371212, 0.0117\n",
      "2017-11-10 15:10:24: Loss and accuracy at step 1151: 0.0370647, 0.018\n",
      "2017-11-10 15:10:26: Loss and accuracy at step 1152: 0.0371608, 0.0098\n",
      "2017-11-10 15:10:28: Loss and accuracy at step 1153: 0.0371306, 0.0149\n",
      "2017-11-10 15:10:30: Loss and accuracy at step 1154: 0.0371142, 0.0142\n",
      "2017-11-10 15:10:32: Loss and accuracy at step 1155: 0.0371387, 0.0112\n",
      "2017-11-10 15:10:34: Loss and accuracy at step 1156: 0.0371527, 0.0168\n",
      "2017-11-10 15:10:36: Loss and accuracy at step 1157: 0.0371376, 0.0109\n",
      "2017-11-10 15:10:38: Loss and accuracy at step 1158: 0.036999, 0.0103\n",
      "2017-11-10 15:10:40: Loss and accuracy at step 1159: 0.0370337, 0.0256\n",
      "2017-11-10 15:10:42: Loss and accuracy at step 1160: 0.0370722, 0.0102\n",
      "2017-11-10 15:10:44: Loss and accuracy at step 1161: 0.0370359, 0.0115\n",
      "2017-11-10 15:10:46: Loss and accuracy at step 1162: 0.0370621, 0.0207\n",
      "2017-11-10 15:10:49: Loss and accuracy at step 1163: 0.0370693, 0.0119\n",
      "2017-11-10 15:10:51: Loss and accuracy at step 1164: 0.037065, 0.0092\n",
      "2017-11-10 15:10:53: Loss and accuracy at step 1165: 0.0370991, 0.0222\n",
      "2017-11-10 15:10:55: Loss and accuracy at step 1166: 0.0370125, 0.0084\n",
      "2017-11-10 15:10:57: Loss and accuracy at step 1167: 0.0369855, 0.0133\n",
      "2017-11-10 15:10:59: Loss and accuracy at step 1168: 0.0370132, 0.0166\n",
      "2017-11-10 15:11:01: Loss and accuracy at step 1169: 0.0370205, 0.0129\n",
      "2017-11-10 15:11:03: Loss and accuracy at step 1170: 0.0371239, 0.0087\n",
      "2017-11-10 15:11:05: Loss and accuracy at step 1171: 0.037088, 0.0156\n",
      "2017-11-10 15:11:07: Loss and accuracy at step 1172: 0.0371719, 0.0146\n",
      "2017-11-10 15:11:09: Loss and accuracy at step 1173: 0.0370044, 0.0107\n",
      "2017-11-10 15:11:12: Loss and accuracy at step 1174: 0.0370416, 0.0127\n",
      "2017-11-10 15:11:14: Loss and accuracy at step 1175: 0.0371199, 0.0143\n",
      "2017-11-10 15:11:16: Loss and accuracy at step 1176: 0.0370916, 0.0134\n",
      "2017-11-10 15:11:18: Loss and accuracy at step 1177: 0.0370368, 0.0113\n",
      "2017-11-10 15:11:20: Loss and accuracy at step 1178: 0.0370797, 0.0148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:11:22: Loss and accuracy at step 1179: 0.037129, 0.0165\n",
      "2017-11-10 15:11:24: Loss and accuracy at step 1180: 0.0371452, 0.0076\n",
      "2017-11-10 15:11:26: Loss and accuracy at step 1181: 0.0370689, 0.0149\n",
      "2017-11-10 15:11:28: Loss and accuracy at step 1182: 0.0371162, 0.0149\n",
      "2017-11-10 15:11:30: Loss and accuracy at step 1183: 0.0369907, 0.0086\n",
      "2017-11-10 15:11:32: Loss and accuracy at step 1184: 0.0370826, 0.013\n",
      "2017-11-10 15:11:35: Loss and accuracy at step 1185: 0.0371077, 0.0126\n",
      "2017-11-10 15:11:37: Loss and accuracy at step 1186: 0.037146, 0.0167\n",
      "2017-11-10 15:11:39: Loss and accuracy at step 1187: 0.0371774, 0.009\n",
      "2017-11-10 15:11:41: Loss and accuracy at step 1188: 0.0371852, 0.0114\n",
      "2017-11-10 15:11:43: Loss and accuracy at step 1189: 0.0372302, 0.0142\n",
      "2017-11-10 15:11:45: Loss and accuracy at step 1190: 0.0371595, 0.0083\n",
      "2017-11-10 15:11:47: Loss and accuracy at step 1191: 0.0371717, 0.0131\n",
      "2017-11-10 15:11:49: Loss and accuracy at step 1192: 0.0371661, 0.0151\n",
      "2017-11-10 15:11:51: Loss and accuracy at step 1193: 0.0371386, 0.0104\n",
      "2017-11-10 15:11:53: Loss and accuracy at step 1194: 0.0371977, 0.0098\n",
      "2017-11-10 15:11:55: Loss and accuracy at step 1195: 0.0371608, 0.019\n",
      "2017-11-10 15:11:57: Loss and accuracy at step 1196: 0.0370695, 0.0104\n",
      "2017-11-10 15:12:00: Loss and accuracy at step 1197: 0.03717, 0.0116\n",
      "2017-11-10 15:12:02: Loss and accuracy at step 1198: 0.0370731, 0.0206\n",
      "2017-11-10 15:12:04: Loss and accuracy at step 1199: 0.0371125, 0.009\n",
      "2017-11-10 15:12:06: Loss and accuracy at step 1200: 0.0369471, 0.0193\n",
      "2017-11-10 15:12:08: Loss and accuracy at step 1201: 0.0370553, 0.0194\n",
      "2017-11-10 15:12:10: Loss and accuracy at step 1202: 0.0370591, 0.01\n",
      "2017-11-10 15:12:12: Loss and accuracy at step 1203: 0.0370526, 0.0162\n",
      "2017-11-10 15:12:14: Loss and accuracy at step 1204: 0.0370779, 0.0212\n",
      "2017-11-10 15:12:16: Loss and accuracy at step 1205: 0.0370187, 0.0106\n",
      "2017-11-10 15:12:19: Loss and accuracy at step 1206: 0.0370115, 0.011\n",
      "2017-11-10 15:12:21: Loss and accuracy at step 1207: 0.037135, 0.0158\n",
      "2017-11-10 15:12:23: Loss and accuracy at step 1208: 0.0370788, 0.0148\n",
      "2017-11-10 15:12:25: Loss and accuracy at step 1209: 0.0371945, 0.009\n",
      "2017-11-10 15:12:27: Loss and accuracy at step 1210: 0.037122, 0.0132\n",
      "2017-11-10 15:12:29: Loss and accuracy at step 1211: 0.0370431, 0.0132\n",
      "2017-11-10 15:12:31: Loss and accuracy at step 1212: 0.037251, 0.0127\n",
      "2017-11-10 15:12:33: Loss and accuracy at step 1213: 0.0370197, 0.0092\n",
      "2017-11-10 15:12:35: Loss and accuracy at step 1214: 0.0371245, 0.0195\n",
      "2017-11-10 15:12:37: Loss and accuracy at step 1215: 0.0370819, 0.0137\n",
      "2017-11-10 15:12:39: Loss and accuracy at step 1216: 0.0371228, 0.0097\n",
      "2017-11-10 15:12:41: Loss and accuracy at step 1217: 0.0369584, 0.0164\n",
      "2017-11-10 15:12:44: Loss and accuracy at step 1218: 0.0370752, 0.0138\n",
      "2017-11-10 15:12:46: Loss and accuracy at step 1219: 0.037008, 0.0122\n",
      "2017-11-10 15:12:48: Loss and accuracy at step 1220: 0.0370795, 0.0154\n",
      "2017-11-10 15:12:50: Loss and accuracy at step 1221: 0.0370509, 0.0125\n",
      "2017-11-10 15:12:52: Loss and accuracy at step 1222: 0.0370614, 0.0097\n",
      "2017-11-10 15:12:54: Loss and accuracy at step 1223: 0.0371512, 0.0138\n",
      "2017-11-10 15:12:56: Loss and accuracy at step 1224: 0.0370892, 0.0211\n",
      "2017-11-10 15:12:58: Loss and accuracy at step 1225: 0.0370108, 0.0093\n",
      "2017-11-10 15:13:00: Loss and accuracy at step 1226: 0.0370277, 0.0108\n",
      "2017-11-10 15:13:02: Loss and accuracy at step 1227: 0.0371172, 0.0192\n",
      "2017-11-10 15:13:04: Loss and accuracy at step 1228: 0.0370369, 0.0124\n",
      "2017-11-10 15:13:07: Loss and accuracy at step 1229: 0.0369988, 0.0091\n",
      "2017-11-10 15:13:09: Loss and accuracy at step 1230: 0.0370145, 0.0137\n",
      "2017-11-10 15:13:11: Loss and accuracy at step 1231: 0.0371864, 0.0125\n",
      "2017-11-10 15:13:13: Loss and accuracy at step 1232: 0.0370685, 0.01\n",
      "2017-11-10 15:13:15: Loss and accuracy at step 1233: 0.0371072, 0.0117\n",
      "2017-11-10 15:13:17: Loss and accuracy at step 1234: 0.0370169, 0.0205\n",
      "2017-11-10 15:13:19: Loss and accuracy at step 1235: 0.0371485, 0.0109\n",
      "2017-11-10 15:13:21: Loss and accuracy at step 1236: 0.0371737, 0.0083\n",
      "2017-11-10 15:13:23: Loss and accuracy at step 1237: 0.0370487, 0.0175\n",
      "2017-11-10 15:13:25: Loss and accuracy at step 1238: 0.0370939, 0.0119\n",
      "2017-11-10 15:13:27: Loss and accuracy at step 1239: 0.0370896, 0.0148\n",
      "2017-11-10 15:13:30: Loss and accuracy at step 1240: 0.0371087, 0.0103\n",
      "2017-11-10 15:13:32: Loss and accuracy at step 1241: 0.0370207, 0.0081\n",
      "2017-11-10 15:13:34: Loss and accuracy at step 1242: 0.0370946, 0.016\n",
      "2017-11-10 15:13:36: Loss and accuracy at step 1243: 0.0370919, 0.01\n",
      "2017-11-10 15:13:38: Loss and accuracy at step 1244: 0.0371201, 0.01\n",
      "2017-11-10 15:13:40: Loss and accuracy at step 1245: 0.0370424, 0.0158\n",
      "2017-11-10 15:13:42: Loss and accuracy at step 1246: 0.0370447, 0.0113\n",
      "2017-11-10 15:13:44: Loss and accuracy at step 1247: 0.0371279, 0.0119\n",
      "2017-11-10 15:13:46: Loss and accuracy at step 1248: 0.0370493, 0.0125\n",
      "2017-11-10 15:13:48: Loss and accuracy at step 1249: 0.0370492, 0.0175\n",
      "2017-11-10 15:13:50: Loss and accuracy at step 1250: 0.0370453, 0.008\n",
      "2017-11-10 15:13:53: Loss and accuracy at step 1251: 0.0370785, 0.0093\n",
      "2017-11-10 15:13:55: Loss and accuracy at step 1252: 0.0370149, 0.0167\n",
      "2017-11-10 15:13:57: Loss and accuracy at step 1253: 0.0369498, 0.0083\n",
      "2017-11-10 15:13:59: Loss and accuracy at step 1254: 0.0370393, 0.016\n",
      "2017-11-10 15:14:01: Loss and accuracy at step 1255: 0.0370536, 0.0087\n",
      "2017-11-10 15:14:03: Loss and accuracy at step 1256: 0.037126, 0.0205\n",
      "2017-11-10 15:14:05: Loss and accuracy at step 1257: 0.0370439, 0.0091\n",
      "2017-11-10 15:14:07: Loss and accuracy at step 1258: 0.0371734, 0.0096\n",
      "2017-11-10 15:14:09: Loss and accuracy at step 1259: 0.0370819, 0.0205\n",
      "2017-11-10 15:14:11: Loss and accuracy at step 1260: 0.037231, 0.0064\n",
      "2017-11-10 15:14:14: Loss and accuracy at step 1261: 0.0371087, 0.0172\n",
      "2017-11-10 15:14:16: Loss and accuracy at step 1262: 0.0371611, 0.0105\n",
      "2017-11-10 15:14:18: Loss and accuracy at step 1263: 0.0370288, 0.0126\n",
      "2017-11-10 15:14:20: Loss and accuracy at step 1264: 0.0371039, 0.0167\n",
      "2017-11-10 15:14:22: Loss and accuracy at step 1265: 0.0370517, 0.0141\n",
      "2017-11-10 15:14:24: Loss and accuracy at step 1266: 0.0370483, 0.0205\n",
      "2017-11-10 15:14:26: Loss and accuracy at step 1267: 0.0370184, 0.0104\n",
      "2017-11-10 15:14:28: Loss and accuracy at step 1268: 0.037057, 0.0117\n",
      "2017-11-10 15:14:30: Loss and accuracy at step 1269: 0.0370824, 0.0245\n",
      "2017-11-10 15:14:32: Loss and accuracy at step 1270: 0.0370699, 0.0082\n",
      "2017-11-10 15:14:34: Loss and accuracy at step 1271: 0.0370881, 0.0165\n",
      "2017-11-10 15:14:37: Loss and accuracy at step 1272: 0.0370629, 0.0137\n",
      "2017-11-10 15:14:39: Loss and accuracy at step 1273: 0.0371164, 0.0114\n",
      "2017-11-10 15:14:41: Loss and accuracy at step 1274: 0.0371338, 0.0147\n",
      "2017-11-10 15:14:43: Loss and accuracy at step 1275: 0.0371791, 0.0138\n",
      "2017-11-10 15:14:45: Loss and accuracy at step 1276: 0.0370761, 0.0118\n",
      "2017-11-10 15:14:47: Loss and accuracy at step 1277: 0.0370666, 0.0106\n",
      "2017-11-10 15:14:49: Loss and accuracy at step 1278: 0.0370601, 0.0172\n",
      "2017-11-10 15:14:51: Loss and accuracy at step 1279: 0.037129, 0.0108\n",
      "2017-11-10 15:14:53: Loss and accuracy at step 1280: 0.0370844, 0.013\n",
      "2017-11-10 15:14:55: Loss and accuracy at step 1281: 0.0369942, 0.0125\n",
      "2017-11-10 15:14:57: Loss and accuracy at step 1282: 0.0370832, 0.0115\n",
      "2017-11-10 15:15:00: Loss and accuracy at step 1283: 0.0370808, 0.0159\n",
      "2017-11-10 15:15:02: Loss and accuracy at step 1284: 0.0371319, 0.011\n",
      "2017-11-10 15:15:04: Loss and accuracy at step 1285: 0.037017, 0.0138\n",
      "2017-11-10 15:15:06: Loss and accuracy at step 1286: 0.0370094, 0.0085\n",
      "2017-11-10 15:15:08: Loss and accuracy at step 1287: 0.0371222, 0.0105\n",
      "2017-11-10 15:15:10: Loss and accuracy at step 1288: 0.0370134, 0.0168\n",
      "2017-11-10 15:15:12: Loss and accuracy at step 1289: 0.0370528, 0.0081\n",
      "2017-11-10 15:15:14: Loss and accuracy at step 1290: 0.036994, 0.0139\n",
      "2017-11-10 15:15:16: Loss and accuracy at step 1291: 0.037065, 0.0144\n",
      "2017-11-10 15:15:18: Loss and accuracy at step 1292: 0.0370819, 0.0079\n",
      "2017-11-10 15:15:20: Loss and accuracy at step 1293: 0.0370967, 0.0161\n",
      "2017-11-10 15:15:22: Loss and accuracy at step 1294: 0.0370267, 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:15:25: Loss and accuracy at step 1295: 0.0370543, 0.0129\n",
      "2017-11-10 15:15:27: Loss and accuracy at step 1296: 0.037086, 0.0152\n",
      "2017-11-10 15:15:29: Loss and accuracy at step 1297: 0.0370903, 0.0123\n",
      "2017-11-10 15:15:31: Loss and accuracy at step 1298: 0.0370388, 0.01\n",
      "2017-11-10 15:15:33: Loss and accuracy at step 1299: 0.0370207, 0.0184\n",
      "2017-11-10 15:15:35: Loss and accuracy at step 1300: 0.0370227, 0.0114\n",
      "2017-11-10 15:15:37: Loss and accuracy at step 1301: 0.037097, 0.0127\n",
      "2017-11-10 15:15:39: Loss and accuracy at step 1302: 0.037077, 0.0145\n",
      "2017-11-10 15:15:41: Loss and accuracy at step 1303: 0.037043, 0.0106\n",
      "2017-11-10 15:15:43: Loss and accuracy at step 1304: 0.0370283, 0.0127\n",
      "2017-11-10 15:15:45: Loss and accuracy at step 1305: 0.0370731, 0.0136\n",
      "2017-11-10 15:15:48: Loss and accuracy at step 1306: 0.0370543, 0.0113\n",
      "2017-11-10 15:15:50: Loss and accuracy at step 1307: 0.0370715, 0.0116\n",
      "2017-11-10 15:15:52: Loss and accuracy at step 1308: 0.0370543, 0.0177\n",
      "2017-11-10 15:15:54: Loss and accuracy at step 1309: 0.0370367, 0.009\n",
      "2017-11-10 15:15:56: Loss and accuracy at step 1310: 0.0370655, 0.0153\n",
      "2017-11-10 15:15:58: Loss and accuracy at step 1311: 0.0370951, 0.0116\n",
      "2017-11-10 15:16:00: Loss and accuracy at step 1312: 0.0371381, 0.0093\n",
      "2017-11-10 15:16:02: Loss and accuracy at step 1313: 0.0370776, 0.0134\n",
      "2017-11-10 15:16:04: Loss and accuracy at step 1314: 0.0370159, 0.0095\n",
      "2017-11-10 15:16:06: Loss and accuracy at step 1315: 0.0369481, 0.0178\n",
      "2017-11-10 15:16:08: Loss and accuracy at step 1316: 0.0371301, 0.0122\n",
      "2017-11-10 15:16:11: Loss and accuracy at step 1317: 0.0370464, 0.0091\n",
      "2017-11-10 15:16:13: Loss and accuracy at step 1318: 0.037108, 0.0189\n",
      "2017-11-10 15:16:15: Loss and accuracy at step 1319: 0.0371406, 0.0086\n",
      "2017-11-10 15:16:17: Loss and accuracy at step 1320: 0.0370312, 0.0152\n",
      "2017-11-10 15:16:19: Loss and accuracy at step 1321: 0.0370937, 0.0154\n",
      "2017-11-10 15:16:21: Loss and accuracy at step 1322: 0.037041, 0.0134\n",
      "2017-11-10 15:16:23: Loss and accuracy at step 1323: 0.0371131, 0.0134\n",
      "2017-11-10 15:16:25: Loss and accuracy at step 1324: 0.0370373, 0.0139\n",
      "2017-11-10 15:16:27: Loss and accuracy at step 1325: 0.0370572, 0.0134\n",
      "2017-11-10 15:16:29: Loss and accuracy at step 1326: 0.0370513, 0.0103\n",
      "2017-11-10 15:16:32: Loss and accuracy at step 1327: 0.0370179, 0.0205\n",
      "2017-11-10 15:16:34: Loss and accuracy at step 1328: 0.0370091, 0.0137\n",
      "2017-11-10 15:16:36: Loss and accuracy at step 1329: 0.0370505, 0.0088\n",
      "2017-11-10 15:16:38: Loss and accuracy at step 1330: 0.0370157, 0.0152\n",
      "2017-11-10 15:16:40: Loss and accuracy at step 1331: 0.0369917, 0.0104\n",
      "2017-11-10 15:16:42: Loss and accuracy at step 1332: 0.037004, 0.016\n",
      "2017-11-10 15:16:44: Loss and accuracy at step 1333: 0.0371255, 0.0146\n",
      "2017-11-10 15:16:46: Loss and accuracy at step 1334: 0.0370762, 0.0081\n",
      "2017-11-10 15:16:48: Loss and accuracy at step 1335: 0.0369668, 0.0112\n",
      "2017-11-10 15:16:50: Loss and accuracy at step 1336: 0.0370444, 0.0116\n",
      "2017-11-10 15:16:52: Loss and accuracy at step 1337: 0.037015, 0.0128\n",
      "2017-11-10 15:16:54: Loss and accuracy at step 1338: 0.0370808, 0.0161\n",
      "2017-11-10 15:16:57: Loss and accuracy at step 1339: 0.0370448, 0.0093\n",
      "2017-11-10 15:16:59: Loss and accuracy at step 1340: 0.0370487, 0.0126\n",
      "2017-11-10 15:17:01: Loss and accuracy at step 1341: 0.0370828, 0.0271\n",
      "2017-11-10 15:17:03: Loss and accuracy at step 1342: 0.0370566, 0.009\n",
      "2017-11-10 15:17:05: Loss and accuracy at step 1343: 0.0370241, 0.0125\n",
      "2017-11-10 15:17:07: Loss and accuracy at step 1344: 0.0370925, 0.0194\n",
      "2017-11-10 15:17:09: Loss and accuracy at step 1345: 0.0370989, 0.0149\n",
      "2017-11-10 15:17:11: Loss and accuracy at step 1346: 0.0370312, 0.0082\n",
      "2017-11-10 15:17:13: Loss and accuracy at step 1347: 0.037024, 0.0125\n",
      "2017-11-10 15:17:15: Loss and accuracy at step 1348: 0.0370896, 0.0142\n",
      "2017-11-10 15:17:17: Loss and accuracy at step 1349: 0.0369954, 0.01\n",
      "2017-11-10 15:17:19: Loss and accuracy at step 1350: 0.0369889, 0.0097\n",
      "2017-11-10 15:17:22: Loss and accuracy at step 1351: 0.0370906, 0.0173\n",
      "2017-11-10 15:17:24: Loss and accuracy at step 1352: 0.0370105, 0.0155\n",
      "2017-11-10 15:17:26: Loss and accuracy at step 1353: 0.0371033, 0.0092\n",
      "2017-11-10 15:17:28: Loss and accuracy at step 1354: 0.0371222, 0.0173\n",
      "2017-11-10 15:17:30: Loss and accuracy at step 1355: 0.0369902, 0.0157\n",
      "2017-11-10 15:17:32: Loss and accuracy at step 1356: 0.0370143, 0.0071\n",
      "2017-11-10 15:17:34: Loss and accuracy at step 1357: 0.037019, 0.0187\n",
      "2017-11-10 15:17:36: Loss and accuracy at step 1358: 0.0370485, 0.0166\n",
      "2017-11-10 15:17:38: Loss and accuracy at step 1359: 0.0370579, 0.0066\n",
      "2017-11-10 15:17:40: Loss and accuracy at step 1360: 0.0370056, 0.019\n",
      "2017-11-10 15:17:43: Loss and accuracy at step 1361: 0.0370534, 0.0108\n",
      "2017-11-10 15:17:45: Loss and accuracy at step 1362: 0.0370417, 0.0119\n",
      "2017-11-10 15:17:47: Loss and accuracy at step 1363: 0.0370607, 0.0132\n",
      "2017-11-10 15:17:49: Loss and accuracy at step 1364: 0.0369778, 0.013\n",
      "2017-11-10 15:17:51: Loss and accuracy at step 1365: 0.037013, 0.0162\n",
      "2017-11-10 15:17:53: Loss and accuracy at step 1366: 0.0370413, 0.009\n",
      "2017-11-10 15:17:55: Loss and accuracy at step 1367: 0.0370381, 0.0169\n",
      "2017-11-10 15:17:57: Loss and accuracy at step 1368: 0.0370784, 0.0157\n",
      "2017-11-10 15:17:59: Loss and accuracy at step 1369: 0.0370048, 0.0105\n",
      "2017-11-10 15:18:01: Loss and accuracy at step 1370: 0.0370491, 0.0121\n",
      "2017-11-10 15:18:03: Loss and accuracy at step 1371: 0.037049, 0.0124\n",
      "2017-11-10 15:18:05: Loss and accuracy at step 1372: 0.0371289, 0.0129\n",
      "2017-11-10 15:18:08: Loss and accuracy at step 1373: 0.0370624, 0.0091\n",
      "2017-11-10 15:18:10: Loss and accuracy at step 1374: 0.0370677, 0.0209\n",
      "2017-11-10 15:18:12: Loss and accuracy at step 1375: 0.0370015, 0.0149\n",
      "2017-11-10 15:18:14: Loss and accuracy at step 1376: 0.0370398, 0.0095\n",
      "2017-11-10 15:18:16: Loss and accuracy at step 1377: 0.0369771, 0.0173\n",
      "2017-11-10 15:18:18: Loss and accuracy at step 1378: 0.0370601, 0.0114\n",
      "2017-11-10 15:18:20: Loss and accuracy at step 1379: 0.0369906, 0.0133\n",
      "2017-11-10 15:18:22: Loss and accuracy at step 1380: 0.0369497, 0.0126\n",
      "2017-11-10 15:18:24: Loss and accuracy at step 1381: 0.036941, 0.0118\n",
      "2017-11-10 15:18:27: Loss and accuracy at step 1382: 0.0370106, 0.013\n",
      "2017-11-10 15:18:29: Loss and accuracy at step 1383: 0.0370104, 0.0097\n",
      "2017-11-10 15:18:31: Loss and accuracy at step 1384: 0.0370073, 0.0139\n",
      "2017-11-10 15:18:33: Loss and accuracy at step 1385: 0.036989, 0.0094\n",
      "2017-11-10 15:18:35: Loss and accuracy at step 1386: 0.0369633, 0.0142\n",
      "2017-11-10 15:18:37: Loss and accuracy at step 1387: 0.0371322, 0.0104\n",
      "2017-11-10 15:18:39: Loss and accuracy at step 1388: 0.0370763, 0.01\n",
      "2017-11-10 15:18:41: Loss and accuracy at step 1389: 0.0370192, 0.0129\n",
      "2017-11-10 15:18:43: Loss and accuracy at step 1390: 0.0369752, 0.0133\n",
      "2017-11-10 15:18:45: Loss and accuracy at step 1391: 0.0370248, 0.0121\n",
      "2017-11-10 15:18:47: Loss and accuracy at step 1392: 0.0370514, 0.0108\n",
      "2017-11-10 15:18:50: Loss and accuracy at step 1393: 0.0370639, 0.0137\n",
      "2017-11-10 15:18:52: Loss and accuracy at step 1394: 0.0371627, 0.0125\n",
      "2017-11-10 15:18:54: Loss and accuracy at step 1395: 0.0371564, 0.0076\n",
      "2017-11-10 15:18:56: Loss and accuracy at step 1396: 0.0371078, 0.0119\n",
      "2017-11-10 15:18:58: Loss and accuracy at step 1397: 0.0370138, 0.012\n",
      "2017-11-10 15:19:00: Loss and accuracy at step 1398: 0.0370457, 0.0119\n",
      "2017-11-10 15:19:02: Loss and accuracy at step 1399: 0.0370528, 0.0119\n",
      "2017-11-10 15:19:04: Loss and accuracy at step 1400: 0.0369384, 0.0105\n",
      "2017-11-10 15:19:06: Loss and accuracy at step 1401: 0.037031, 0.0127\n",
      "2017-11-10 15:19:08: Loss and accuracy at step 1402: 0.0370444, 0.0128\n",
      "2017-11-10 15:19:10: Loss and accuracy at step 1403: 0.0369676, 0.012\n",
      "2017-11-10 15:19:12: Loss and accuracy at step 1404: 0.0370191, 0.0112\n",
      "2017-11-10 15:19:15: Loss and accuracy at step 1405: 0.0371151, 0.0126\n",
      "2017-11-10 15:19:17: Loss and accuracy at step 1406: 0.0369977, 0.0159\n",
      "2017-11-10 15:19:19: Loss and accuracy at step 1407: 0.0370309, 0.008\n",
      "2017-11-10 15:19:21: Loss and accuracy at step 1408: 0.0370901, 0.0254\n",
      "2017-11-10 15:19:23: Loss and accuracy at step 1409: 0.0370483, 0.0102\n",
      "2017-11-10 15:19:25: Loss and accuracy at step 1410: 0.0370029, 0.0121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:19:27: Loss and accuracy at step 1411: 0.0370361, 0.019\n",
      "2017-11-10 15:19:29: Loss and accuracy at step 1412: 0.037061, 0.0087\n",
      "2017-11-10 15:19:31: Loss and accuracy at step 1413: 0.0370611, 0.0131\n",
      "2017-11-10 15:19:33: Loss and accuracy at step 1414: 0.0370194, 0.0141\n",
      "2017-11-10 15:19:35: Loss and accuracy at step 1415: 0.0370145, 0.0095\n",
      "2017-11-10 15:19:38: Loss and accuracy at step 1416: 0.0369394, 0.0141\n",
      "2017-11-10 15:19:40: Loss and accuracy at step 1417: 0.0370107, 0.0109\n",
      "2017-11-10 15:19:42: Loss and accuracy at step 1418: 0.0370883, 0.0131\n",
      "2017-11-10 15:19:44: Loss and accuracy at step 1419: 0.0370053, 0.0141\n",
      "2017-11-10 15:19:46: Loss and accuracy at step 1420: 0.0370735, 0.008\n",
      "2017-11-10 15:19:48: Loss and accuracy at step 1421: 0.0370163, 0.0171\n",
      "2017-11-10 15:19:50: Loss and accuracy at step 1422: 0.0370048, 0.0127\n",
      "2017-11-10 15:19:52: Loss and accuracy at step 1423: 0.0371012, 0.0086\n",
      "2017-11-10 15:19:54: Loss and accuracy at step 1424: 0.0371127, 0.0191\n",
      "2017-11-10 15:19:56: Loss and accuracy at step 1425: 0.0371515, 0.0085\n",
      "2017-11-10 15:19:58: Loss and accuracy at step 1426: 0.0370762, 0.0161\n",
      "2017-11-10 15:20:00: Loss and accuracy at step 1427: 0.0370804, 0.0121\n",
      "2017-11-10 15:20:02: Loss and accuracy at step 1428: 0.0371027, 0.0121\n",
      "2017-11-10 15:20:05: Loss and accuracy at step 1429: 0.0370356, 0.0142\n",
      "2017-11-10 15:20:07: Loss and accuracy at step 1430: 0.0370077, 0.0102\n",
      "2017-11-10 15:20:09: Loss and accuracy at step 1431: 0.0370205, 0.0146\n",
      "2017-11-10 15:20:11: Loss and accuracy at step 1432: 0.0370378, 0.0115\n",
      "2017-11-10 15:20:13: Loss and accuracy at step 1433: 0.0370215, 0.0124\n",
      "2017-11-10 15:20:15: Loss and accuracy at step 1434: 0.0370455, 0.014\n",
      "2017-11-10 15:20:17: Loss and accuracy at step 1435: 0.0369477, 0.0081\n",
      "2017-11-10 15:20:19: Loss and accuracy at step 1436: 0.0369662, 0.0171\n",
      "2017-11-10 15:20:21: Loss and accuracy at step 1437: 0.0370128, 0.0093\n",
      "2017-11-10 15:20:24: Loss and accuracy at step 1438: 0.0370656, 0.0121\n",
      "2017-11-10 15:20:26: Loss and accuracy at step 1439: 0.0369653, 0.016\n",
      "2017-11-10 15:20:28: Loss and accuracy at step 1440: 0.0369907, 0.0083\n",
      "2017-11-10 15:20:30: Loss and accuracy at step 1441: 0.0370072, 0.0161\n",
      "2017-11-10 15:20:32: Loss and accuracy at step 1442: 0.0370432, 0.0122\n",
      "2017-11-10 15:20:34: Loss and accuracy at step 1443: 0.0371239, 0.016\n",
      "2017-11-10 15:20:36: Loss and accuracy at step 1444: 0.0370159, 0.0124\n",
      "2017-11-10 15:20:38: Loss and accuracy at step 1445: 0.0369854, 0.0133\n",
      "2017-11-10 15:20:40: Loss and accuracy at step 1446: 0.0369886, 0.0167\n",
      "2017-11-10 15:20:42: Loss and accuracy at step 1447: 0.0370715, 0.0094\n",
      "2017-11-10 15:20:44: Loss and accuracy at step 1448: 0.0369599, 0.0207\n",
      "2017-11-10 15:20:46: Loss and accuracy at step 1449: 0.0370237, 0.011\n",
      "2017-11-10 15:20:49: Loss and accuracy at step 1450: 0.0369958, 0.0099\n",
      "2017-11-10 15:20:51: Loss and accuracy at step 1451: 0.0370394, 0.0226\n",
      "2017-11-10 15:20:53: Loss and accuracy at step 1452: 0.0370705, 0.0076\n",
      "2017-11-10 15:20:55: Loss and accuracy at step 1453: 0.0369652, 0.0164\n",
      "2017-11-10 15:20:57: Loss and accuracy at step 1454: 0.0370647, 0.014\n",
      "2017-11-10 15:20:59: Loss and accuracy at step 1455: 0.0369827, 0.0106\n",
      "2017-11-10 15:21:01: Loss and accuracy at step 1456: 0.0369977, 0.0194\n",
      "2017-11-10 15:21:03: Loss and accuracy at step 1457: 0.0370621, 0.0095\n",
      "2017-11-10 15:21:05: Loss and accuracy at step 1458: 0.0369913, 0.0124\n",
      "2017-11-10 15:21:07: Loss and accuracy at step 1459: 0.0370405, 0.0166\n",
      "2017-11-10 15:21:09: Loss and accuracy at step 1460: 0.0369972, 0.0073\n",
      "2017-11-10 15:21:12: Loss and accuracy at step 1461: 0.0371027, 0.0183\n",
      "2017-11-10 15:21:14: Loss and accuracy at step 1462: 0.037069, 0.0076\n",
      "2017-11-10 15:21:16: Loss and accuracy at step 1463: 0.0370371, 0.0173\n",
      "2017-11-10 15:21:18: Loss and accuracy at step 1464: 0.0370257, 0.0119\n",
      "2017-11-10 15:21:20: Loss and accuracy at step 1465: 0.0370157, 0.0115\n",
      "2017-11-10 15:21:22: Loss and accuracy at step 1466: 0.0370101, 0.0165\n",
      "2017-11-10 15:21:24: Loss and accuracy at step 1467: 0.0369123, 0.0107\n",
      "2017-11-10 15:21:26: Loss and accuracy at step 1468: 0.0370272, 0.0188\n",
      "2017-11-10 15:21:28: Loss and accuracy at step 1469: 0.0369974, 0.0096\n",
      "2017-11-10 15:21:30: Loss and accuracy at step 1470: 0.0369291, 0.0131\n",
      "2017-11-10 15:21:32: Loss and accuracy at step 1471: 0.0369682, 0.0213\n",
      "2017-11-10 15:21:35: Loss and accuracy at step 1472: 0.0369887, 0.0067\n",
      "2017-11-10 15:21:37: Loss and accuracy at step 1473: 0.0370067, 0.0241\n",
      "2017-11-10 15:21:39: Loss and accuracy at step 1474: 0.0369895, 0.0095\n",
      "2017-11-10 15:21:41: Loss and accuracy at step 1475: 0.0369365, 0.0142\n",
      "2017-11-10 15:21:43: Loss and accuracy at step 1476: 0.0370458, 0.0161\n",
      "2017-11-10 15:21:45: Loss and accuracy at step 1477: 0.0370472, 0.0087\n",
      "2017-11-10 15:21:47: Loss and accuracy at step 1478: 0.0370253, 0.0129\n",
      "2017-11-10 15:21:49: Loss and accuracy at step 1479: 0.037007, 0.0174\n",
      "2017-11-10 15:21:51: Loss and accuracy at step 1480: 0.036947, 0.0103\n",
      "2017-11-10 15:21:53: Loss and accuracy at step 1481: 0.0370012, 0.0093\n",
      "2017-11-10 15:21:55: Loss and accuracy at step 1482: 0.0369254, 0.0209\n",
      "2017-11-10 15:21:58: Loss and accuracy at step 1483: 0.0369943, 0.0118\n",
      "2017-11-10 15:22:00: Loss and accuracy at step 1484: 0.0369817, 0.0113\n",
      "2017-11-10 15:22:02: Loss and accuracy at step 1485: 0.0369866, 0.0185\n",
      "2017-11-10 15:22:04: Loss and accuracy at step 1486: 0.0371011, 0.0102\n",
      "2017-11-10 15:22:06: Loss and accuracy at step 1487: 0.0370671, 0.0109\n",
      "2017-11-10 15:22:08: Loss and accuracy at step 1488: 0.0370309, 0.0158\n",
      "2017-11-10 15:22:10: Loss and accuracy at step 1489: 0.03693, 0.0086\n",
      "2017-11-10 15:22:12: Loss and accuracy at step 1490: 0.0370693, 0.0122\n",
      "2017-11-10 15:22:14: Loss and accuracy at step 1491: 0.0370139, 0.0113\n",
      "2017-11-10 15:22:17: Loss and accuracy at step 1492: 0.0370127, 0.0151\n",
      "2017-11-10 15:22:19: Loss and accuracy at step 1493: 0.0371001, 0.0113\n",
      "2017-11-10 15:22:21: Loss and accuracy at step 1494: 0.0369589, 0.008\n",
      "2017-11-10 15:22:23: Loss and accuracy at step 1495: 0.037001, 0.02\n",
      "2017-11-10 15:22:25: Loss and accuracy at step 1496: 0.0370128, 0.0096\n",
      "2017-11-10 15:22:27: Loss and accuracy at step 1497: 0.03699, 0.0105\n",
      "2017-11-10 15:22:29: Loss and accuracy at step 1498: 0.0369731, 0.0242\n",
      "2017-11-10 15:22:31: Loss and accuracy at step 1499: 0.0370457, 0.0074\n",
      "2017-11-10 15:22:33: Loss and accuracy at step 1500: 0.0370096, 0.0174\n",
      "2017-11-10 15:22:35: Loss and accuracy at step 1501: 0.0370492, 0.013\n",
      "2017-11-10 15:22:37: Loss and accuracy at step 1502: 0.0370647, 0.0131\n",
      "2017-11-10 15:22:39: Loss and accuracy at step 1503: 0.0370332, 0.0115\n",
      "2017-11-10 15:22:42: Loss and accuracy at step 1504: 0.0370036, 0.0102\n",
      "2017-11-10 15:22:44: Loss and accuracy at step 1505: 0.037128, 0.014\n",
      "2017-11-10 15:22:46: Loss and accuracy at step 1506: 0.03699, 0.012\n",
      "2017-11-10 15:22:48: Loss and accuracy at step 1507: 0.0370011, 0.0142\n",
      "2017-11-10 15:22:50: Loss and accuracy at step 1508: 0.0370853, 0.0123\n",
      "2017-11-10 15:22:52: Loss and accuracy at step 1509: 0.0370174, 0.0094\n",
      "2017-11-10 15:22:54: Loss and accuracy at step 1510: 0.0370164, 0.0118\n",
      "2017-11-10 15:22:56: Loss and accuracy at step 1511: 0.0370074, 0.0128\n",
      "2017-11-10 15:22:58: Loss and accuracy at step 1512: 0.0370601, 0.0102\n",
      "2017-11-10 15:23:00: Loss and accuracy at step 1513: 0.0370142, 0.0128\n",
      "2017-11-10 15:23:02: Loss and accuracy at step 1514: 0.0369583, 0.0096\n",
      "2017-11-10 15:23:05: Loss and accuracy at step 1515: 0.0370562, 0.0164\n",
      "2017-11-10 15:23:07: Loss and accuracy at step 1516: 0.0370081, 0.0099\n",
      "2017-11-10 15:23:09: Loss and accuracy at step 1517: 0.0370454, 0.0144\n",
      "2017-11-10 15:23:11: Loss and accuracy at step 1518: 0.0370628, 0.0212\n",
      "2017-11-10 15:23:13: Loss and accuracy at step 1519: 0.0370523, 0.0111\n",
      "2017-11-10 15:23:15: Loss and accuracy at step 1520: 0.0370906, 0.0105\n",
      "2017-11-10 15:23:17: Loss and accuracy at step 1521: 0.0370222, 0.0173\n",
      "2017-11-10 15:23:19: Loss and accuracy at step 1522: 0.0369766, 0.0093\n",
      "2017-11-10 15:23:21: Loss and accuracy at step 1523: 0.0369986, 0.0151\n",
      "2017-11-10 15:23:23: Loss and accuracy at step 1524: 0.0369735, 0.0134\n",
      "2017-11-10 15:23:25: Loss and accuracy at step 1525: 0.0369408, 0.0102\n",
      "2017-11-10 15:23:27: Loss and accuracy at step 1526: 0.0370646, 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:23:30: Loss and accuracy at step 1527: 0.0370218, 0.0112\n",
      "2017-11-10 15:23:32: Loss and accuracy at step 1528: 0.037066, 0.0148\n",
      "2017-11-10 15:23:34: Loss and accuracy at step 1529: 0.0370099, 0.0107\n",
      "2017-11-10 15:23:36: Loss and accuracy at step 1530: 0.0370576, 0.017\n",
      "2017-11-10 15:23:38: Loss and accuracy at step 1531: 0.0370582, 0.0127\n",
      "2017-11-10 15:23:40: Loss and accuracy at step 1532: 0.0370145, 0.0108\n",
      "2017-11-10 15:23:42: Loss and accuracy at step 1533: 0.0369562, 0.014\n",
      "2017-11-10 15:23:44: Loss and accuracy at step 1534: 0.0370123, 0.0159\n",
      "2017-11-10 15:23:46: Loss and accuracy at step 1535: 0.0369684, 0.0161\n",
      "2017-11-10 15:23:48: Loss and accuracy at step 1536: 0.0369844, 0.0108\n",
      "2017-11-10 15:23:50: Loss and accuracy at step 1537: 0.0371648, 0.0121\n",
      "2017-11-10 15:23:52: Loss and accuracy at step 1538: 0.0371036, 0.0174\n",
      "2017-11-10 15:23:55: Loss and accuracy at step 1539: 0.0370143, 0.0094\n",
      "2017-11-10 15:23:57: Loss and accuracy at step 1540: 0.037022, 0.0111\n",
      "2017-11-10 15:23:59: Loss and accuracy at step 1541: 0.0371909, 0.0213\n",
      "2017-11-10 15:24:01: Loss and accuracy at step 1542: 0.0370959, 0.007\n",
      "2017-11-10 15:24:03: Loss and accuracy at step 1543: 0.0370478, 0.015\n",
      "2017-11-10 15:24:05: Loss and accuracy at step 1544: 0.0369463, 0.013\n",
      "2017-11-10 15:24:07: Loss and accuracy at step 1545: 0.0370214, 0.0119\n",
      "2017-11-10 15:24:09: Loss and accuracy at step 1546: 0.0369784, 0.0127\n",
      "2017-11-10 15:24:11: Loss and accuracy at step 1547: 0.036889, 0.0103\n",
      "2017-11-10 15:24:14: Loss and accuracy at step 1548: 0.0369965, 0.0148\n",
      "2017-11-10 15:24:16: Loss and accuracy at step 1549: 0.0369552, 0.0157\n",
      "2017-11-10 15:24:18: Loss and accuracy at step 1550: 0.0371009, 0.0108\n",
      "2017-11-10 15:24:20: Loss and accuracy at step 1551: 0.0369999, 0.0122\n",
      "2017-11-10 15:24:22: Loss and accuracy at step 1552: 0.0370416, 0.0146\n",
      "2017-11-10 15:24:24: Loss and accuracy at step 1553: 0.0370449, 0.0094\n",
      "2017-11-10 15:24:26: Loss and accuracy at step 1554: 0.0370276, 0.0129\n",
      "2017-11-10 15:24:28: Loss and accuracy at step 1555: 0.0369824, 0.0077\n",
      "2017-11-10 15:24:30: Loss and accuracy at step 1556: 0.037129, 0.0104\n",
      "2017-11-10 15:24:32: Loss and accuracy at step 1557: 0.0370086, 0.0177\n",
      "2017-11-10 15:24:34: Loss and accuracy at step 1558: 0.0369635, 0.0089\n",
      "2017-11-10 15:24:36: Loss and accuracy at step 1559: 0.0369878, 0.01\n",
      "2017-11-10 15:24:39: Loss and accuracy at step 1560: 0.0370448, 0.0231\n",
      "2017-11-10 15:24:41: Loss and accuracy at step 1561: 0.0370239, 0.0099\n",
      "2017-11-10 15:24:43: Loss and accuracy at step 1562: 0.0369747, 0.0114\n",
      "2017-11-10 15:24:45: Loss and accuracy at step 1563: 0.0369687, 0.0164\n",
      "2017-11-10 15:24:47: Loss and accuracy at step 1564: 0.0369645, 0.0102\n",
      "2017-11-10 15:24:49: Loss and accuracy at step 1565: 0.0369213, 0.0148\n",
      "2017-11-10 15:24:51: Loss and accuracy at step 1566: 0.036972, 0.0144\n",
      "2017-11-10 15:24:53: Loss and accuracy at step 1567: 0.0369585, 0.0088\n",
      "2017-11-10 15:24:55: Loss and accuracy at step 1568: 0.0370122, 0.013\n",
      "2017-11-10 15:24:57: Loss and accuracy at step 1569: 0.0370516, 0.0153\n",
      "2017-11-10 15:24:59: Loss and accuracy at step 1570: 0.0370923, 0.0103\n",
      "2017-11-10 15:25:02: Loss and accuracy at step 1571: 0.0369616, 0.0122\n",
      "2017-11-10 15:25:04: Loss and accuracy at step 1572: 0.036966, 0.0136\n",
      "2017-11-10 15:25:06: Loss and accuracy at step 1573: 0.0369706, 0.0098\n",
      "2017-11-10 15:25:08: Loss and accuracy at step 1574: 0.0370418, 0.0132\n",
      "2017-11-10 15:25:10: Loss and accuracy at step 1575: 0.037009, 0.0093\n",
      "2017-11-10 15:25:12: Loss and accuracy at step 1576: 0.0369924, 0.0165\n",
      "2017-11-10 15:25:14: Loss and accuracy at step 1577: 0.0370371, 0.0104\n",
      "2017-11-10 15:25:16: Loss and accuracy at step 1578: 0.0369815, 0.0098\n",
      "2017-11-10 15:25:18: Loss and accuracy at step 1579: 0.0369364, 0.0173\n",
      "2017-11-10 15:25:20: Loss and accuracy at step 1580: 0.0370226, 0.01\n",
      "2017-11-10 15:25:22: Loss and accuracy at step 1581: 0.037051, 0.0105\n",
      "2017-11-10 15:25:25: Loss and accuracy at step 1582: 0.0370561, 0.0169\n",
      "2017-11-10 15:25:27: Loss and accuracy at step 1583: 0.037046, 0.0114\n",
      "2017-11-10 15:25:29: Loss and accuracy at step 1584: 0.0369813, 0.0111\n",
      "2017-11-10 15:25:31: Loss and accuracy at step 1585: 0.0369914, 0.0237\n",
      "2017-11-10 15:25:33: Loss and accuracy at step 1586: 0.0369501, 0.0087\n",
      "2017-11-10 15:25:35: Loss and accuracy at step 1587: 0.0369435, 0.0103\n",
      "2017-11-10 15:25:37: Loss and accuracy at step 1588: 0.0369834, 0.0286\n",
      "2017-11-10 15:25:39: Loss and accuracy at step 1589: 0.0369749, 0.0117\n",
      "2017-11-10 15:25:41: Loss and accuracy at step 1590: 0.0370195, 0.0095\n",
      "2017-11-10 15:25:43: Loss and accuracy at step 1591: 0.03701, 0.0341\n",
      "2017-11-10 15:25:45: Loss and accuracy at step 1592: 0.0370502, 0.01\n",
      "2017-11-10 15:25:48: Loss and accuracy at step 1593: 0.0369746, 0.0132\n",
      "2017-11-10 15:25:50: Loss and accuracy at step 1594: 0.0369369, 0.0148\n",
      "2017-11-10 15:25:52: Loss and accuracy at step 1595: 0.0370628, 0.0087\n",
      "2017-11-10 15:25:54: Loss and accuracy at step 1596: 0.0370441, 0.0229\n",
      "2017-11-10 15:25:56: Loss and accuracy at step 1597: 0.037019, 0.0067\n",
      "2017-11-10 15:25:58: Loss and accuracy at step 1598: 0.0370756, 0.0104\n",
      "2017-11-10 15:26:00: Loss and accuracy at step 1599: 0.0369989, 0.02\n",
      "2017-11-10 15:26:02: Loss and accuracy at step 1600: 0.0370476, 0.0052\n",
      "2017-11-10 15:26:04: Loss and accuracy at step 1601: 0.0369852, 0.0128\n",
      "2017-11-10 15:26:06: Loss and accuracy at step 1602: 0.0370537, 0.0165\n",
      "2017-11-10 15:26:08: Loss and accuracy at step 1603: 0.0370012, 0.0077\n",
      "2017-11-10 15:26:11: Loss and accuracy at step 1604: 0.0369111, 0.0123\n",
      "2017-11-10 15:26:13: Loss and accuracy at step 1605: 0.0368763, 0.0162\n",
      "2017-11-10 15:26:15: Loss and accuracy at step 1606: 0.0369796, 0.0097\n",
      "2017-11-10 15:26:17: Loss and accuracy at step 1607: 0.0369977, 0.0126\n",
      "2017-11-10 15:26:19: Loss and accuracy at step 1608: 0.0369114, 0.0138\n",
      "2017-11-10 15:26:21: Loss and accuracy at step 1609: 0.0369832, 0.0143\n",
      "2017-11-10 15:26:23: Loss and accuracy at step 1610: 0.0369933, 0.0107\n",
      "2017-11-10 15:26:25: Loss and accuracy at step 1611: 0.0369834, 0.0159\n",
      "2017-11-10 15:26:27: Loss and accuracy at step 1612: 0.0369738, 0.0178\n",
      "2017-11-10 15:26:29: Loss and accuracy at step 1613: 0.0369579, 0.0099\n",
      "2017-11-10 15:26:32: Loss and accuracy at step 1614: 0.036945, 0.0161\n",
      "2017-11-10 15:26:34: Loss and accuracy at step 1615: 0.0370244, 0.0222\n",
      "2017-11-10 15:26:36: Loss and accuracy at step 1616: 0.037006, 0.0101\n",
      "2017-11-10 15:26:38: Loss and accuracy at step 1617: 0.0370073, 0.0115\n",
      "2017-11-10 15:26:40: Loss and accuracy at step 1618: 0.0369838, 0.0339\n",
      "2017-11-10 15:26:42: Loss and accuracy at step 1619: 0.036972, 0.0061\n",
      "2017-11-10 15:26:44: Loss and accuracy at step 1620: 0.0369646, 0.0153\n",
      "2017-11-10 15:26:46: Loss and accuracy at step 1621: 0.0370307, 0.0226\n",
      "2017-11-10 15:26:48: Loss and accuracy at step 1622: 0.0370199, 0.008\n",
      "2017-11-10 15:26:50: Loss and accuracy at step 1623: 0.0369903, 0.0123\n",
      "2017-11-10 15:26:52: Loss and accuracy at step 1624: 0.0369759, 0.013\n",
      "2017-11-10 15:26:55: Loss and accuracy at step 1625: 0.0369724, 0.0131\n",
      "2017-11-10 15:26:57: Loss and accuracy at step 1626: 0.0370159, 0.0085\n",
      "2017-11-10 15:26:59: Loss and accuracy at step 1627: 0.0368964, 0.0136\n",
      "2017-11-10 15:27:01: Loss and accuracy at step 1628: 0.0368817, 0.0197\n",
      "2017-11-10 15:27:03: Loss and accuracy at step 1629: 0.0369854, 0.0094\n",
      "2017-11-10 15:27:05: Loss and accuracy at step 1630: 0.0369981, 0.0123\n",
      "2017-11-10 15:27:07: Loss and accuracy at step 1631: 0.0369391, 0.0135\n",
      "2017-11-10 15:27:09: Loss and accuracy at step 1632: 0.0369818, 0.012\n",
      "2017-11-10 15:27:11: Loss and accuracy at step 1633: 0.0370284, 0.0097\n",
      "2017-11-10 15:27:13: Loss and accuracy at step 1634: 0.0370829, 0.013\n",
      "2017-11-10 15:27:15: Loss and accuracy at step 1635: 0.0369433, 0.0099\n",
      "2017-11-10 15:27:18: Loss and accuracy at step 1636: 0.0370752, 0.0109\n",
      "2017-11-10 15:27:20: Loss and accuracy at step 1637: 0.0370009, 0.0139\n",
      "2017-11-10 15:27:22: Loss and accuracy at step 1638: 0.0370016, 0.0107\n",
      "2017-11-10 15:27:24: Loss and accuracy at step 1639: 0.0370116, 0.012\n",
      "2017-11-10 15:27:26: Loss and accuracy at step 1640: 0.0370236, 0.0127\n",
      "2017-11-10 15:27:28: Loss and accuracy at step 1641: 0.0370797, 0.012\n",
      "2017-11-10 15:27:30: Loss and accuracy at step 1642: 0.0370083, 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:27:32: Loss and accuracy at step 1643: 0.0370824, 0.01\n",
      "2017-11-10 15:27:34: Loss and accuracy at step 1644: 0.036973, 0.0194\n",
      "2017-11-10 15:27:36: Loss and accuracy at step 1645: 0.0370019, 0.0071\n",
      "2017-11-10 15:27:38: Loss and accuracy at step 1646: 0.036975, 0.0127\n",
      "2017-11-10 15:27:41: Loss and accuracy at step 1647: 0.0370523, 0.0178\n",
      "2017-11-10 15:27:43: Loss and accuracy at step 1648: 0.0370875, 0.0089\n",
      "2017-11-10 15:27:45: Loss and accuracy at step 1649: 0.0369893, 0.013\n",
      "2017-11-10 15:27:47: Loss and accuracy at step 1650: 0.0369922, 0.0104\n",
      "2017-11-10 15:27:49: Loss and accuracy at step 1651: 0.0370321, 0.0156\n",
      "2017-11-10 15:27:51: Loss and accuracy at step 1652: 0.0369639, 0.008\n",
      "2017-11-10 15:27:53: Loss and accuracy at step 1653: 0.0370598, 0.0141\n",
      "2017-11-10 15:27:55: Loss and accuracy at step 1654: 0.037044, 0.013\n",
      "2017-11-10 15:27:57: Loss and accuracy at step 1655: 0.0369703, 0.0106\n",
      "2017-11-10 15:27:59: Loss and accuracy at step 1656: 0.0369395, 0.016\n",
      "2017-11-10 15:28:01: Loss and accuracy at step 1657: 0.0370097, 0.0104\n",
      "2017-11-10 15:28:03: Loss and accuracy at step 1658: 0.0369944, 0.0114\n",
      "2017-11-10 15:28:06: Loss and accuracy at step 1659: 0.0370027, 0.0148\n",
      "2017-11-10 15:28:08: Loss and accuracy at step 1660: 0.0369718, 0.0112\n",
      "2017-11-10 15:28:10: Loss and accuracy at step 1661: 0.0369483, 0.012\n",
      "2017-11-10 15:28:12: Loss and accuracy at step 1662: 0.0370598, 0.0163\n",
      "2017-11-10 15:28:14: Loss and accuracy at step 1663: 0.0370367, 0.0144\n",
      "2017-11-10 15:28:16: Loss and accuracy at step 1664: 0.0370258, 0.0101\n",
      "2017-11-10 15:28:18: Loss and accuracy at step 1665: 0.0369686, 0.0182\n",
      "2017-11-10 15:28:20: Loss and accuracy at step 1666: 0.0370106, 0.0102\n",
      "2017-11-10 15:28:22: Loss and accuracy at step 1667: 0.0369614, 0.013\n",
      "2017-11-10 15:28:24: Loss and accuracy at step 1668: 0.036912, 0.0127\n",
      "2017-11-10 15:28:27: Loss and accuracy at step 1669: 0.0369632, 0.0124\n",
      "2017-11-10 15:28:29: Loss and accuracy at step 1670: 0.0369927, 0.0175\n",
      "2017-11-10 15:28:31: Loss and accuracy at step 1671: 0.0370072, 0.007\n",
      "2017-11-10 15:28:33: Loss and accuracy at step 1672: 0.0369482, 0.0101\n",
      "2017-11-10 15:28:35: Loss and accuracy at step 1673: 0.0370202, 0.0134\n",
      "2017-11-10 15:28:37: Loss and accuracy at step 1674: 0.0369874, 0.0088\n",
      "2017-11-10 15:28:39: Loss and accuracy at step 1675: 0.0368959, 0.0102\n",
      "2017-11-10 15:28:41: Loss and accuracy at step 1676: 0.0369728, 0.0122\n",
      "2017-11-10 15:28:43: Loss and accuracy at step 1677: 0.0370408, 0.013\n",
      "2017-11-10 15:28:45: Loss and accuracy at step 1678: 0.0369543, 0.0112\n",
      "2017-11-10 15:28:47: Loss and accuracy at step 1679: 0.0369455, 0.0143\n",
      "2017-11-10 15:28:50: Loss and accuracy at step 1680: 0.0370224, 0.0117\n",
      "2017-11-10 15:28:52: Loss and accuracy at step 1681: 0.0369126, 0.0097\n",
      "2017-11-10 15:28:54: Loss and accuracy at step 1682: 0.0368862, 0.0116\n",
      "2017-11-10 15:28:56: Loss and accuracy at step 1683: 0.0369157, 0.0137\n",
      "2017-11-10 15:28:58: Loss and accuracy at step 1684: 0.0369502, 0.0093\n",
      "2017-11-10 15:29:00: Loss and accuracy at step 1685: 0.0369759, 0.0127\n",
      "2017-11-10 15:29:02: Loss and accuracy at step 1686: 0.0369636, 0.0163\n",
      "2017-11-10 15:29:04: Loss and accuracy at step 1687: 0.0369971, 0.0079\n",
      "2017-11-10 15:29:06: Loss and accuracy at step 1688: 0.0370199, 0.0101\n",
      "2017-11-10 15:29:08: Loss and accuracy at step 1689: 0.0370512, 0.0095\n",
      "2017-11-10 15:29:10: Loss and accuracy at step 1690: 0.0370205, 0.0112\n",
      "2017-11-10 15:29:13: Loss and accuracy at step 1691: 0.0368987, 0.0124\n",
      "2017-11-10 15:29:15: Loss and accuracy at step 1692: 0.0370066, 0.0068\n",
      "2017-11-10 15:29:17: Loss and accuracy at step 1693: 0.0369772, 0.0181\n",
      "2017-11-10 15:29:19: Loss and accuracy at step 1694: 0.0368746, 0.0114\n",
      "2017-11-10 15:29:21: Loss and accuracy at step 1695: 0.0369206, 0.0088\n",
      "2017-11-10 15:29:23: Loss and accuracy at step 1696: 0.0369106, 0.0168\n",
      "2017-11-10 15:29:25: Loss and accuracy at step 1697: 0.0368871, 0.0085\n",
      "2017-11-10 15:29:27: Loss and accuracy at step 1698: 0.0369639, 0.01\n",
      "2017-11-10 15:29:29: Loss and accuracy at step 1699: 0.0368892, 0.0104\n",
      "2017-11-10 15:29:31: Loss and accuracy at step 1700: 0.0369605, 0.0102\n",
      "2017-11-10 15:29:34: Loss and accuracy at step 1701: 0.0369888, 0.0164\n",
      "2017-11-10 15:29:36: Loss and accuracy at step 1702: 0.0369802, 0.0071\n",
      "2017-11-10 15:29:38: Loss and accuracy at step 1703: 0.0370066, 0.0146\n",
      "2017-11-10 15:29:40: Loss and accuracy at step 1704: 0.0370195, 0.0133\n",
      "2017-11-10 15:29:42: Loss and accuracy at step 1705: 0.0369492, 0.0096\n",
      "2017-11-10 15:29:44: Loss and accuracy at step 1706: 0.0369461, 0.0164\n",
      "2017-11-10 15:29:46: Loss and accuracy at step 1707: 0.0370164, 0.0065\n",
      "2017-11-10 15:29:48: Loss and accuracy at step 1708: 0.0369006, 0.0225\n",
      "2017-11-10 15:29:50: Loss and accuracy at step 1709: 0.0369706, 0.0108\n",
      "2017-11-10 15:29:52: Loss and accuracy at step 1710: 0.0369786, 0.0098\n",
      "2017-11-10 15:29:54: Loss and accuracy at step 1711: 0.0369898, 0.0161\n",
      "2017-11-10 15:29:56: Loss and accuracy at step 1712: 0.0369577, 0.0114\n",
      "2017-11-10 15:29:59: Loss and accuracy at step 1713: 0.0369488, 0.0133\n",
      "2017-11-10 15:30:01: Loss and accuracy at step 1714: 0.0370088, 0.0086\n",
      "2017-11-10 15:30:03: Loss and accuracy at step 1715: 0.0369446, 0.0148\n",
      "2017-11-10 15:30:05: Loss and accuracy at step 1716: 0.0370032, 0.0148\n",
      "2017-11-10 15:30:07: Loss and accuracy at step 1717: 0.0369417, 0.009\n",
      "2017-11-10 15:30:09: Loss and accuracy at step 1718: 0.0369735, 0.0176\n",
      "2017-11-10 15:30:11: Loss and accuracy at step 1719: 0.036985, 0.0112\n",
      "2017-11-10 15:30:13: Loss and accuracy at step 1720: 0.03697, 0.0119\n",
      "2017-11-10 15:30:15: Loss and accuracy at step 1721: 0.036949, 0.0117\n",
      "2017-11-10 15:30:17: Loss and accuracy at step 1722: 0.0370621, 0.0106\n",
      "2017-11-10 15:30:20: Loss and accuracy at step 1723: 0.0369912, 0.0121\n",
      "2017-11-10 15:30:22: Loss and accuracy at step 1724: 0.0370101, 0.0145\n",
      "2017-11-10 15:30:24: Loss and accuracy at step 1725: 0.036925, 0.0125\n",
      "2017-11-10 15:30:26: Loss and accuracy at step 1726: 0.0369659, 0.0128\n",
      "2017-11-10 15:30:28: Loss and accuracy at step 1727: 0.0369968, 0.0135\n",
      "2017-11-10 15:30:30: Loss and accuracy at step 1728: 0.0370296, 0.0115\n",
      "2017-11-10 15:30:32: Loss and accuracy at step 1729: 0.0368471, 0.0089\n",
      "2017-11-10 15:30:34: Loss and accuracy at step 1730: 0.0369147, 0.0163\n",
      "2017-11-10 15:30:36: Loss and accuracy at step 1731: 0.0370413, 0.0069\n",
      "2017-11-10 15:30:38: Loss and accuracy at step 1732: 0.0370147, 0.0152\n",
      "2017-11-10 15:30:41: Loss and accuracy at step 1733: 0.0369734, 0.0135\n",
      "2017-11-10 15:30:43: Loss and accuracy at step 1734: 0.037022, 0.011\n",
      "2017-11-10 15:30:45: Loss and accuracy at step 1735: 0.0369093, 0.0126\n",
      "2017-11-10 15:30:47: Loss and accuracy at step 1736: 0.0369205, 0.0119\n",
      "2017-11-10 15:30:49: Loss and accuracy at step 1737: 0.0369893, 0.0126\n",
      "2017-11-10 15:30:51: Loss and accuracy at step 1738: 0.0370083, 0.0088\n",
      "2017-11-10 15:30:53: Loss and accuracy at step 1739: 0.0369577, 0.0111\n",
      "2017-11-10 15:30:55: Loss and accuracy at step 1740: 0.0371115, 0.015\n",
      "2017-11-10 15:30:57: Loss and accuracy at step 1741: 0.0369562, 0.0118\n",
      "2017-11-10 15:30:59: Loss and accuracy at step 1742: 0.0369505, 0.0115\n",
      "2017-11-10 15:31:01: Loss and accuracy at step 1743: 0.0370049, 0.0149\n",
      "2017-11-10 15:31:04: Loss and accuracy at step 1744: 0.0369423, 0.0106\n",
      "2017-11-10 15:31:06: Loss and accuracy at step 1745: 0.0370614, 0.0096\n",
      "2017-11-10 15:31:08: Loss and accuracy at step 1746: 0.036994, 0.0162\n",
      "2017-11-10 15:31:10: Loss and accuracy at step 1747: 0.0369228, 0.0123\n",
      "2017-11-10 15:31:12: Loss and accuracy at step 1748: 0.0370592, 0.0105\n",
      "2017-11-10 15:31:14: Loss and accuracy at step 1749: 0.0369419, 0.0138\n",
      "2017-11-10 15:31:16: Loss and accuracy at step 1750: 0.0370318, 0.0118\n",
      "2017-11-10 15:31:18: Loss and accuracy at step 1751: 0.0370236, 0.0088\n",
      "2017-11-10 15:31:20: Loss and accuracy at step 1752: 0.0370746, 0.0129\n",
      "2017-11-10 15:31:22: Loss and accuracy at step 1753: 0.0370216, 0.0133\n",
      "2017-11-10 15:31:24: Loss and accuracy at step 1754: 0.0369942, 0.0105\n",
      "2017-11-10 15:31:26: Loss and accuracy at step 1755: 0.0369903, 0.0103\n",
      "2017-11-10 15:31:29: Loss and accuracy at step 1756: 0.0369011, 0.014\n",
      "2017-11-10 15:31:31: Loss and accuracy at step 1757: 0.036977, 0.0107\n",
      "2017-11-10 15:31:33: Loss and accuracy at step 1758: 0.036963, 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:31:35: Loss and accuracy at step 1759: 0.0369577, 0.0128\n",
      "2017-11-10 15:31:37: Loss and accuracy at step 1760: 0.0369708, 0.01\n",
      "2017-11-10 15:31:39: Loss and accuracy at step 1761: 0.0370946, 0.0153\n",
      "2017-11-10 15:31:41: Loss and accuracy at step 1762: 0.0369775, 0.0107\n",
      "2017-11-10 15:31:43: Loss and accuracy at step 1763: 0.0370656, 0.0083\n",
      "2017-11-10 15:31:45: Loss and accuracy at step 1764: 0.0369395, 0.0132\n",
      "2017-11-10 15:31:47: Loss and accuracy at step 1765: 0.0369826, 0.009\n",
      "2017-11-10 15:31:49: Loss and accuracy at step 1766: 0.036923, 0.014\n",
      "2017-11-10 15:31:51: Loss and accuracy at step 1767: 0.0368927, 0.0102\n",
      "2017-11-10 15:31:54: Loss and accuracy at step 1768: 0.0369758, 0.0127\n",
      "2017-11-10 15:31:56: Loss and accuracy at step 1769: 0.0370717, 0.0125\n",
      "2017-11-10 15:31:58: Loss and accuracy at step 1770: 0.036959, 0.0092\n",
      "2017-11-10 15:32:00: Loss and accuracy at step 1771: 0.0370493, 0.0099\n",
      "2017-11-10 15:32:02: Loss and accuracy at step 1772: 0.0370256, 0.0162\n",
      "2017-11-10 15:32:04: Loss and accuracy at step 1773: 0.0370211, 0.0106\n",
      "2017-11-10 15:32:06: Loss and accuracy at step 1774: 0.0369824, 0.0097\n",
      "2017-11-10 15:32:08: Loss and accuracy at step 1775: 0.0369603, 0.0176\n",
      "2017-11-10 15:32:10: Loss and accuracy at step 1776: 0.0368824, 0.0111\n",
      "2017-11-10 15:32:12: Loss and accuracy at step 1777: 0.0370126, 0.0108\n",
      "2017-11-10 15:32:15: Loss and accuracy at step 1778: 0.0368853, 0.0138\n",
      "2017-11-10 15:32:17: Loss and accuracy at step 1779: 0.0368851, 0.0109\n",
      "2017-11-10 15:32:19: Loss and accuracy at step 1780: 0.0368421, 0.0106\n",
      "2017-11-10 15:32:21: Loss and accuracy at step 1781: 0.036939, 0.0122\n",
      "2017-11-10 15:32:23: Loss and accuracy at step 1782: 0.0369366, 0.0092\n",
      "2017-11-10 15:32:25: Loss and accuracy at step 1783: 0.0369715, 0.0172\n",
      "2017-11-10 15:32:27: Loss and accuracy at step 1784: 0.0368488, 0.0111\n",
      "2017-11-10 15:32:29: Loss and accuracy at step 1785: 0.0369897, 0.0106\n",
      "2017-11-10 15:32:31: Loss and accuracy at step 1786: 0.0369179, 0.0149\n",
      "2017-11-10 15:32:33: Loss and accuracy at step 1787: 0.0369566, 0.0075\n",
      "2017-11-10 15:32:36: Loss and accuracy at step 1788: 0.0369855, 0.0182\n",
      "2017-11-10 15:32:38: Loss and accuracy at step 1789: 0.0369495, 0.0108\n",
      "2017-11-10 15:32:40: Loss and accuracy at step 1790: 0.0369722, 0.0065\n",
      "2017-11-10 15:32:42: Loss and accuracy at step 1791: 0.0369824, 0.0207\n",
      "2017-11-10 15:32:44: Loss and accuracy at step 1792: 0.0370284, 0.0075\n",
      "2017-11-10 15:32:46: Loss and accuracy at step 1793: 0.0368818, 0.0108\n",
      "2017-11-10 15:32:48: Loss and accuracy at step 1794: 0.0369512, 0.0164\n",
      "2017-11-10 15:32:50: Loss and accuracy at step 1795: 0.0369218, 0.011\n",
      "2017-11-10 15:32:52: Loss and accuracy at step 1796: 0.0370215, 0.0155\n",
      "2017-11-10 15:32:54: Loss and accuracy at step 1797: 0.0369214, 0.0115\n",
      "2017-11-10 15:32:56: Loss and accuracy at step 1798: 0.0369563, 0.0132\n",
      "2017-11-10 15:32:59: Loss and accuracy at step 1799: 0.0369158, 0.013\n",
      "2017-11-10 15:33:01: Loss and accuracy at step 1800: 0.0369935, 0.0127\n",
      "2017-11-10 15:33:03: Loss and accuracy at step 1801: 0.0369114, 0.0147\n",
      "2017-11-10 15:33:05: Loss and accuracy at step 1802: 0.0369174, 0.0111\n",
      "2017-11-10 15:33:07: Loss and accuracy at step 1803: 0.0369295, 0.0127\n",
      "2017-11-10 15:33:09: Loss and accuracy at step 1804: 0.0369429, 0.0163\n",
      "2017-11-10 15:33:11: Loss and accuracy at step 1805: 0.0369694, 0.0101\n",
      "2017-11-10 15:33:13: Loss and accuracy at step 1806: 0.0370043, 0.0126\n",
      "2017-11-10 15:33:15: Loss and accuracy at step 1807: 0.0369953, 0.0107\n",
      "2017-11-10 15:33:17: Loss and accuracy at step 1808: 0.0369976, 0.0154\n",
      "2017-11-10 15:33:19: Loss and accuracy at step 1809: 0.0369735, 0.0097\n",
      "2017-11-10 15:33:22: Loss and accuracy at step 1810: 0.0370056, 0.0153\n",
      "2017-11-10 15:33:24: Loss and accuracy at step 1811: 0.0369859, 0.0132\n",
      "2017-11-10 15:33:26: Loss and accuracy at step 1812: 0.0370422, 0.0077\n",
      "2017-11-10 15:33:28: Loss and accuracy at step 1813: 0.0370042, 0.0192\n",
      "2017-11-10 15:33:30: Loss and accuracy at step 1814: 0.0369856, 0.0122\n",
      "2017-11-10 15:33:32: Loss and accuracy at step 1815: 0.0368992, 0.0099\n",
      "2017-11-10 15:33:34: Loss and accuracy at step 1816: 0.0370347, 0.0213\n",
      "2017-11-10 15:33:36: Loss and accuracy at step 1817: 0.0369074, 0.0078\n",
      "2017-11-10 15:33:38: Loss and accuracy at step 1818: 0.0370005, 0.0164\n",
      "2017-11-10 15:33:40: Loss and accuracy at step 1819: 0.0369804, 0.0118\n",
      "2017-11-10 15:33:42: Loss and accuracy at step 1820: 0.0369787, 0.0093\n",
      "2017-11-10 15:33:44: Loss and accuracy at step 1821: 0.0368714, 0.017\n",
      "2017-11-10 15:33:47: Loss and accuracy at step 1822: 0.0369522, 0.0122\n",
      "2017-11-10 15:33:49: Loss and accuracy at step 1823: 0.0369791, 0.0123\n",
      "2017-11-10 15:33:51: Loss and accuracy at step 1824: 0.0370004, 0.0108\n",
      "2017-11-10 15:33:53: Loss and accuracy at step 1825: 0.0369278, 0.0091\n",
      "2017-11-10 15:33:55: Loss and accuracy at step 1826: 0.0369274, 0.0196\n",
      "2017-11-10 15:33:57: Loss and accuracy at step 1827: 0.0369773, 0.0071\n",
      "2017-11-10 15:33:59: Loss and accuracy at step 1828: 0.0368722, 0.0122\n",
      "2017-11-10 15:34:01: Loss and accuracy at step 1829: 0.0369594, 0.0136\n",
      "2017-11-10 15:34:03: Loss and accuracy at step 1830: 0.0369096, 0.0072\n",
      "2017-11-10 15:34:05: Loss and accuracy at step 1831: 0.0368887, 0.0172\n",
      "2017-11-10 15:34:07: Loss and accuracy at step 1832: 0.0369437, 0.0079\n",
      "2017-11-10 15:34:10: Loss and accuracy at step 1833: 0.0369832, 0.0099\n",
      "2017-11-10 15:34:12: Loss and accuracy at step 1834: 0.0369795, 0.0169\n",
      "2017-11-10 15:34:14: Loss and accuracy at step 1835: 0.0369598, 0.0073\n",
      "2017-11-10 15:34:16: Loss and accuracy at step 1836: 0.0370362, 0.0156\n",
      "2017-11-10 15:34:18: Loss and accuracy at step 1837: 0.0370291, 0.0104\n",
      "2017-11-10 15:34:20: Loss and accuracy at step 1838: 0.0369206, 0.0117\n",
      "2017-11-10 15:34:22: Loss and accuracy at step 1839: 0.0369606, 0.0127\n",
      "2017-11-10 15:34:24: Loss and accuracy at step 1840: 0.036971, 0.0106\n",
      "2017-11-10 15:34:26: Loss and accuracy at step 1841: 0.0369037, 0.0186\n",
      "2017-11-10 15:34:29: Loss and accuracy at step 1842: 0.0370459, 0.0082\n",
      "2017-11-10 15:34:31: Loss and accuracy at step 1843: 0.0369748, 0.0139\n",
      "2017-11-10 15:34:33: Loss and accuracy at step 1844: 0.036971, 0.0131\n",
      "2017-11-10 15:34:35: Loss and accuracy at step 1845: 0.0369834, 0.0076\n",
      "2017-11-10 15:34:37: Loss and accuracy at step 1846: 0.0369516, 0.0114\n",
      "2017-11-10 15:34:39: Loss and accuracy at step 1847: 0.0369657, 0.0134\n",
      "2017-11-10 15:34:41: Loss and accuracy at step 1848: 0.0370151, 0.0119\n",
      "2017-11-10 15:34:43: Loss and accuracy at step 1849: 0.0370167, 0.0065\n",
      "2017-11-10 15:34:45: Loss and accuracy at step 1850: 0.0369477, 0.0158\n",
      "2017-11-10 15:34:47: Loss and accuracy at step 1851: 0.0370437, 0.0088\n",
      "2017-11-10 15:34:49: Loss and accuracy at step 1852: 0.0369851, 0.0101\n",
      "2017-11-10 15:34:52: Loss and accuracy at step 1853: 0.0369448, 0.0101\n",
      "2017-11-10 15:34:54: Loss and accuracy at step 1854: 0.0369363, 0.0131\n",
      "2017-11-10 15:34:56: Loss and accuracy at step 1855: 0.0369012, 0.0122\n",
      "2017-11-10 15:34:58: Loss and accuracy at step 1856: 0.0369307, 0.0082\n",
      "2017-11-10 15:35:00: Loss and accuracy at step 1857: 0.0369557, 0.0145\n",
      "2017-11-10 15:35:02: Loss and accuracy at step 1858: 0.036949, 0.011\n",
      "2017-11-10 15:35:04: Loss and accuracy at step 1859: 0.0369327, 0.01\n",
      "2017-11-10 15:35:06: Loss and accuracy at step 1860: 0.0369669, 0.0156\n",
      "2017-11-10 15:35:08: Loss and accuracy at step 1861: 0.0369963, 0.0092\n",
      "2017-11-10 15:35:10: Loss and accuracy at step 1862: 0.0369141, 0.0151\n",
      "2017-11-10 15:35:12: Loss and accuracy at step 1863: 0.0369712, 0.0142\n",
      "2017-11-10 15:35:14: Loss and accuracy at step 1864: 0.0369577, 0.01\n",
      "2017-11-10 15:35:17: Loss and accuracy at step 1865: 0.0369821, 0.0182\n",
      "2017-11-10 15:35:19: Loss and accuracy at step 1866: 0.036955, 0.0138\n",
      "2017-11-10 15:35:21: Loss and accuracy at step 1867: 0.0369673, 0.009\n",
      "2017-11-10 15:35:23: Loss and accuracy at step 1868: 0.0368976, 0.0211\n",
      "2017-11-10 15:35:25: Loss and accuracy at step 1869: 0.0369343, 0.0117\n",
      "2017-11-10 15:35:27: Loss and accuracy at step 1870: 0.0369597, 0.0096\n",
      "2017-11-10 15:35:29: Loss and accuracy at step 1871: 0.037037, 0.0169\n",
      "2017-11-10 15:35:31: Loss and accuracy at step 1872: 0.0369926, 0.0116\n",
      "2017-11-10 15:35:33: Loss and accuracy at step 1873: 0.0369068, 0.0108\n",
      "2017-11-10 15:35:35: Loss and accuracy at step 1874: 0.0369409, 0.0142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:35:37: Loss and accuracy at step 1875: 0.0369631, 0.0156\n",
      "2017-11-10 15:35:40: Loss and accuracy at step 1876: 0.0369363, 0.0083\n",
      "2017-11-10 15:35:42: Loss and accuracy at step 1877: 0.0369896, 0.0135\n",
      "2017-11-10 15:35:44: Loss and accuracy at step 1878: 0.0369857, 0.0207\n",
      "2017-11-10 15:35:46: Loss and accuracy at step 1879: 0.0369622, 0.0085\n",
      "2017-11-10 15:35:48: Loss and accuracy at step 1880: 0.0370042, 0.0125\n",
      "2017-11-10 15:35:50: Loss and accuracy at step 1881: 0.0370259, 0.0257\n",
      "2017-11-10 15:35:52: Loss and accuracy at step 1882: 0.0369711, 0.0081\n",
      "2017-11-10 15:35:54: Loss and accuracy at step 1883: 0.0369963, 0.0111\n",
      "2017-11-10 15:35:56: Loss and accuracy at step 1884: 0.0370116, 0.0335\n",
      "2017-11-10 15:35:58: Loss and accuracy at step 1885: 0.0370103, 0.0095\n",
      "2017-11-10 15:36:00: Loss and accuracy at step 1886: 0.0368853, 0.0073\n",
      "2017-11-10 15:36:03: Loss and accuracy at step 1887: 0.0369365, 0.0629\n",
      "2017-11-10 15:36:05: Loss and accuracy at step 1888: 0.0369538, 0.007\n",
      "2017-11-10 15:36:07: Loss and accuracy at step 1889: 0.0369892, 0.0121\n",
      "2017-11-10 15:36:09: Loss and accuracy at step 1890: 0.0369485, 0.0545\n",
      "2017-11-10 15:36:11: Loss and accuracy at step 1891: 0.0370136, 0.0105\n",
      "2017-11-10 15:36:13: Loss and accuracy at step 1892: 0.0369439, 0.0173\n",
      "2017-11-10 15:36:15: Loss and accuracy at step 1893: 0.0370343, 0.024\n",
      "2017-11-10 15:36:17: Loss and accuracy at step 1894: 0.0370662, 0.0135\n",
      "2017-11-10 15:36:19: Loss and accuracy at step 1895: 0.0370484, 0.0152\n",
      "2017-11-10 15:36:22: Loss and accuracy at step 1896: 0.0370427, 0.0088\n",
      "2017-11-10 15:36:24: Loss and accuracy at step 1897: 0.0369405, 0.0146\n",
      "2017-11-10 15:36:26: Loss and accuracy at step 1898: 0.0370834, 0.0124\n",
      "2017-11-10 15:36:28: Loss and accuracy at step 1899: 0.0370007, 0.011\n",
      "2017-11-10 15:36:30: Loss and accuracy at step 1900: 0.0369965, 0.0129\n",
      "2017-11-10 15:36:32: Loss and accuracy at step 1901: 0.0369731, 0.0127\n",
      "2017-11-10 15:36:34: Loss and accuracy at step 1902: 0.0369677, 0.0104\n",
      "2017-11-10 15:36:36: Loss and accuracy at step 1903: 0.0369705, 0.0116\n",
      "2017-11-10 15:36:38: Loss and accuracy at step 1904: 0.0369658, 0.0182\n",
      "2017-11-10 15:36:40: Loss and accuracy at step 1905: 0.0370169, 0.0106\n",
      "2017-11-10 15:36:42: Loss and accuracy at step 1906: 0.0369911, 0.0131\n",
      "2017-11-10 15:36:44: Loss and accuracy at step 1907: 0.036981, 0.0161\n",
      "2017-11-10 15:36:47: Loss and accuracy at step 1908: 0.0369658, 0.0125\n",
      "2017-11-10 15:36:49: Loss and accuracy at step 1909: 0.0369534, 0.0116\n",
      "2017-11-10 15:36:51: Loss and accuracy at step 1910: 0.0370308, 0.0156\n",
      "2017-11-10 15:36:53: Loss and accuracy at step 1911: 0.0370613, 0.0136\n",
      "2017-11-10 15:36:55: Loss and accuracy at step 1912: 0.0370183, 0.0126\n",
      "2017-11-10 15:36:57: Loss and accuracy at step 1913: 0.0369632, 0.013\n",
      "2017-11-10 15:36:59: Loss and accuracy at step 1914: 0.0370375, 0.0195\n",
      "2017-11-10 15:37:01: Loss and accuracy at step 1915: 0.0369842, 0.012\n",
      "2017-11-10 15:37:03: Loss and accuracy at step 1916: 0.0368337, 0.0125\n",
      "2017-11-10 15:37:05: Loss and accuracy at step 1917: 0.0369208, 0.0177\n",
      "2017-11-10 15:37:07: Loss and accuracy at step 1918: 0.0369114, 0.0072\n",
      "2017-11-10 15:37:09: Loss and accuracy at step 1919: 0.0369821, 0.013\n",
      "2017-11-10 15:37:12: Loss and accuracy at step 1920: 0.0369773, 0.0147\n",
      "2017-11-10 15:37:14: Loss and accuracy at step 1921: 0.0369544, 0.0101\n",
      "2017-11-10 15:37:16: Loss and accuracy at step 1922: 0.0370191, 0.013\n",
      "2017-11-10 15:37:18: Loss and accuracy at step 1923: 0.0369937, 0.0109\n",
      "2017-11-10 15:37:20: Loss and accuracy at step 1924: 0.0369752, 0.0186\n",
      "2017-11-10 15:37:22: Loss and accuracy at step 1925: 0.0368367, 0.0104\n",
      "2017-11-10 15:37:24: Loss and accuracy at step 1926: 0.0370255, 0.0089\n",
      "2017-11-10 15:37:26: Loss and accuracy at step 1927: 0.0368734, 0.0208\n",
      "2017-11-10 15:37:28: Loss and accuracy at step 1928: 0.03696, 0.0061\n",
      "2017-11-10 15:37:30: Loss and accuracy at step 1929: 0.0369309, 0.0109\n",
      "2017-11-10 15:37:32: Loss and accuracy at step 1930: 0.0370355, 0.022\n",
      "2017-11-10 15:37:35: Loss and accuracy at step 1931: 0.0369018, 0.0056\n",
      "2017-11-10 15:37:37: Loss and accuracy at step 1932: 0.0369561, 0.0113\n",
      "2017-11-10 15:37:39: Loss and accuracy at step 1933: 0.0369099, 0.0097\n",
      "2017-11-10 15:37:41: Loss and accuracy at step 1934: 0.0369938, 0.0136\n",
      "2017-11-10 15:37:43: Loss and accuracy at step 1935: 0.0369521, 0.0088\n",
      "2017-11-10 15:37:45: Loss and accuracy at step 1936: 0.0370044, 0.0088\n",
      "2017-11-10 15:37:47: Loss and accuracy at step 1937: 0.0370054, 0.0138\n",
      "2017-11-10 15:37:49: Loss and accuracy at step 1938: 0.0370328, 0.0112\n",
      "2017-11-10 15:37:51: Loss and accuracy at step 1939: 0.0369426, 0.012\n",
      "2017-11-10 15:37:53: Loss and accuracy at step 1940: 0.0370694, 0.0112\n",
      "2017-11-10 15:37:55: Loss and accuracy at step 1941: 0.0369827, 0.0122\n",
      "2017-11-10 15:37:57: Loss and accuracy at step 1942: 0.0370081, 0.0123\n",
      "2017-11-10 15:37:59: Loss and accuracy at step 1943: 0.0370134, 0.0105\n",
      "2017-11-10 15:38:02: Loss and accuracy at step 1944: 0.0368842, 0.011\n",
      "2017-11-10 15:38:04: Loss and accuracy at step 1945: 0.0368794, 0.0134\n",
      "2017-11-10 15:38:06: Loss and accuracy at step 1946: 0.0369461, 0.0119\n",
      "2017-11-10 15:38:08: Loss and accuracy at step 1947: 0.0369241, 0.0103\n",
      "2017-11-10 15:38:10: Loss and accuracy at step 1948: 0.0369713, 0.0126\n",
      "2017-11-10 15:38:12: Loss and accuracy at step 1949: 0.0369418, 0.0119\n",
      "2017-11-10 15:38:14: Loss and accuracy at step 1950: 0.0369591, 0.0109\n",
      "2017-11-10 15:38:16: Loss and accuracy at step 1951: 0.0369301, 0.0076\n",
      "2017-11-10 15:38:18: Loss and accuracy at step 1952: 0.0370142, 0.0224\n",
      "2017-11-10 15:38:20: Loss and accuracy at step 1953: 0.0370692, 0.0107\n",
      "2017-11-10 15:38:23: Loss and accuracy at step 1954: 0.0369812, 0.0081\n",
      "2017-11-10 15:38:25: Loss and accuracy at step 1955: 0.036956, 0.0143\n",
      "2017-11-10 15:38:27: Loss and accuracy at step 1956: 0.0370382, 0.0075\n",
      "2017-11-10 15:38:29: Loss and accuracy at step 1957: 0.0369163, 0.0135\n",
      "2017-11-10 15:38:31: Loss and accuracy at step 1958: 0.0368588, 0.0101\n",
      "2017-11-10 15:38:33: Loss and accuracy at step 1959: 0.0368755, 0.0093\n",
      "2017-11-10 15:38:35: Loss and accuracy at step 1960: 0.0369573, 0.0135\n",
      "2017-11-10 15:38:37: Loss and accuracy at step 1961: 0.0369036, 0.0164\n",
      "2017-11-10 15:38:39: Loss and accuracy at step 1962: 0.0369457, 0.0082\n",
      "2017-11-10 15:38:41: Loss and accuracy at step 1963: 0.0369769, 0.0112\n",
      "2017-11-10 15:38:43: Loss and accuracy at step 1964: 0.0369312, 0.0166\n",
      "2017-11-10 15:38:46: Loss and accuracy at step 1965: 0.0368997, 0.0091\n",
      "2017-11-10 15:38:48: Loss and accuracy at step 1966: 0.0368971, 0.0115\n",
      "2017-11-10 15:38:50: Loss and accuracy at step 1967: 0.0369243, 0.0109\n",
      "2017-11-10 15:38:52: Loss and accuracy at step 1968: 0.0369621, 0.0103\n",
      "2017-11-10 15:38:54: Loss and accuracy at step 1969: 0.0369304, 0.0111\n",
      "2017-11-10 15:38:56: Loss and accuracy at step 1970: 0.0369971, 0.0105\n",
      "2017-11-10 15:38:58: Loss and accuracy at step 1971: 0.0369362, 0.0151\n",
      "2017-11-10 15:39:00: Loss and accuracy at step 1972: 0.0369363, 0.0088\n",
      "2017-11-10 15:39:02: Loss and accuracy at step 1973: 0.03697, 0.0134\n",
      "2017-11-10 15:39:04: Loss and accuracy at step 1974: 0.036814, 0.0111\n",
      "2017-11-10 15:39:06: Loss and accuracy at step 1975: 0.0369673, 0.0108\n",
      "2017-11-10 15:39:09: Loss and accuracy at step 1976: 0.0368875, 0.0191\n",
      "2017-11-10 15:39:11: Loss and accuracy at step 1977: 0.0369203, 0.0094\n",
      "2017-11-10 15:39:13: Loss and accuracy at step 1978: 0.0369603, 0.0108\n",
      "2017-11-10 15:39:15: Loss and accuracy at step 1979: 0.0369141, 0.0168\n",
      "2017-11-10 15:39:17: Loss and accuracy at step 1980: 0.0368862, 0.0069\n",
      "2017-11-10 15:39:19: Loss and accuracy at step 1981: 0.0369481, 0.0135\n",
      "2017-11-10 15:39:21: Loss and accuracy at step 1982: 0.0369317, 0.0137\n",
      "2017-11-10 15:39:23: Loss and accuracy at step 1983: 0.0369691, 0.0117\n",
      "2017-11-10 15:39:25: Loss and accuracy at step 1984: 0.0368826, 0.0143\n",
      "2017-11-10 15:39:27: Loss and accuracy at step 1985: 0.0369183, 0.0084\n",
      "2017-11-10 15:39:29: Loss and accuracy at step 1986: 0.0369405, 0.0159\n",
      "2017-11-10 15:39:32: Loss and accuracy at step 1987: 0.0368623, 0.0106\n",
      "2017-11-10 15:39:34: Loss and accuracy at step 1988: 0.0368675, 0.0094\n",
      "2017-11-10 15:39:36: Loss and accuracy at step 1989: 0.0369151, 0.0155\n",
      "2017-11-10 15:39:38: Loss and accuracy at step 1990: 0.0369713, 0.008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:39:40: Loss and accuracy at step 1991: 0.0369671, 0.0104\n",
      "2017-11-10 15:39:42: Loss and accuracy at step 1992: 0.0370142, 0.0124\n",
      "2017-11-10 15:39:44: Loss and accuracy at step 1993: 0.0369262, 0.0134\n",
      "2017-11-10 15:39:46: Loss and accuracy at step 1994: 0.0369325, 0.0083\n",
      "2017-11-10 15:39:48: Loss and accuracy at step 1995: 0.0369524, 0.0148\n",
      "2017-11-10 15:39:50: Loss and accuracy at step 1996: 0.0368827, 0.0126\n",
      "2017-11-10 15:39:52: Loss and accuracy at step 1997: 0.0369371, 0.0118\n",
      "2017-11-10 15:39:55: Loss and accuracy at step 1998: 0.0369229, 0.0104\n",
      "2017-11-10 15:39:57: Loss and accuracy at step 1999: 0.0368616, 0.0159\n",
      "2017-11-10 15:39:59: Loss and accuracy at step 2000: 0.0369826, 0.0094\n",
      "2017-11-10 15:40:01: Loss and accuracy at step 2001: 0.0369402, 0.0085\n",
      "2017-11-10 15:40:03: Loss and accuracy at step 2002: 0.0368762, 0.0131\n",
      "2017-11-10 15:40:05: Loss and accuracy at step 2003: 0.0369063, 0.0113\n",
      "2017-11-10 15:40:07: Loss and accuracy at step 2004: 0.0368932, 0.0123\n",
      "2017-11-10 15:40:09: Loss and accuracy at step 2005: 0.036895, 0.0107\n",
      "2017-11-10 15:40:11: Loss and accuracy at step 2006: 0.0369384, 0.0113\n",
      "2017-11-10 15:40:14: Loss and accuracy at step 2007: 0.036843, 0.014\n",
      "2017-11-10 15:40:16: Loss and accuracy at step 2008: 0.0368653, 0.0102\n",
      "2017-11-10 15:40:18: Loss and accuracy at step 2009: 0.0368188, 0.0121\n",
      "2017-11-10 15:40:20: Loss and accuracy at step 2010: 0.036911, 0.0113\n",
      "2017-11-10 15:40:22: Loss and accuracy at step 2011: 0.0368401, 0.0083\n",
      "2017-11-10 15:40:24: Loss and accuracy at step 2012: 0.0369274, 0.0139\n",
      "2017-11-10 15:40:26: Loss and accuracy at step 2013: 0.03689, 0.01\n",
      "2017-11-10 15:40:28: Loss and accuracy at step 2014: 0.0369019, 0.0103\n",
      "2017-11-10 15:40:30: Loss and accuracy at step 2015: 0.0368896, 0.014\n",
      "2017-11-10 15:40:32: Loss and accuracy at step 2016: 0.0369052, 0.0121\n",
      "2017-11-10 15:40:34: Loss and accuracy at step 2017: 0.03693, 0.0127\n",
      "2017-11-10 15:40:37: Loss and accuracy at step 2018: 0.0369395, 0.0108\n",
      "2017-11-10 15:40:39: Loss and accuracy at step 2019: 0.0368771, 0.014\n",
      "2017-11-10 15:40:41: Loss and accuracy at step 2020: 0.0368851, 0.01\n",
      "2017-11-10 15:40:43: Loss and accuracy at step 2021: 0.036887, 0.0115\n",
      "2017-11-10 15:40:45: Loss and accuracy at step 2022: 0.0369167, 0.0147\n",
      "2017-11-10 15:40:47: Loss and accuracy at step 2023: 0.0368839, 0.0125\n",
      "2017-11-10 15:40:49: Loss and accuracy at step 2024: 0.036923, 0.0082\n",
      "2017-11-10 15:40:51: Loss and accuracy at step 2025: 0.0369505, 0.0181\n",
      "2017-11-10 15:40:53: Loss and accuracy at step 2026: 0.036891, 0.0069\n",
      "2017-11-10 15:40:55: Loss and accuracy at step 2027: 0.0370043, 0.0141\n",
      "2017-11-10 15:40:57: Loss and accuracy at step 2028: 0.0369768, 0.0156\n",
      "2017-11-10 15:41:00: Loss and accuracy at step 2029: 0.0369268, 0.0071\n",
      "2017-11-10 15:41:02: Loss and accuracy at step 2030: 0.0369447, 0.0166\n",
      "2017-11-10 15:41:04: Loss and accuracy at step 2031: 0.0369903, 0.0111\n",
      "2017-11-10 15:41:06: Loss and accuracy at step 2032: 0.0369464, 0.0121\n",
      "2017-11-10 15:41:08: Loss and accuracy at step 2033: 0.0370488, 0.0091\n",
      "2017-11-10 15:41:10: Loss and accuracy at step 2034: 0.0369576, 0.0145\n",
      "2017-11-10 15:41:12: Loss and accuracy at step 2035: 0.037014, 0.0116\n",
      "2017-11-10 15:41:14: Loss and accuracy at step 2036: 0.0370095, 0.009\n",
      "2017-11-10 15:41:16: Loss and accuracy at step 2037: 0.0368957, 0.0123\n",
      "2017-11-10 15:41:18: Loss and accuracy at step 2038: 0.0369214, 0.0086\n",
      "2017-11-10 15:41:20: Loss and accuracy at step 2039: 0.0368701, 0.0134\n",
      "2017-11-10 15:41:22: Loss and accuracy at step 2040: 0.0369094, 0.0087\n",
      "2017-11-10 15:41:25: Loss and accuracy at step 2041: 0.0369634, 0.0095\n",
      "2017-11-10 15:41:27: Loss and accuracy at step 2042: 0.036898, 0.0136\n",
      "2017-11-10 15:41:29: Loss and accuracy at step 2043: 0.0368884, 0.0064\n",
      "2017-11-10 15:41:31: Loss and accuracy at step 2044: 0.0370881, 0.0138\n",
      "2017-11-10 15:41:33: Loss and accuracy at step 2045: 0.0369988, 0.0105\n",
      "2017-11-10 15:41:35: Loss and accuracy at step 2046: 0.0369868, 0.0116\n",
      "2017-11-10 15:41:37: Loss and accuracy at step 2047: 0.0369523, 0.007\n",
      "2017-11-10 15:41:39: Loss and accuracy at step 2048: 0.0369799, 0.015\n",
      "2017-11-10 15:41:41: Loss and accuracy at step 2049: 0.036902, 0.012\n",
      "2017-11-10 15:41:43: Loss and accuracy at step 2050: 0.0370038, 0.0074\n",
      "2017-11-10 15:41:45: Loss and accuracy at step 2051: 0.0368502, 0.0174\n",
      "2017-11-10 15:41:48: Loss and accuracy at step 2052: 0.0369493, 0.0129\n",
      "2017-11-10 15:41:50: Loss and accuracy at step 2053: 0.0368973, 0.014\n",
      "2017-11-10 15:41:52: Loss and accuracy at step 2054: 0.036925, 0.0078\n",
      "2017-11-10 15:41:54: Loss and accuracy at step 2055: 0.0368588, 0.0127\n",
      "2017-11-10 15:41:56: Loss and accuracy at step 2056: 0.0369645, 0.0139\n",
      "2017-11-10 15:41:58: Loss and accuracy at step 2057: 0.0369379, 0.0086\n",
      "2017-11-10 15:42:00: Loss and accuracy at step 2058: 0.036894, 0.017\n",
      "2017-11-10 15:42:02: Loss and accuracy at step 2059: 0.0369987, 0.0112\n",
      "2017-11-10 15:42:04: Loss and accuracy at step 2060: 0.0369163, 0.0087\n",
      "2017-11-10 15:42:06: Loss and accuracy at step 2061: 0.0368692, 0.0135\n",
      "2017-11-10 15:42:08: Loss and accuracy at step 2062: 0.0369828, 0.0115\n",
      "2017-11-10 15:42:11: Loss and accuracy at step 2063: 0.036914, 0.0108\n",
      "2017-11-10 15:42:13: Loss and accuracy at step 2064: 0.0369285, 0.015\n",
      "2017-11-10 15:42:15: Loss and accuracy at step 2065: 0.0369038, 0.012\n",
      "2017-11-10 15:42:17: Loss and accuracy at step 2066: 0.0369977, 0.0146\n",
      "2017-11-10 15:42:19: Loss and accuracy at step 2067: 0.0370368, 0.0101\n",
      "2017-11-10 15:42:21: Loss and accuracy at step 2068: 0.0369306, 0.0109\n",
      "2017-11-10 15:42:23: Loss and accuracy at step 2069: 0.0369023, 0.0124\n",
      "2017-11-10 15:42:25: Loss and accuracy at step 2070: 0.0369856, 0.0115\n",
      "2017-11-10 15:42:27: Loss and accuracy at step 2071: 0.0370182, 0.0132\n",
      "2017-11-10 15:42:29: Loss and accuracy at step 2072: 0.0369262, 0.0126\n",
      "2017-11-10 15:42:31: Loss and accuracy at step 2073: 0.0368639, 0.0114\n",
      "2017-11-10 15:42:34: Loss and accuracy at step 2074: 0.036895, 0.0114\n",
      "2017-11-10 15:42:36: Loss and accuracy at step 2075: 0.0368726, 0.0179\n",
      "2017-11-10 15:42:38: Loss and accuracy at step 2076: 0.0370031, 0.0094\n",
      "2017-11-10 15:42:40: Loss and accuracy at step 2077: 0.0369145, 0.0118\n",
      "2017-11-10 15:42:42: Loss and accuracy at step 2078: 0.0368926, 0.0231\n",
      "2017-11-10 15:42:44: Loss and accuracy at step 2079: 0.036986, 0.0098\n",
      "2017-11-10 15:42:46: Loss and accuracy at step 2080: 0.0368886, 0.0108\n",
      "2017-11-10 15:42:48: Loss and accuracy at step 2081: 0.0368921, 0.0258\n",
      "2017-11-10 15:42:50: Loss and accuracy at step 2082: 0.0370828, 0.0108\n",
      "2017-11-10 15:42:52: Loss and accuracy at step 2083: 0.0368799, 0.0131\n",
      "2017-11-10 15:42:54: Loss and accuracy at step 2084: 0.0369405, 0.0175\n",
      "2017-11-10 15:42:56: Loss and accuracy at step 2085: 0.0369543, 0.0129\n",
      "2017-11-10 15:42:59: Loss and accuracy at step 2086: 0.0368713, 0.0118\n",
      "2017-11-10 15:43:01: Loss and accuracy at step 2087: 0.0369174, 0.0134\n",
      "2017-11-10 15:43:03: Loss and accuracy at step 2088: 0.0368378, 0.0136\n",
      "2017-11-10 15:43:05: Loss and accuracy at step 2089: 0.0369431, 0.0093\n",
      "2017-11-10 15:43:07: Loss and accuracy at step 2090: 0.0368921, 0.0153\n",
      "2017-11-10 15:43:09: Loss and accuracy at step 2091: 0.0369932, 0.0107\n",
      "2017-11-10 15:43:11: Loss and accuracy at step 2092: 0.0369725, 0.0121\n",
      "2017-11-10 15:43:13: Loss and accuracy at step 2093: 0.0368742, 0.0086\n",
      "2017-11-10 15:43:15: Loss and accuracy at step 2094: 0.0369597, 0.012\n",
      "2017-11-10 15:43:17: Loss and accuracy at step 2095: 0.0368387, 0.015\n",
      "2017-11-10 15:43:19: Loss and accuracy at step 2096: 0.0369226, 0.0094\n",
      "2017-11-10 15:43:22: Loss and accuracy at step 2097: 0.0369269, 0.013\n",
      "2017-11-10 15:43:24: Loss and accuracy at step 2098: 0.037008, 0.0091\n",
      "2017-11-10 15:43:26: Loss and accuracy at step 2099: 0.0369111, 0.0095\n",
      "2017-11-10 15:43:28: Loss and accuracy at step 2100: 0.0369657, 0.0154\n",
      "2017-11-10 15:43:30: Loss and accuracy at step 2101: 0.0369081, 0.0103\n",
      "2017-11-10 15:43:32: Loss and accuracy at step 2102: 0.0369514, 0.0102\n",
      "2017-11-10 15:43:34: Loss and accuracy at step 2103: 0.0368434, 0.0122\n",
      "2017-11-10 15:43:36: Loss and accuracy at step 2104: 0.0369299, 0.012\n",
      "2017-11-10 15:43:38: Loss and accuracy at step 2105: 0.0369753, 0.0107\n",
      "2017-11-10 15:43:40: Loss and accuracy at step 2106: 0.0368448, 0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:43:42: Loss and accuracy at step 2107: 0.0370518, 0.0145\n",
      "2017-11-10 15:43:45: Loss and accuracy at step 2108: 0.036918, 0.0064\n",
      "2017-11-10 15:43:47: Loss and accuracy at step 2109: 0.0370104, 0.0173\n",
      "2017-11-10 15:43:49: Loss and accuracy at step 2110: 0.0368297, 0.0082\n",
      "2017-11-10 15:43:51: Loss and accuracy at step 2111: 0.0369155, 0.0143\n",
      "2017-11-10 15:43:53: Loss and accuracy at step 2112: 0.0368292, 0.0155\n",
      "2017-11-10 15:43:55: Loss and accuracy at step 2113: 0.0369027, 0.0078\n",
      "2017-11-10 15:43:57: Loss and accuracy at step 2114: 0.0369419, 0.015\n",
      "2017-11-10 15:43:59: Loss and accuracy at step 2115: 0.0369257, 0.0128\n",
      "2017-11-10 15:44:01: Loss and accuracy at step 2116: 0.0369483, 0.0146\n",
      "2017-11-10 15:44:03: Loss and accuracy at step 2117: 0.0368621, 0.0087\n",
      "2017-11-10 15:44:05: Loss and accuracy at step 2118: 0.0369167, 0.0129\n",
      "2017-11-10 15:44:07: Loss and accuracy at step 2119: 0.0368505, 0.0105\n",
      "2017-11-10 15:44:10: Loss and accuracy at step 2120: 0.0369176, 0.013\n",
      "2017-11-10 15:44:12: Loss and accuracy at step 2121: 0.0369133, 0.0129\n",
      "2017-11-10 15:44:14: Loss and accuracy at step 2122: 0.0368588, 0.0074\n",
      "2017-11-10 15:44:16: Loss and accuracy at step 2123: 0.0369077, 0.0179\n",
      "2017-11-10 15:44:18: Loss and accuracy at step 2124: 0.0368508, 0.0116\n",
      "2017-11-10 15:44:20: Loss and accuracy at step 2125: 0.0368372, 0.0102\n",
      "2017-11-10 15:44:22: Loss and accuracy at step 2126: 0.0370386, 0.0142\n",
      "2017-11-10 15:44:24: Loss and accuracy at step 2127: 0.0369189, 0.0091\n",
      "2017-11-10 15:44:27: Loss and accuracy at step 2128: 0.0369443, 0.016\n",
      "2017-11-10 15:44:29: Loss and accuracy at step 2129: 0.0370345, 0.011\n",
      "2017-11-10 15:44:31: Loss and accuracy at step 2130: 0.0368721, 0.0098\n",
      "2017-11-10 15:44:33: Loss and accuracy at step 2131: 0.0370062, 0.012\n",
      "2017-11-10 15:44:35: Loss and accuracy at step 2132: 0.0369145, 0.0125\n",
      "2017-11-10 15:44:37: Loss and accuracy at step 2133: 0.0369088, 0.0113\n",
      "2017-11-10 15:44:39: Loss and accuracy at step 2134: 0.0369029, 0.0101\n",
      "2017-11-10 15:44:41: Loss and accuracy at step 2135: 0.036954, 0.0087\n",
      "2017-11-10 15:44:43: Loss and accuracy at step 2136: 0.0369006, 0.0162\n",
      "2017-11-10 15:44:45: Loss and accuracy at step 2137: 0.0368466, 0.01\n",
      "2017-11-10 15:44:47: Loss and accuracy at step 2138: 0.0369342, 0.0126\n",
      "2017-11-10 15:44:49: Loss and accuracy at step 2139: 0.0368785, 0.0093\n",
      "2017-11-10 15:44:52: Loss and accuracy at step 2140: 0.0369076, 0.0156\n",
      "2017-11-10 15:44:54: Loss and accuracy at step 2141: 0.0369482, 0.0174\n",
      "2017-11-10 15:44:56: Loss and accuracy at step 2142: 0.0369477, 0.007\n",
      "2017-11-10 15:44:58: Loss and accuracy at step 2143: 0.0368949, 0.0213\n",
      "2017-11-10 15:45:00: Loss and accuracy at step 2144: 0.0369119, 0.0106\n",
      "2017-11-10 15:45:02: Loss and accuracy at step 2145: 0.0369406, 0.0163\n",
      "2017-11-10 15:45:04: Loss and accuracy at step 2146: 0.0369677, 0.0116\n",
      "2017-11-10 15:45:06: Loss and accuracy at step 2147: 0.0368575, 0.0094\n",
      "2017-11-10 15:45:08: Loss and accuracy at step 2148: 0.0369848, 0.0187\n",
      "2017-11-10 15:45:10: Loss and accuracy at step 2149: 0.0369484, 0.0107\n",
      "2017-11-10 15:45:12: Loss and accuracy at step 2150: 0.036925, 0.0094\n",
      "2017-11-10 15:45:14: Loss and accuracy at step 2151: 0.0369107, 0.0159\n",
      "2017-11-10 15:45:17: Loss and accuracy at step 2152: 0.0368964, 0.0133\n",
      "2017-11-10 15:45:19: Loss and accuracy at step 2153: 0.0369253, 0.011\n",
      "2017-11-10 15:45:21: Loss and accuracy at step 2154: 0.03699, 0.0116\n",
      "2017-11-10 15:45:23: Loss and accuracy at step 2155: 0.0368301, 0.0101\n",
      "2017-11-10 15:45:25: Loss and accuracy at step 2156: 0.0368402, 0.0117\n",
      "2017-11-10 15:45:27: Loss and accuracy at step 2157: 0.0369501, 0.0124\n",
      "2017-11-10 15:45:29: Loss and accuracy at step 2158: 0.0368233, 0.0088\n",
      "2017-11-10 15:45:31: Loss and accuracy at step 2159: 0.0369261, 0.0128\n",
      "2017-11-10 15:45:33: Loss and accuracy at step 2160: 0.0368722, 0.0153\n",
      "2017-11-10 15:45:35: Loss and accuracy at step 2161: 0.0369193, 0.0074\n",
      "2017-11-10 15:45:37: Loss and accuracy at step 2162: 0.0368752, 0.0155\n",
      "2017-11-10 15:45:40: Loss and accuracy at step 2163: 0.0370015, 0.0173\n",
      "2017-11-10 15:45:42: Loss and accuracy at step 2164: 0.0369745, 0.0069\n",
      "2017-11-10 15:45:44: Loss and accuracy at step 2165: 0.0368312, 0.015\n",
      "2017-11-10 15:45:46: Loss and accuracy at step 2166: 0.0368908, 0.0131\n",
      "2017-11-10 15:45:48: Loss and accuracy at step 2167: 0.0368435, 0.0095\n",
      "2017-11-10 15:45:50: Loss and accuracy at step 2168: 0.0369738, 0.0099\n",
      "2017-11-10 15:45:52: Loss and accuracy at step 2169: 0.0369837, 0.0128\n",
      "2017-11-10 15:45:54: Loss and accuracy at step 2170: 0.0369008, 0.012\n",
      "2017-11-10 15:45:56: Loss and accuracy at step 2171: 0.0368713, 0.0083\n",
      "2017-11-10 15:45:58: Loss and accuracy at step 2172: 0.0369199, 0.014\n",
      "2017-11-10 15:46:00: Loss and accuracy at step 2173: 0.0369037, 0.0134\n",
      "2017-11-10 15:46:02: Loss and accuracy at step 2174: 0.0368821, 0.0107\n",
      "2017-11-10 15:46:05: Loss and accuracy at step 2175: 0.0368665, 0.0127\n",
      "2017-11-10 15:46:07: Loss and accuracy at step 2176: 0.0368053, 0.015\n",
      "2017-11-10 15:46:09: Loss and accuracy at step 2177: 0.0368374, 0.0134\n",
      "2017-11-10 15:46:11: Loss and accuracy at step 2178: 0.0368795, 0.0112\n",
      "2017-11-10 15:46:13: Loss and accuracy at step 2179: 0.0369028, 0.0115\n",
      "2017-11-10 15:46:15: Loss and accuracy at step 2180: 0.0368513, 0.0105\n",
      "2017-11-10 15:46:17: Loss and accuracy at step 2181: 0.0369837, 0.0109\n",
      "2017-11-10 15:46:19: Loss and accuracy at step 2182: 0.036982, 0.0173\n",
      "2017-11-10 15:46:22: Loss and accuracy at step 2183: 0.0368717, 0.0105\n",
      "2017-11-10 15:46:24: Loss and accuracy at step 2184: 0.03687, 0.0087\n",
      "2017-11-10 15:46:26: Loss and accuracy at step 2185: 0.0368945, 0.0204\n",
      "2017-11-10 15:46:28: Loss and accuracy at step 2186: 0.0369031, 0.0097\n",
      "2017-11-10 15:46:30: Loss and accuracy at step 2187: 0.0369457, 0.0084\n",
      "2017-11-10 15:46:32: Loss and accuracy at step 2188: 0.0368481, 0.0164\n",
      "2017-11-10 15:46:34: Loss and accuracy at step 2189: 0.0369706, 0.0119\n",
      "2017-11-10 15:46:36: Loss and accuracy at step 2190: 0.0370097, 0.0069\n",
      "2017-11-10 15:46:38: Loss and accuracy at step 2191: 0.0369011, 0.0168\n",
      "2017-11-10 15:46:40: Loss and accuracy at step 2192: 0.0369791, 0.0133\n",
      "2017-11-10 15:46:42: Loss and accuracy at step 2193: 0.0369551, 0.0109\n",
      "2017-11-10 15:46:44: Loss and accuracy at step 2194: 0.0369215, 0.0151\n",
      "2017-11-10 15:46:47: Loss and accuracy at step 2195: 0.0368627, 0.0116\n",
      "2017-11-10 15:46:49: Loss and accuracy at step 2196: 0.0369695, 0.0152\n",
      "2017-11-10 15:46:51: Loss and accuracy at step 2197: 0.0369385, 0.0078\n",
      "2017-11-10 15:46:53: Loss and accuracy at step 2198: 0.0368735, 0.0177\n",
      "2017-11-10 15:46:55: Loss and accuracy at step 2199: 0.0369191, 0.013\n",
      "2017-11-10 15:46:57: Loss and accuracy at step 2200: 0.0369591, 0.0086\n",
      "2017-11-10 15:46:59: Loss and accuracy at step 2201: 0.0369472, 0.0142\n",
      "2017-11-10 15:47:01: Loss and accuracy at step 2202: 0.0369186, 0.0102\n",
      "2017-11-10 15:47:03: Loss and accuracy at step 2203: 0.036903, 0.0177\n",
      "2017-11-10 15:47:05: Loss and accuracy at step 2204: 0.0369091, 0.0081\n",
      "2017-11-10 15:47:07: Loss and accuracy at step 2205: 0.0368476, 0.0114\n",
      "2017-11-10 15:47:10: Loss and accuracy at step 2206: 0.0369338, 0.0116\n",
      "2017-11-10 15:47:12: Loss and accuracy at step 2207: 0.0369033, 0.0112\n",
      "2017-11-10 15:47:14: Loss and accuracy at step 2208: 0.0369083, 0.0146\n",
      "2017-11-10 15:47:16: Loss and accuracy at step 2209: 0.03681, 0.0069\n",
      "2017-11-10 15:47:18: Loss and accuracy at step 2210: 0.0369366, 0.0159\n",
      "2017-11-10 15:47:20: Loss and accuracy at step 2211: 0.0368885, 0.011\n",
      "2017-11-10 15:47:22: Loss and accuracy at step 2212: 0.0368886, 0.0101\n",
      "2017-11-10 15:47:24: Loss and accuracy at step 2213: 0.0369258, 0.0142\n",
      "2017-11-10 15:47:26: Loss and accuracy at step 2214: 0.0369925, 0.0106\n",
      "2017-11-10 15:47:28: Loss and accuracy at step 2215: 0.036833, 0.0167\n",
      "2017-11-10 15:47:30: Loss and accuracy at step 2216: 0.0368829, 0.0075\n",
      "2017-11-10 15:47:33: Loss and accuracy at step 2217: 0.0369502, 0.016\n",
      "2017-11-10 15:47:35: Loss and accuracy at step 2218: 0.0367537, 0.0141\n",
      "2017-11-10 15:47:37: Loss and accuracy at step 2219: 0.0369283, 0.007\n",
      "2017-11-10 15:47:39: Loss and accuracy at step 2220: 0.0369476, 0.0194\n",
      "2017-11-10 15:47:41: Loss and accuracy at step 2221: 0.0368403, 0.0094\n",
      "2017-11-10 15:47:43: Loss and accuracy at step 2222: 0.0369517, 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:47:45: Loss and accuracy at step 2223: 0.0370344, 0.0132\n",
      "2017-11-10 15:47:47: Loss and accuracy at step 2224: 0.0369086, 0.0117\n",
      "2017-11-10 15:47:49: Loss and accuracy at step 2225: 0.0369844, 0.0096\n",
      "2017-11-10 15:47:51: Loss and accuracy at step 2226: 0.0369247, 0.0095\n",
      "2017-11-10 15:47:53: Loss and accuracy at step 2227: 0.036905, 0.0156\n",
      "2017-11-10 15:47:55: Loss and accuracy at step 2228: 0.0368222, 0.0076\n",
      "2017-11-10 15:47:58: Loss and accuracy at step 2229: 0.0369734, 0.0124\n",
      "2017-11-10 15:48:00: Loss and accuracy at step 2230: 0.0368343, 0.0149\n",
      "2017-11-10 15:48:02: Loss and accuracy at step 2231: 0.0369198, 0.0082\n",
      "2017-11-10 15:48:04: Loss and accuracy at step 2232: 0.0368944, 0.0131\n",
      "2017-11-10 15:48:06: Loss and accuracy at step 2233: 0.0368642, 0.0132\n",
      "2017-11-10 15:48:08: Loss and accuracy at step 2234: 0.036893, 0.0103\n",
      "2017-11-10 15:48:10: Loss and accuracy at step 2235: 0.0369296, 0.0112\n",
      "2017-11-10 15:48:12: Loss and accuracy at step 2236: 0.0369564, 0.0179\n",
      "2017-11-10 15:48:14: Loss and accuracy at step 2237: 0.0368516, 0.013\n",
      "2017-11-10 15:48:17: Loss and accuracy at step 2238: 0.0370014, 0.0081\n",
      "2017-11-10 15:48:19: Loss and accuracy at step 2239: 0.0369629, 0.0167\n",
      "2017-11-10 15:48:21: Loss and accuracy at step 2240: 0.0369754, 0.012\n",
      "2017-11-10 15:48:23: Loss and accuracy at step 2241: 0.0369233, 0.012\n",
      "2017-11-10 15:48:25: Loss and accuracy at step 2242: 0.0368908, 0.0105\n",
      "2017-11-10 15:48:27: Loss and accuracy at step 2243: 0.0370193, 0.01\n",
      "2017-11-10 15:48:29: Loss and accuracy at step 2244: 0.0369558, 0.0165\n",
      "2017-11-10 15:48:31: Loss and accuracy at step 2245: 0.0368382, 0.008\n",
      "2017-11-10 15:48:33: Loss and accuracy at step 2246: 0.0369718, 0.0171\n",
      "2017-11-10 15:48:35: Loss and accuracy at step 2247: 0.0369673, 0.012\n",
      "2017-11-10 15:48:37: Loss and accuracy at step 2248: 0.0370214, 0.0097\n",
      "2017-11-10 15:48:39: Loss and accuracy at step 2249: 0.0368338, 0.0159\n",
      "2017-11-10 15:48:42: Loss and accuracy at step 2250: 0.036889, 0.0084\n",
      "2017-11-10 15:48:44: Loss and accuracy at step 2251: 0.0368748, 0.0118\n",
      "2017-11-10 15:48:46: Loss and accuracy at step 2252: 0.0369602, 0.0131\n",
      "2017-11-10 15:48:48: Loss and accuracy at step 2253: 0.0369199, 0.0079\n",
      "2017-11-10 15:48:50: Loss and accuracy at step 2254: 0.0368999, 0.0149\n",
      "2017-11-10 15:48:52: Loss and accuracy at step 2255: 0.0369615, 0.0121\n",
      "2017-11-10 15:48:54: Loss and accuracy at step 2256: 0.0368933, 0.013\n",
      "2017-11-10 15:48:56: Loss and accuracy at step 2257: 0.0369535, 0.0104\n",
      "2017-11-10 15:48:58: Loss and accuracy at step 2258: 0.0369141, 0.0123\n",
      "2017-11-10 15:49:00: Loss and accuracy at step 2259: 0.0370258, 0.009\n",
      "2017-11-10 15:49:02: Loss and accuracy at step 2260: 0.0369458, 0.0121\n",
      "2017-11-10 15:49:05: Loss and accuracy at step 2261: 0.0368294, 0.0124\n",
      "2017-11-10 15:49:07: Loss and accuracy at step 2262: 0.0369526, 0.0104\n",
      "2017-11-10 15:49:09: Loss and accuracy at step 2263: 0.0368639, 0.0175\n",
      "2017-11-10 15:49:11: Loss and accuracy at step 2264: 0.0368301, 0.0089\n",
      "2017-11-10 15:49:13: Loss and accuracy at step 2265: 0.036914, 0.0127\n",
      "2017-11-10 15:49:15: Loss and accuracy at step 2266: 0.0369177, 0.013\n",
      "2017-11-10 15:49:17: Loss and accuracy at step 2267: 0.0368803, 0.0143\n",
      "2017-11-10 15:49:19: Loss and accuracy at step 2268: 0.0369808, 0.0088\n",
      "2017-11-10 15:49:21: Loss and accuracy at step 2269: 0.036897, 0.0101\n",
      "2017-11-10 15:49:23: Loss and accuracy at step 2270: 0.0368512, 0.0189\n",
      "2017-11-10 15:49:25: Loss and accuracy at step 2271: 0.0369874, 0.0089\n",
      "2017-11-10 15:49:27: Loss and accuracy at step 2272: 0.0370124, 0.0132\n",
      "2017-11-10 15:49:30: Loss and accuracy at step 2273: 0.0369848, 0.0113\n",
      "2017-11-10 15:49:32: Loss and accuracy at step 2274: 0.0368669, 0.0112\n",
      "2017-11-10 15:49:34: Loss and accuracy at step 2275: 0.0369572, 0.0132\n",
      "2017-11-10 15:49:36: Loss and accuracy at step 2276: 0.0368339, 0.0109\n",
      "2017-11-10 15:49:38: Loss and accuracy at step 2277: 0.0368912, 0.0121\n",
      "2017-11-10 15:49:40: Loss and accuracy at step 2278: 0.0368898, 0.0094\n",
      "2017-11-10 15:49:42: Loss and accuracy at step 2279: 0.0368598, 0.014\n",
      "2017-11-10 15:49:44: Loss and accuracy at step 2280: 0.0369465, 0.0098\n",
      "2017-11-10 15:49:46: Loss and accuracy at step 2281: 0.0369595, 0.0119\n",
      "2017-11-10 15:49:48: Loss and accuracy at step 2282: 0.0369287, 0.0118\n",
      "2017-11-10 15:49:50: Loss and accuracy at step 2283: 0.0368529, 0.0075\n",
      "2017-11-10 15:49:53: Loss and accuracy at step 2284: 0.0368748, 0.0105\n",
      "2017-11-10 15:49:55: Loss and accuracy at step 2285: 0.0367556, 0.0143\n",
      "2017-11-10 15:49:57: Loss and accuracy at step 2286: 0.0368924, 0.0065\n",
      "2017-11-10 15:49:59: Loss and accuracy at step 2287: 0.0368392, 0.0132\n",
      "2017-11-10 15:50:01: Loss and accuracy at step 2288: 0.0368487, 0.0121\n",
      "2017-11-10 15:50:03: Loss and accuracy at step 2289: 0.0369417, 0.0082\n",
      "2017-11-10 15:50:05: Loss and accuracy at step 2290: 0.0369426, 0.0191\n",
      "2017-11-10 15:50:07: Loss and accuracy at step 2291: 0.036918, 0.012\n",
      "2017-11-10 15:50:09: Loss and accuracy at step 2292: 0.0368301, 0.0095\n",
      "2017-11-10 15:50:11: Loss and accuracy at step 2293: 0.0368619, 0.0238\n",
      "2017-11-10 15:50:14: Loss and accuracy at step 2294: 0.0368768, 0.0091\n",
      "2017-11-10 15:50:16: Loss and accuracy at step 2295: 0.0369432, 0.0133\n",
      "2017-11-10 15:50:18: Loss and accuracy at step 2296: 0.0368725, 0.0156\n",
      "2017-11-10 15:50:20: Loss and accuracy at step 2297: 0.0368673, 0.01\n",
      "2017-11-10 15:50:22: Loss and accuracy at step 2298: 0.036993, 0.0176\n",
      "2017-11-10 15:50:24: Loss and accuracy at step 2299: 0.0368849, 0.009\n",
      "2017-11-10 15:50:26: Loss and accuracy at step 2300: 0.0370332, 0.0177\n",
      "2017-11-10 15:50:28: Loss and accuracy at step 2301: 0.0369358, 0.0232\n",
      "2017-11-10 15:50:30: Loss and accuracy at step 2302: 0.0369031, 0.0081\n",
      "2017-11-10 15:50:32: Loss and accuracy at step 2303: 0.0369739, 0.0137\n",
      "2017-11-10 15:50:34: Loss and accuracy at step 2304: 0.0369334, 0.0132\n",
      "2017-11-10 15:50:37: Loss and accuracy at step 2305: 0.0369482, 0.0115\n",
      "2017-11-10 15:50:39: Loss and accuracy at step 2306: 0.0369303, 0.0118\n",
      "2017-11-10 15:50:41: Loss and accuracy at step 2307: 0.0368233, 0.0122\n",
      "2017-11-10 15:50:43: Loss and accuracy at step 2308: 0.0369384, 0.0129\n",
      "2017-11-10 15:50:45: Loss and accuracy at step 2309: 0.0369149, 0.0147\n",
      "2017-11-10 15:50:47: Loss and accuracy at step 2310: 0.0368544, 0.0071\n",
      "2017-11-10 15:50:49: Loss and accuracy at step 2311: 0.0368602, 0.0147\n",
      "2017-11-10 15:50:51: Loss and accuracy at step 2312: 0.0369381, 0.0098\n",
      "2017-11-10 15:50:53: Loss and accuracy at step 2313: 0.0369623, 0.0125\n",
      "2017-11-10 15:50:55: Loss and accuracy at step 2314: 0.0369047, 0.0084\n",
      "2017-11-10 15:50:57: Loss and accuracy at step 2315: 0.0368785, 0.0153\n",
      "2017-11-10 15:51:00: Loss and accuracy at step 2316: 0.036822, 0.0121\n",
      "2017-11-10 15:51:02: Loss and accuracy at step 2317: 0.0369253, 0.0079\n",
      "2017-11-10 15:51:04: Loss and accuracy at step 2318: 0.0368523, 0.0174\n",
      "2017-11-10 15:51:06: Loss and accuracy at step 2319: 0.0369585, 0.0103\n",
      "2017-11-10 15:51:08: Loss and accuracy at step 2320: 0.0369426, 0.0088\n",
      "2017-11-10 15:51:10: Loss and accuracy at step 2321: 0.0368337, 0.0162\n",
      "2017-11-10 15:51:12: Loss and accuracy at step 2322: 0.0369676, 0.0073\n",
      "2017-11-10 15:51:14: Loss and accuracy at step 2323: 0.0369405, 0.0145\n",
      "2017-11-10 15:51:16: Loss and accuracy at step 2324: 0.0368596, 0.0115\n",
      "2017-11-10 15:51:18: Loss and accuracy at step 2325: 0.0368823, 0.0096\n",
      "2017-11-10 15:51:20: Loss and accuracy at step 2326: 0.0370112, 0.0126\n",
      "2017-11-10 15:51:23: Loss and accuracy at step 2327: 0.0369116, 0.0089\n",
      "2017-11-10 15:51:25: Loss and accuracy at step 2328: 0.036873, 0.0123\n",
      "2017-11-10 15:51:27: Loss and accuracy at step 2329: 0.0369265, 0.0095\n",
      "2017-11-10 15:51:29: Loss and accuracy at step 2330: 0.0368939, 0.0083\n",
      "2017-11-10 15:51:31: Loss and accuracy at step 2331: 0.0368456, 0.0145\n",
      "2017-11-10 15:51:33: Loss and accuracy at step 2332: 0.0369191, 0.0074\n",
      "2017-11-10 15:51:35: Loss and accuracy at step 2333: 0.0369752, 0.0108\n",
      "2017-11-10 15:51:37: Loss and accuracy at step 2334: 0.036954, 0.01\n",
      "2017-11-10 15:51:39: Loss and accuracy at step 2335: 0.0369181, 0.0099\n",
      "2017-11-10 15:51:41: Loss and accuracy at step 2336: 0.036946, 0.0117\n",
      "2017-11-10 15:51:43: Loss and accuracy at step 2337: 0.0369056, 0.0088\n",
      "2017-11-10 15:51:46: Loss and accuracy at step 2338: 0.036879, 0.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:51:48: Loss and accuracy at step 2339: 0.0370326, 0.0137\n",
      "2017-11-10 15:51:50: Loss and accuracy at step 2340: 0.0368753, 0.0097\n",
      "2017-11-10 15:51:52: Loss and accuracy at step 2341: 0.0369302, 0.0134\n",
      "2017-11-10 15:51:54: Loss and accuracy at step 2342: 0.036917, 0.0133\n",
      "2017-11-10 15:51:56: Loss and accuracy at step 2343: 0.0369417, 0.0094\n",
      "2017-11-10 15:51:58: Loss and accuracy at step 2344: 0.0368632, 0.0145\n",
      "2017-11-10 15:52:00: Loss and accuracy at step 2345: 0.0369835, 0.0087\n",
      "2017-11-10 15:52:02: Loss and accuracy at step 2346: 0.0369719, 0.0118\n",
      "2017-11-10 15:52:04: Loss and accuracy at step 2347: 0.0369397, 0.0148\n",
      "2017-11-10 15:52:06: Loss and accuracy at step 2348: 0.0369393, 0.0099\n",
      "2017-11-10 15:52:08: Loss and accuracy at step 2349: 0.0369545, 0.0174\n",
      "2017-11-10 15:52:11: Loss and accuracy at step 2350: 0.036936, 0.009\n",
      "2017-11-10 15:52:13: Loss and accuracy at step 2351: 0.0369775, 0.01\n",
      "2017-11-10 15:52:15: Loss and accuracy at step 2352: 0.0369466, 0.0138\n",
      "2017-11-10 15:52:17: Loss and accuracy at step 2353: 0.0370025, 0.0076\n",
      "2017-11-10 15:52:19: Loss and accuracy at step 2354: 0.0370075, 0.0178\n",
      "2017-11-10 15:52:21: Loss and accuracy at step 2355: 0.0369684, 0.0083\n",
      "2017-11-10 15:52:23: Loss and accuracy at step 2356: 0.0369035, 0.0121\n",
      "2017-11-10 15:52:25: Loss and accuracy at step 2357: 0.0369686, 0.0171\n",
      "2017-11-10 15:52:27: Loss and accuracy at step 2358: 0.0370229, 0.0082\n",
      "2017-11-10 15:52:29: Loss and accuracy at step 2359: 0.0368845, 0.0168\n",
      "2017-11-10 15:52:31: Loss and accuracy at step 2360: 0.0369313, 0.0111\n",
      "2017-11-10 15:52:34: Loss and accuracy at step 2361: 0.0369279, 0.0095\n",
      "2017-11-10 15:52:36: Loss and accuracy at step 2362: 0.0369382, 0.0143\n",
      "2017-11-10 15:52:38: Loss and accuracy at step 2363: 0.0368787, 0.0107\n",
      "2017-11-10 15:52:40: Loss and accuracy at step 2364: 0.0368633, 0.0137\n",
      "2017-11-10 15:52:42: Loss and accuracy at step 2365: 0.0367822, 0.0147\n",
      "2017-11-10 15:52:44: Loss and accuracy at step 2366: 0.0368696, 0.0089\n",
      "2017-11-10 15:52:46: Loss and accuracy at step 2367: 0.0369207, 0.0114\n",
      "2017-11-10 15:52:48: Loss and accuracy at step 2368: 0.0368637, 0.0172\n",
      "2017-11-10 15:52:50: Loss and accuracy at step 2369: 0.0368467, 0.0093\n",
      "2017-11-10 15:52:52: Loss and accuracy at step 2370: 0.0369917, 0.0086\n",
      "2017-11-10 15:52:54: Loss and accuracy at step 2371: 0.0369084, 0.0264\n",
      "2017-11-10 15:52:57: Loss and accuracy at step 2372: 0.0369382, 0.0109\n",
      "2017-11-10 15:52:59: Loss and accuracy at step 2373: 0.0369822, 0.0095\n",
      "2017-11-10 15:53:01: Loss and accuracy at step 2374: 0.0368618, 0.0303\n",
      "2017-11-10 15:53:03: Loss and accuracy at step 2375: 0.0369256, 0.0145\n",
      "2017-11-10 15:53:05: Loss and accuracy at step 2376: 0.0368887, 0.0097\n",
      "2017-11-10 15:53:07: Loss and accuracy at step 2377: 0.0368527, 0.0186\n",
      "2017-11-10 15:53:09: Loss and accuracy at step 2378: 0.0369344, 0.0095\n",
      "2017-11-10 15:53:11: Loss and accuracy at step 2379: 0.036875, 0.0113\n",
      "2017-11-10 15:53:13: Loss and accuracy at step 2380: 0.0368237, 0.011\n",
      "2017-11-10 15:53:15: Loss and accuracy at step 2381: 0.0369479, 0.0124\n",
      "2017-11-10 15:53:17: Loss and accuracy at step 2382: 0.0369351, 0.0169\n",
      "2017-11-10 15:53:20: Loss and accuracy at step 2383: 0.0368418, 0.0087\n",
      "2017-11-10 15:53:22: Loss and accuracy at step 2384: 0.0369323, 0.0121\n",
      "2017-11-10 15:53:24: Loss and accuracy at step 2385: 0.0367988, 0.0197\n",
      "2017-11-10 15:53:26: Loss and accuracy at step 2386: 0.0369363, 0.0097\n",
      "2017-11-10 15:53:28: Loss and accuracy at step 2387: 0.0369215, 0.0142\n",
      "2017-11-10 15:53:30: Loss and accuracy at step 2388: 0.0368027, 0.0156\n",
      "2017-11-10 15:53:32: Loss and accuracy at step 2389: 0.0368699, 0.0078\n",
      "2017-11-10 15:53:34: Loss and accuracy at step 2390: 0.037027, 0.0152\n",
      "2017-11-10 15:53:36: Loss and accuracy at step 2391: 0.0368878, 0.0249\n",
      "2017-11-10 15:53:38: Loss and accuracy at step 2392: 0.0369229, 0.0065\n",
      "2017-11-10 15:53:40: Loss and accuracy at step 2393: 0.0368819, 0.0123\n",
      "2017-11-10 15:53:43: Loss and accuracy at step 2394: 0.0369623, 0.0453\n",
      "2017-11-10 15:53:45: Loss and accuracy at step 2395: 0.0368422, 0.0086\n",
      "2017-11-10 15:53:47: Loss and accuracy at step 2396: 0.0369276, 0.0131\n",
      "2017-11-10 15:53:49: Loss and accuracy at step 2397: 0.0368957, 0.0375\n",
      "2017-11-10 15:53:51: Loss and accuracy at step 2398: 0.0367689, 0.0095\n",
      "2017-11-10 15:53:53: Loss and accuracy at step 2399: 0.0369944, 0.0128\n",
      "2017-11-10 15:53:55: Loss and accuracy at step 2400: 0.0368224, 0.0111\n",
      "2017-11-10 15:53:57: Loss and accuracy at step 2401: 0.0368953, 0.0127\n",
      "2017-11-10 15:53:59: Loss and accuracy at step 2402: 0.0369088, 0.0102\n",
      "2017-11-10 15:54:01: Loss and accuracy at step 2403: 0.0369116, 0.009\n",
      "2017-11-10 15:54:03: Loss and accuracy at step 2404: 0.0368624, 0.0083\n",
      "2017-11-10 15:54:06: Loss and accuracy at step 2405: 0.0369344, 0.0104\n",
      "2017-11-10 15:54:08: Loss and accuracy at step 2406: 0.0369147, 0.0175\n",
      "2017-11-10 15:54:10: Loss and accuracy at step 2407: 0.0368862, 0.0083\n",
      "2017-11-10 15:54:12: Loss and accuracy at step 2408: 0.0368707, 0.0136\n",
      "2017-11-10 15:54:14: Loss and accuracy at step 2409: 0.0369217, 0.0116\n",
      "2017-11-10 15:54:16: Loss and accuracy at step 2410: 0.0369123, 0.0085\n",
      "2017-11-10 15:54:18: Loss and accuracy at step 2411: 0.0369647, 0.0213\n",
      "2017-11-10 15:54:20: Loss and accuracy at step 2412: 0.0369537, 0.0093\n",
      "2017-11-10 15:54:22: Loss and accuracy at step 2413: 0.0369105, 0.013\n",
      "2017-11-10 15:54:25: Loss and accuracy at step 2414: 0.0369586, 0.0173\n",
      "2017-11-10 15:54:27: Loss and accuracy at step 2415: 0.0369086, 0.0084\n",
      "2017-11-10 15:54:29: Loss and accuracy at step 2416: 0.0369277, 0.0155\n",
      "2017-11-10 15:54:31: Loss and accuracy at step 2417: 0.0367788, 0.012\n",
      "2017-11-10 15:54:33: Loss and accuracy at step 2418: 0.0368912, 0.0122\n",
      "2017-11-10 15:54:35: Loss and accuracy at step 2419: 0.0369545, 0.0109\n",
      "2017-11-10 15:54:37: Loss and accuracy at step 2420: 0.0368341, 0.0118\n",
      "2017-11-10 15:54:39: Loss and accuracy at step 2421: 0.0369095, 0.0173\n",
      "2017-11-10 15:54:41: Loss and accuracy at step 2422: 0.0369958, 0.0075\n",
      "2017-11-10 15:54:43: Loss and accuracy at step 2423: 0.0369508, 0.0134\n",
      "2017-11-10 15:54:45: Loss and accuracy at step 2424: 0.0370045, 0.0145\n",
      "2017-11-10 15:54:47: Loss and accuracy at step 2425: 0.0369821, 0.0062\n",
      "2017-11-10 15:54:50: Loss and accuracy at step 2426: 0.0369432, 0.0132\n",
      "2017-11-10 15:54:52: Loss and accuracy at step 2427: 0.0368808, 0.0128\n",
      "2017-11-10 15:54:54: Loss and accuracy at step 2428: 0.0369388, 0.0122\n",
      "2017-11-10 15:54:56: Loss and accuracy at step 2429: 0.0368544, 0.0092\n",
      "2017-11-10 15:54:58: Loss and accuracy at step 2430: 0.0368904, 0.0124\n",
      "2017-11-10 15:55:00: Loss and accuracy at step 2431: 0.0369299, 0.0153\n",
      "2017-11-10 15:55:02: Loss and accuracy at step 2432: 0.036892, 0.0106\n",
      "2017-11-10 15:55:04: Loss and accuracy at step 2433: 0.0368823, 0.0099\n",
      "2017-11-10 15:55:06: Loss and accuracy at step 2434: 0.0369917, 0.0183\n",
      "2017-11-10 15:55:08: Loss and accuracy at step 2435: 0.0369244, 0.0064\n",
      "2017-11-10 15:55:10: Loss and accuracy at step 2436: 0.0368936, 0.0125\n",
      "2017-11-10 15:55:12: Loss and accuracy at step 2437: 0.0369106, 0.017\n",
      "2017-11-10 15:55:15: Loss and accuracy at step 2438: 0.0369042, 0.0082\n",
      "2017-11-10 15:55:17: Loss and accuracy at step 2439: 0.0368785, 0.0091\n",
      "2017-11-10 15:55:19: Loss and accuracy at step 2440: 0.0369534, 0.0152\n",
      "2017-11-10 15:55:21: Loss and accuracy at step 2441: 0.0368744, 0.0157\n",
      "2017-11-10 15:55:23: Loss and accuracy at step 2442: 0.0368278, 0.0073\n",
      "2017-11-10 15:55:25: Loss and accuracy at step 2443: 0.0368707, 0.0135\n",
      "2017-11-10 15:55:27: Loss and accuracy at step 2444: 0.0368658, 0.0144\n",
      "2017-11-10 15:55:29: Loss and accuracy at step 2445: 0.0368914, 0.0104\n",
      "2017-11-10 15:55:31: Loss and accuracy at step 2446: 0.0368356, 0.0189\n",
      "2017-11-10 15:55:33: Loss and accuracy at step 2447: 0.0369343, 0.0091\n",
      "2017-11-10 15:55:35: Loss and accuracy at step 2448: 0.0369747, 0.0128\n",
      "2017-11-10 15:55:38: Loss and accuracy at step 2449: 0.0367678, 0.0104\n",
      "2017-11-10 15:55:40: Loss and accuracy at step 2450: 0.0369438, 0.0101\n",
      "2017-11-10 15:55:42: Loss and accuracy at step 2451: 0.0369331, 0.0109\n",
      "2017-11-10 15:55:44: Loss and accuracy at step 2452: 0.0367539, 0.0097\n",
      "2017-11-10 15:55:46: Loss and accuracy at step 2453: 0.0367816, 0.0114\n",
      "2017-11-10 15:55:48: Loss and accuracy at step 2454: 0.03686, 0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:55:50: Loss and accuracy at step 2455: 0.036882, 0.0104\n",
      "2017-11-10 15:55:52: Loss and accuracy at step 2456: 0.0368356, 0.0116\n",
      "2017-11-10 15:55:54: Loss and accuracy at step 2457: 0.0369201, 0.0115\n",
      "2017-11-10 15:55:56: Loss and accuracy at step 2458: 0.036821, 0.0154\n",
      "2017-11-10 15:55:58: Loss and accuracy at step 2459: 0.0369041, 0.009\n",
      "2017-11-10 15:56:01: Loss and accuracy at step 2460: 0.0369237, 0.0117\n",
      "2017-11-10 15:56:03: Loss and accuracy at step 2461: 0.0368944, 0.0188\n",
      "2017-11-10 15:56:05: Loss and accuracy at step 2462: 0.0369647, 0.0093\n",
      "2017-11-10 15:56:07: Loss and accuracy at step 2463: 0.036923, 0.011\n",
      "2017-11-10 15:56:09: Loss and accuracy at step 2464: 0.036905, 0.0091\n",
      "2017-11-10 15:56:11: Loss and accuracy at step 2465: 0.036981, 0.0152\n",
      "2017-11-10 15:56:13: Loss and accuracy at step 2466: 0.0369065, 0.0096\n",
      "2017-11-10 15:56:15: Loss and accuracy at step 2467: 0.0369019, 0.0101\n",
      "2017-11-10 15:56:17: Loss and accuracy at step 2468: 0.0368829, 0.0216\n",
      "2017-11-10 15:56:19: Loss and accuracy at step 2469: 0.0368944, 0.009\n",
      "2017-11-10 15:56:22: Loss and accuracy at step 2470: 0.0368453, 0.0114\n",
      "2017-11-10 15:56:24: Loss and accuracy at step 2471: 0.0369362, 0.0119\n",
      "2017-11-10 15:56:26: Loss and accuracy at step 2472: 0.0369486, 0.0165\n",
      "2017-11-10 15:56:28: Loss and accuracy at step 2473: 0.0369217, 0.008\n",
      "2017-11-10 15:56:30: Loss and accuracy at step 2474: 0.036978, 0.0112\n",
      "2017-11-10 15:56:32: Loss and accuracy at step 2475: 0.0368742, 0.0166\n",
      "2017-11-10 15:56:34: Loss and accuracy at step 2476: 0.0368911, 0.0079\n",
      "2017-11-10 15:56:36: Loss and accuracy at step 2477: 0.0369658, 0.0169\n",
      "2017-11-10 15:56:38: Loss and accuracy at step 2478: 0.0369032, 0.0101\n",
      "2017-11-10 15:56:40: Loss and accuracy at step 2479: 0.0368784, 0.0097\n",
      "2017-11-10 15:56:42: Loss and accuracy at step 2480: 0.0369004, 0.0147\n",
      "2017-11-10 15:56:44: Loss and accuracy at step 2481: 0.0369012, 0.0094\n",
      "2017-11-10 15:56:47: Loss and accuracy at step 2482: 0.0369333, 0.0122\n",
      "2017-11-10 15:56:49: Loss and accuracy at step 2483: 0.0368226, 0.0084\n",
      "2017-11-10 15:56:51: Loss and accuracy at step 2484: 0.0368986, 0.0134\n",
      "2017-11-10 15:56:53: Loss and accuracy at step 2485: 0.0369592, 0.0134\n",
      "2017-11-10 15:56:55: Loss and accuracy at step 2486: 0.0369614, 0.0091\n",
      "2017-11-10 15:56:57: Loss and accuracy at step 2487: 0.0368096, 0.0201\n",
      "2017-11-10 15:56:59: Loss and accuracy at step 2488: 0.0369822, 0.0089\n",
      "2017-11-10 15:57:01: Loss and accuracy at step 2489: 0.03694, 0.0095\n",
      "2017-11-10 15:57:03: Loss and accuracy at step 2490: 0.0368747, 0.0151\n",
      "2017-11-10 15:57:05: Loss and accuracy at step 2491: 0.0369441, 0.0095\n",
      "2017-11-10 15:57:07: Loss and accuracy at step 2492: 0.037015, 0.0128\n",
      "2017-11-10 15:57:09: Loss and accuracy at step 2493: 0.0368932, 0.0085\n",
      "2017-11-10 15:57:12: Loss and accuracy at step 2494: 0.0369209, 0.0131\n",
      "2017-11-10 15:57:14: Loss and accuracy at step 2495: 0.0369728, 0.0135\n",
      "2017-11-10 15:57:16: Loss and accuracy at step 2496: 0.0368888, 0.0085\n",
      "2017-11-10 15:57:18: Loss and accuracy at step 2497: 0.036852, 0.0145\n",
      "2017-11-10 15:57:20: Loss and accuracy at step 2498: 0.0368776, 0.0154\n",
      "2017-11-10 15:57:22: Loss and accuracy at step 2499: 0.0368714, 0.0112\n",
      "2017-11-10 15:57:24: Loss and accuracy at step 2500: 0.0368475, 0.0122\n",
      "2017-11-10 15:57:26: Loss and accuracy at step 2501: 0.0368489, 0.02\n",
      "2017-11-10 15:57:28: Loss and accuracy at step 2502: 0.0368552, 0.01\n",
      "2017-11-10 15:57:30: Loss and accuracy at step 2503: 0.0368781, 0.009\n",
      "2017-11-10 15:57:32: Loss and accuracy at step 2504: 0.0369092, 0.0304\n",
      "2017-11-10 15:57:34: Loss and accuracy at step 2505: 0.0369003, 0.0085\n",
      "2017-11-10 15:57:37: Loss and accuracy at step 2506: 0.0368947, 0.0097\n",
      "2017-11-10 15:57:39: Loss and accuracy at step 2507: 0.036866, 0.0171\n",
      "2017-11-10 15:57:41: Loss and accuracy at step 2508: 0.0369275, 0.0067\n",
      "2017-11-10 15:57:43: Loss and accuracy at step 2509: 0.0368616, 0.0125\n",
      "2017-11-10 15:57:45: Loss and accuracy at step 2510: 0.0369183, 0.0078\n",
      "2017-11-10 15:57:47: Loss and accuracy at step 2511: 0.0368779, 0.0115\n",
      "2017-11-10 15:57:49: Loss and accuracy at step 2512: 0.0369328, 0.0112\n",
      "2017-11-10 15:57:51: Loss and accuracy at step 2513: 0.0368647, 0.0094\n",
      "2017-11-10 15:57:53: Loss and accuracy at step 2514: 0.0369273, 0.0198\n",
      "2017-11-10 15:57:55: Loss and accuracy at step 2515: 0.0369279, 0.0068\n",
      "2017-11-10 15:57:57: Loss and accuracy at step 2516: 0.0368549, 0.0113\n",
      "2017-11-10 15:57:59: Loss and accuracy at step 2517: 0.0369035, 0.0202\n",
      "2017-11-10 15:58:02: Loss and accuracy at step 2518: 0.0368995, 0.0108\n",
      "2017-11-10 15:58:04: Loss and accuracy at step 2519: 0.0368438, 0.0118\n",
      "2017-11-10 15:58:06: Loss and accuracy at step 2520: 0.0368826, 0.0142\n",
      "2017-11-10 15:58:08: Loss and accuracy at step 2521: 0.0368934, 0.01\n",
      "2017-11-10 15:58:10: Loss and accuracy at step 2522: 0.036887, 0.0133\n",
      "2017-11-10 15:58:12: Loss and accuracy at step 2523: 0.0368581, 0.0094\n",
      "2017-11-10 15:58:14: Loss and accuracy at step 2524: 0.0368699, 0.0108\n",
      "2017-11-10 15:58:16: Loss and accuracy at step 2525: 0.0369031, 0.0143\n",
      "2017-11-10 15:58:18: Loss and accuracy at step 2526: 0.0368742, 0.012\n",
      "2017-11-10 15:58:21: Loss and accuracy at step 2527: 0.0369444, 0.0111\n",
      "2017-11-10 15:58:23: Loss and accuracy at step 2528: 0.0369213, 0.0106\n",
      "2017-11-10 15:58:25: Loss and accuracy at step 2529: 0.0368579, 0.0152\n",
      "2017-11-10 15:58:27: Loss and accuracy at step 2530: 0.0368512, 0.0131\n",
      "2017-11-10 15:58:29: Loss and accuracy at step 2531: 0.0368624, 0.0091\n",
      "2017-11-10 15:58:31: Loss and accuracy at step 2532: 0.0368737, 0.0135\n",
      "2017-11-10 15:58:33: Loss and accuracy at step 2533: 0.0368317, 0.0126\n",
      "2017-11-10 15:58:35: Loss and accuracy at step 2534: 0.0368194, 0.0094\n",
      "2017-11-10 15:58:37: Loss and accuracy at step 2535: 0.0369353, 0.0086\n",
      "2017-11-10 15:58:39: Loss and accuracy at step 2536: 0.0368981, 0.0157\n",
      "2017-11-10 15:58:41: Loss and accuracy at step 2537: 0.0369122, 0.011\n",
      "2017-11-10 15:58:44: Loss and accuracy at step 2538: 0.0368353, 0.0087\n",
      "2017-11-10 15:58:46: Loss and accuracy at step 2539: 0.0368198, 0.0113\n",
      "2017-11-10 15:58:48: Loss and accuracy at step 2540: 0.0368886, 0.0095\n",
      "2017-11-10 15:58:50: Loss and accuracy at step 2541: 0.0368408, 0.0128\n",
      "2017-11-10 15:58:52: Loss and accuracy at step 2542: 0.0368439, 0.0092\n",
      "2017-11-10 15:58:54: Loss and accuracy at step 2543: 0.036883, 0.0142\n",
      "2017-11-10 15:58:56: Loss and accuracy at step 2544: 0.0369266, 0.0094\n",
      "2017-11-10 15:58:58: Loss and accuracy at step 2545: 0.0369097, 0.0122\n",
      "2017-11-10 15:59:00: Loss and accuracy at step 2546: 0.0369818, 0.01\n",
      "2017-11-10 15:59:02: Loss and accuracy at step 2547: 0.0368506, 0.0104\n",
      "2017-11-10 15:59:04: Loss and accuracy at step 2548: 0.0368506, 0.0141\n",
      "2017-11-10 15:59:07: Loss and accuracy at step 2549: 0.0369706, 0.0115\n",
      "2017-11-10 15:59:09: Loss and accuracy at step 2550: 0.0368008, 0.0108\n",
      "2017-11-10 15:59:11: Loss and accuracy at step 2551: 0.0368948, 0.0119\n",
      "2017-11-10 15:59:13: Loss and accuracy at step 2552: 0.0368582, 0.0126\n",
      "2017-11-10 15:59:15: Loss and accuracy at step 2553: 0.0369186, 0.0111\n",
      "2017-11-10 15:59:17: Loss and accuracy at step 2554: 0.0368017, 0.0109\n",
      "2017-11-10 15:59:19: Loss and accuracy at step 2555: 0.0369684, 0.0165\n",
      "2017-11-10 15:59:21: Loss and accuracy at step 2556: 0.0368762, 0.0104\n",
      "2017-11-10 15:59:23: Loss and accuracy at step 2557: 0.0369955, 0.013\n",
      "2017-11-10 15:59:25: Loss and accuracy at step 2558: 0.0368726, 0.011\n",
      "2017-11-10 15:59:27: Loss and accuracy at step 2559: 0.0370128, 0.0116\n",
      "2017-11-10 15:59:30: Loss and accuracy at step 2560: 0.0368724, 0.0181\n",
      "2017-11-10 15:59:32: Loss and accuracy at step 2561: 0.036918, 0.0093\n",
      "2017-11-10 15:59:34: Loss and accuracy at step 2562: 0.036935, 0.012\n",
      "2017-11-10 15:59:36: Loss and accuracy at step 2563: 0.0367571, 0.0108\n",
      "2017-11-10 15:59:38: Loss and accuracy at step 2564: 0.0369338, 0.0107\n",
      "2017-11-10 15:59:40: Loss and accuracy at step 2565: 0.036966, 0.0124\n",
      "2017-11-10 15:59:42: Loss and accuracy at step 2566: 0.036924, 0.0104\n",
      "2017-11-10 15:59:44: Loss and accuracy at step 2567: 0.0369199, 0.0122\n",
      "2017-11-10 15:59:46: Loss and accuracy at step 2568: 0.0368795, 0.0105\n",
      "2017-11-10 15:59:48: Loss and accuracy at step 2569: 0.0368309, 0.0131\n",
      "2017-11-10 15:59:50: Loss and accuracy at step 2570: 0.0368213, 0.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 15:59:53: Loss and accuracy at step 2571: 0.0369157, 0.0133\n",
      "2017-11-10 15:59:55: Loss and accuracy at step 2572: 0.0368555, 0.0148\n",
      "2017-11-10 15:59:57: Loss and accuracy at step 2573: 0.0369206, 0.0092\n",
      "2017-11-10 15:59:59: Loss and accuracy at step 2574: 0.0368339, 0.0108\n",
      "2017-11-10 16:00:01: Loss and accuracy at step 2575: 0.0369031, 0.0182\n",
      "2017-11-10 16:00:03: Loss and accuracy at step 2576: 0.0369571, 0.0107\n",
      "2017-11-10 16:00:05: Loss and accuracy at step 2577: 0.036893, 0.0107\n",
      "2017-11-10 16:00:07: Loss and accuracy at step 2578: 0.0369436, 0.0177\n",
      "2017-11-10 16:00:09: Loss and accuracy at step 2579: 0.0369416, 0.0084\n",
      "2017-11-10 16:00:11: Loss and accuracy at step 2580: 0.0368858, 0.0148\n",
      "2017-11-10 16:00:14: Loss and accuracy at step 2581: 0.0368887, 0.0159\n",
      "2017-11-10 16:00:16: Loss and accuracy at step 2582: 0.0369056, 0.0115\n",
      "2017-11-10 16:00:18: Loss and accuracy at step 2583: 0.0368309, 0.0152\n",
      "2017-11-10 16:00:20: Loss and accuracy at step 2584: 0.0367947, 0.0132\n",
      "2017-11-10 16:00:22: Loss and accuracy at step 2585: 0.0368551, 0.014\n",
      "2017-11-10 16:00:24: Loss and accuracy at step 2586: 0.0367701, 0.0103\n",
      "2017-11-10 16:00:26: Loss and accuracy at step 2587: 0.0368766, 0.0122\n",
      "2017-11-10 16:00:28: Loss and accuracy at step 2588: 0.0368961, 0.0168\n",
      "2017-11-10 16:00:30: Loss and accuracy at step 2589: 0.0369042, 0.0089\n",
      "2017-11-10 16:00:32: Loss and accuracy at step 2590: 0.0368573, 0.0134\n",
      "2017-11-10 16:00:35: Loss and accuracy at step 2591: 0.036799, 0.0164\n",
      "2017-11-10 16:00:37: Loss and accuracy at step 2592: 0.0369165, 0.0096\n",
      "2017-11-10 16:00:39: Loss and accuracy at step 2593: 0.0368581, 0.0132\n",
      "2017-11-10 16:00:41: Loss and accuracy at step 2594: 0.0368592, 0.0121\n",
      "2017-11-10 16:00:43: Loss and accuracy at step 2595: 0.0368278, 0.0104\n",
      "2017-11-10 16:00:45: Loss and accuracy at step 2596: 0.0367782, 0.013\n",
      "2017-11-10 16:00:47: Loss and accuracy at step 2597: 0.0369138, 0.014\n",
      "2017-11-10 16:00:49: Loss and accuracy at step 2598: 0.036864, 0.0093\n",
      "2017-11-10 16:00:51: Loss and accuracy at step 2599: 0.036878, 0.0099\n",
      "2017-11-10 16:00:53: Loss and accuracy at step 2600: 0.0368315, 0.0122\n",
      "2017-11-10 16:00:56: Loss and accuracy at step 2601: 0.0368752, 0.0125\n",
      "2017-11-10 16:00:58: Loss and accuracy at step 2602: 0.0368777, 0.0102\n",
      "2017-11-10 16:01:00: Loss and accuracy at step 2603: 0.0369195, 0.0094\n",
      "2017-11-10 16:01:02: Loss and accuracy at step 2604: 0.0368842, 0.0126\n",
      "2017-11-10 16:01:04: Loss and accuracy at step 2605: 0.0369077, 0.0115\n",
      "2017-11-10 16:01:06: Loss and accuracy at step 2606: 0.0369585, 0.0109\n",
      "2017-11-10 16:01:08: Loss and accuracy at step 2607: 0.0368796, 0.011\n",
      "2017-11-10 16:01:10: Loss and accuracy at step 2608: 0.0369172, 0.0117\n",
      "2017-11-10 16:01:12: Loss and accuracy at step 2609: 0.0368562, 0.0147\n",
      "2017-11-10 16:01:14: Loss and accuracy at step 2610: 0.0369014, 0.0114\n",
      "2017-11-10 16:01:16: Loss and accuracy at step 2611: 0.0368755, 0.0091\n",
      "2017-11-10 16:01:18: Loss and accuracy at step 2612: 0.0368774, 0.0177\n",
      "2017-11-10 16:01:21: Loss and accuracy at step 2613: 0.0368618, 0.0113\n",
      "2017-11-10 16:01:23: Loss and accuracy at step 2614: 0.0368776, 0.0198\n",
      "2017-11-10 16:01:25: Loss and accuracy at step 2615: 0.0368504, 0.0088\n",
      "2017-11-10 16:01:27: Loss and accuracy at step 2616: 0.0370157, 0.0102\n",
      "2017-11-10 16:01:29: Loss and accuracy at step 2617: 0.0368785, 0.0276\n",
      "2017-11-10 16:01:31: Loss and accuracy at step 2618: 0.0368889, 0.0076\n",
      "2017-11-10 16:01:33: Loss and accuracy at step 2619: 0.0368158, 0.0143\n",
      "2017-11-10 16:01:35: Loss and accuracy at step 2620: 0.036913, 0.0125\n",
      "2017-11-10 16:01:37: Loss and accuracy at step 2621: 0.0368237, 0.0101\n",
      "2017-11-10 16:01:39: Loss and accuracy at step 2622: 0.0368518, 0.0143\n",
      "2017-11-10 16:01:41: Loss and accuracy at step 2623: 0.0368557, 0.0079\n",
      "2017-11-10 16:01:44: Loss and accuracy at step 2624: 0.0368004, 0.0144\n",
      "2017-11-10 16:01:46: Loss and accuracy at step 2625: 0.0368701, 0.0134\n",
      "2017-11-10 16:01:48: Loss and accuracy at step 2626: 0.0369172, 0.0088\n",
      "2017-11-10 16:01:50: Loss and accuracy at step 2627: 0.0368585, 0.0175\n",
      "2017-11-10 16:01:52: Loss and accuracy at step 2628: 0.0369568, 0.0101\n",
      "2017-11-10 16:01:54: Loss and accuracy at step 2629: 0.0368043, 0.0132\n",
      "2017-11-10 16:01:56: Loss and accuracy at step 2630: 0.0368829, 0.0121\n",
      "2017-11-10 16:01:58: Loss and accuracy at step 2631: 0.0367645, 0.009\n",
      "2017-11-10 16:02:00: Loss and accuracy at step 2632: 0.0368514, 0.0178\n",
      "2017-11-10 16:02:02: Loss and accuracy at step 2633: 0.0369436, 0.0134\n",
      "2017-11-10 16:02:04: Loss and accuracy at step 2634: 0.0369257, 0.0098\n",
      "2017-11-10 16:02:07: Loss and accuracy at step 2635: 0.0369008, 0.0127\n",
      "2017-11-10 16:02:09: Loss and accuracy at step 2636: 0.0368818, 0.0121\n",
      "2017-11-10 16:02:11: Loss and accuracy at step 2637: 0.0368285, 0.011\n",
      "2017-11-10 16:02:13: Loss and accuracy at step 2638: 0.0368775, 0.0115\n",
      "2017-11-10 16:02:15: Loss and accuracy at step 2639: 0.0369184, 0.0108\n",
      "2017-11-10 16:02:17: Loss and accuracy at step 2640: 0.0368578, 0.0092\n",
      "2017-11-10 16:02:19: Loss and accuracy at step 2641: 0.0367905, 0.0139\n",
      "2017-11-10 16:02:21: Loss and accuracy at step 2642: 0.0368166, 0.0152\n",
      "2017-11-10 16:02:23: Loss and accuracy at step 2643: 0.0369018, 0.0074\n",
      "2017-11-10 16:02:25: Loss and accuracy at step 2644: 0.0368824, 0.0195\n",
      "2017-11-10 16:02:28: Loss and accuracy at step 2645: 0.0368614, 0.0126\n",
      "2017-11-10 16:02:30: Loss and accuracy at step 2646: 0.0368533, 0.0067\n",
      "2017-11-10 16:02:32: Loss and accuracy at step 2647: 0.0368357, 0.0185\n",
      "2017-11-10 16:02:34: Loss and accuracy at step 2648: 0.0368639, 0.0087\n",
      "2017-11-10 16:02:36: Loss and accuracy at step 2649: 0.036979, 0.0104\n",
      "2017-11-10 16:02:38: Loss and accuracy at step 2650: 0.0368892, 0.0255\n",
      "2017-11-10 16:02:40: Loss and accuracy at step 2651: 0.0369693, 0.0075\n",
      "2017-11-10 16:02:42: Loss and accuracy at step 2652: 0.0369418, 0.0158\n",
      "2017-11-10 16:02:44: Loss and accuracy at step 2653: 0.0369276, 0.0103\n",
      "2017-11-10 16:02:46: Loss and accuracy at step 2654: 0.0369078, 0.0104\n",
      "2017-11-10 16:02:48: Loss and accuracy at step 2655: 0.0369566, 0.0127\n",
      "2017-11-10 16:02:50: Loss and accuracy at step 2656: 0.0368334, 0.0081\n",
      "2017-11-10 16:02:53: Loss and accuracy at step 2657: 0.0369847, 0.0113\n",
      "2017-11-10 16:02:55: Loss and accuracy at step 2658: 0.0369079, 0.0102\n",
      "2017-11-10 16:02:57: Loss and accuracy at step 2659: 0.0368497, 0.0137\n",
      "2017-11-10 16:02:59: Loss and accuracy at step 2660: 0.0369249, 0.0134\n",
      "2017-11-10 16:03:01: Loss and accuracy at step 2661: 0.036897, 0.0095\n",
      "2017-11-10 16:03:03: Loss and accuracy at step 2662: 0.0369268, 0.0125\n",
      "2017-11-10 16:03:05: Loss and accuracy at step 2663: 0.0368301, 0.0138\n",
      "2017-11-10 16:03:07: Loss and accuracy at step 2664: 0.0368562, 0.0064\n",
      "2017-11-10 16:03:09: Loss and accuracy at step 2665: 0.0368628, 0.0134\n",
      "2017-11-10 16:03:11: Loss and accuracy at step 2666: 0.0368198, 0.0096\n",
      "2017-11-10 16:03:13: Loss and accuracy at step 2667: 0.0369243, 0.0133\n",
      "2017-11-10 16:03:16: Loss and accuracy at step 2668: 0.0369271, 0.0078\n",
      "2017-11-10 16:03:18: Loss and accuracy at step 2669: 0.0368317, 0.0105\n",
      "2017-11-10 16:03:20: Loss and accuracy at step 2670: 0.0368616, 0.016\n",
      "2017-11-10 16:03:22: Loss and accuracy at step 2671: 0.0367942, 0.0112\n",
      "2017-11-10 16:03:24: Loss and accuracy at step 2672: 0.0369103, 0.0131\n",
      "2017-11-10 16:03:26: Loss and accuracy at step 2673: 0.0368839, 0.011\n",
      "2017-11-10 16:03:28: Loss and accuracy at step 2674: 0.0369085, 0.0173\n",
      "2017-11-10 16:03:30: Loss and accuracy at step 2675: 0.0370286, 0.0109\n",
      "2017-11-10 16:03:32: Loss and accuracy at step 2676: 0.0369078, 0.012\n",
      "2017-11-10 16:03:34: Loss and accuracy at step 2677: 0.0369785, 0.0083\n",
      "2017-11-10 16:03:36: Loss and accuracy at step 2678: 0.0369189, 0.0134\n",
      "2017-11-10 16:03:39: Loss and accuracy at step 2679: 0.0368882, 0.0123\n",
      "2017-11-10 16:03:41: Loss and accuracy at step 2680: 0.0368953, 0.0101\n",
      "2017-11-10 16:03:43: Loss and accuracy at step 2681: 0.0369141, 0.0109\n",
      "2017-11-10 16:03:45: Loss and accuracy at step 2682: 0.0369152, 0.0134\n",
      "2017-11-10 16:03:47: Loss and accuracy at step 2683: 0.0368722, 0.0096\n",
      "2017-11-10 16:03:49: Loss and accuracy at step 2684: 0.0369001, 0.0131\n",
      "2017-11-10 16:03:51: Loss and accuracy at step 2685: 0.0368255, 0.0106\n",
      "2017-11-10 16:03:53: Loss and accuracy at step 2686: 0.036848, 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:03:55: Loss and accuracy at step 2687: 0.0369022, 0.0145\n",
      "2017-11-10 16:03:57: Loss and accuracy at step 2688: 0.0368595, 0.0093\n",
      "2017-11-10 16:03:59: Loss and accuracy at step 2689: 0.0368709, 0.01\n",
      "2017-11-10 16:04:01: Loss and accuracy at step 2690: 0.0368741, 0.0137\n",
      "2017-11-10 16:04:04: Loss and accuracy at step 2691: 0.0368776, 0.0132\n",
      "2017-11-10 16:04:06: Loss and accuracy at step 2692: 0.0368873, 0.0088\n",
      "2017-11-10 16:04:08: Loss and accuracy at step 2693: 0.0368679, 0.0077\n",
      "2017-11-10 16:04:10: Loss and accuracy at step 2694: 0.0368499, 0.019\n",
      "2017-11-10 16:04:12: Loss and accuracy at step 2695: 0.0369578, 0.0097\n",
      "2017-11-10 16:04:14: Loss and accuracy at step 2696: 0.0368297, 0.0117\n",
      "2017-11-10 16:04:16: Loss and accuracy at step 2697: 0.0368901, 0.0112\n",
      "2017-11-10 16:04:18: Loss and accuracy at step 2698: 0.0367671, 0.0105\n",
      "2017-11-10 16:04:20: Loss and accuracy at step 2699: 0.036879, 0.0182\n",
      "2017-11-10 16:04:23: Loss and accuracy at step 2700: 0.0369237, 0.0086\n",
      "2017-11-10 16:04:25: Loss and accuracy at step 2701: 0.0368518, 0.0097\n",
      "2017-11-10 16:04:27: Loss and accuracy at step 2702: 0.0369091, 0.0205\n",
      "2017-11-10 16:04:29: Loss and accuracy at step 2703: 0.036836, 0.0103\n",
      "2017-11-10 16:04:31: Loss and accuracy at step 2704: 0.0369113, 0.0092\n",
      "2017-11-10 16:04:33: Loss and accuracy at step 2705: 0.0368283, 0.0135\n",
      "2017-11-10 16:04:35: Loss and accuracy at step 2706: 0.0368283, 0.0107\n",
      "2017-11-10 16:04:37: Loss and accuracy at step 2707: 0.036859, 0.0082\n",
      "2017-11-10 16:04:39: Loss and accuracy at step 2708: 0.0369106, 0.0164\n",
      "2017-11-10 16:04:41: Loss and accuracy at step 2709: 0.0368381, 0.0106\n",
      "2017-11-10 16:04:43: Loss and accuracy at step 2710: 0.036903, 0.0085\n",
      "2017-11-10 16:04:45: Loss and accuracy at step 2711: 0.036887, 0.0097\n",
      "2017-11-10 16:04:48: Loss and accuracy at step 2712: 0.0368802, 0.0104\n",
      "2017-11-10 16:04:50: Loss and accuracy at step 2713: 0.0368738, 0.0115\n",
      "2017-11-10 16:04:52: Loss and accuracy at step 2714: 0.0368622, 0.0135\n",
      "2017-11-10 16:04:54: Loss and accuracy at step 2715: 0.0368588, 0.011\n",
      "2017-11-10 16:04:56: Loss and accuracy at step 2716: 0.0368666, 0.0105\n",
      "2017-11-10 16:04:58: Loss and accuracy at step 2717: 0.0368523, 0.0115\n",
      "2017-11-10 16:05:00: Loss and accuracy at step 2718: 0.0368864, 0.0115\n",
      "2017-11-10 16:05:02: Loss and accuracy at step 2719: 0.0368945, 0.0116\n",
      "2017-11-10 16:05:04: Loss and accuracy at step 2720: 0.0369686, 0.0077\n",
      "2017-11-10 16:05:06: Loss and accuracy at step 2721: 0.0368199, 0.0121\n",
      "2017-11-10 16:05:08: Loss and accuracy at step 2722: 0.0368492, 0.0161\n",
      "2017-11-10 16:05:11: Loss and accuracy at step 2723: 0.0368812, 0.0075\n",
      "2017-11-10 16:05:13: Loss and accuracy at step 2724: 0.0368753, 0.0153\n",
      "2017-11-10 16:05:15: Loss and accuracy at step 2725: 0.0368586, 0.0155\n",
      "2017-11-10 16:05:17: Loss and accuracy at step 2726: 0.0368284, 0.0111\n",
      "2017-11-10 16:05:19: Loss and accuracy at step 2727: 0.0368338, 0.0098\n",
      "2017-11-10 16:05:21: Loss and accuracy at step 2728: 0.0368768, 0.0111\n",
      "2017-11-10 16:05:23: Loss and accuracy at step 2729: 0.0369419, 0.0115\n",
      "2017-11-10 16:05:25: Loss and accuracy at step 2730: 0.0369005, 0.0112\n",
      "2017-11-10 16:05:27: Loss and accuracy at step 2731: 0.0369848, 0.0106\n",
      "2017-11-10 16:05:29: Loss and accuracy at step 2732: 0.0368822, 0.0076\n",
      "2017-11-10 16:05:31: Loss and accuracy at step 2733: 0.0369049, 0.015\n",
      "2017-11-10 16:05:34: Loss and accuracy at step 2734: 0.0369818, 0.0143\n",
      "2017-11-10 16:05:36: Loss and accuracy at step 2735: 0.0369127, 0.0087\n",
      "2017-11-10 16:05:38: Loss and accuracy at step 2736: 0.0369361, 0.0168\n",
      "2017-11-10 16:05:40: Loss and accuracy at step 2737: 0.0368739, 0.0092\n",
      "2017-11-10 16:05:42: Loss and accuracy at step 2738: 0.0368436, 0.0102\n",
      "2017-11-10 16:05:44: Loss and accuracy at step 2739: 0.0367924, 0.0152\n",
      "2017-11-10 16:05:46: Loss and accuracy at step 2740: 0.036845, 0.0092\n",
      "2017-11-10 16:05:48: Loss and accuracy at step 2741: 0.0368786, 0.0122\n",
      "2017-11-10 16:05:50: Loss and accuracy at step 2742: 0.0369262, 0.0105\n",
      "2017-11-10 16:05:52: Loss and accuracy at step 2743: 0.0368816, 0.0096\n",
      "2017-11-10 16:05:54: Loss and accuracy at step 2744: 0.03689, 0.0123\n",
      "2017-11-10 16:05:56: Loss and accuracy at step 2745: 0.0368671, 0.0122\n",
      "2017-11-10 16:05:59: Loss and accuracy at step 2746: 0.0368697, 0.0085\n",
      "2017-11-10 16:06:01: Loss and accuracy at step 2747: 0.0369356, 0.0104\n",
      "2017-11-10 16:06:03: Loss and accuracy at step 2748: 0.0368129, 0.0153\n",
      "2017-11-10 16:06:05: Loss and accuracy at step 2749: 0.0369859, 0.0089\n",
      "2017-11-10 16:06:07: Loss and accuracy at step 2750: 0.0368863, 0.0098\n",
      "2017-11-10 16:06:09: Loss and accuracy at step 2751: 0.0369993, 0.0121\n",
      "2017-11-10 16:06:11: Loss and accuracy at step 2752: 0.0368166, 0.0122\n",
      "2017-11-10 16:06:13: Loss and accuracy at step 2753: 0.0368653, 0.0127\n",
      "2017-11-10 16:06:15: Loss and accuracy at step 2754: 0.0367418, 0.0111\n",
      "2017-11-10 16:06:18: Loss and accuracy at step 2755: 0.0368272, 0.0094\n",
      "2017-11-10 16:06:20: Loss and accuracy at step 2756: 0.0367015, 0.0124\n",
      "2017-11-10 16:06:22: Loss and accuracy at step 2757: 0.0368132, 0.0101\n",
      "2017-11-10 16:06:24: Loss and accuracy at step 2758: 0.0368484, 0.0105\n",
      "2017-11-10 16:06:26: Loss and accuracy at step 2759: 0.0368407, 0.0141\n",
      "2017-11-10 16:06:28: Loss and accuracy at step 2760: 0.0368943, 0.0102\n",
      "2017-11-10 16:06:30: Loss and accuracy at step 2761: 0.0368372, 0.0109\n",
      "2017-11-10 16:06:32: Loss and accuracy at step 2762: 0.0369549, 0.0153\n",
      "2017-11-10 16:06:34: Loss and accuracy at step 2763: 0.0368675, 0.01\n",
      "2017-11-10 16:06:36: Loss and accuracy at step 2764: 0.0368794, 0.0167\n",
      "2017-11-10 16:06:38: Loss and accuracy at step 2765: 0.0368877, 0.0083\n",
      "2017-11-10 16:06:41: Loss and accuracy at step 2766: 0.0370259, 0.0149\n",
      "2017-11-10 16:06:43: Loss and accuracy at step 2767: 0.0368915, 0.0096\n",
      "2017-11-10 16:06:45: Loss and accuracy at step 2768: 0.0368807, 0.0114\n",
      "2017-11-10 16:06:47: Loss and accuracy at step 2769: 0.0369089, 0.0141\n",
      "2017-11-10 16:06:49: Loss and accuracy at step 2770: 0.0368555, 0.0082\n",
      "2017-11-10 16:06:51: Loss and accuracy at step 2771: 0.0368844, 0.0162\n",
      "2017-11-10 16:06:53: Loss and accuracy at step 2772: 0.0368664, 0.009\n",
      "2017-11-10 16:06:55: Loss and accuracy at step 2773: 0.0369263, 0.0144\n",
      "2017-11-10 16:06:57: Loss and accuracy at step 2774: 0.0369278, 0.0112\n",
      "2017-11-10 16:06:59: Loss and accuracy at step 2775: 0.0369384, 0.0083\n",
      "2017-11-10 16:07:01: Loss and accuracy at step 2776: 0.0369539, 0.0109\n",
      "2017-11-10 16:07:03: Loss and accuracy at step 2777: 0.0369374, 0.011\n",
      "2017-11-10 16:07:06: Loss and accuracy at step 2778: 0.0368241, 0.0131\n",
      "2017-11-10 16:07:08: Loss and accuracy at step 2779: 0.0368941, 0.0117\n",
      "2017-11-10 16:07:10: Loss and accuracy at step 2780: 0.0368353, 0.0103\n",
      "2017-11-10 16:07:12: Loss and accuracy at step 2781: 0.0369046, 0.013\n",
      "2017-11-10 16:07:14: Loss and accuracy at step 2782: 0.0368251, 0.0112\n",
      "2017-11-10 16:07:16: Loss and accuracy at step 2783: 0.0369746, 0.0089\n",
      "2017-11-10 16:07:18: Loss and accuracy at step 2784: 0.0369505, 0.012\n",
      "2017-11-10 16:07:20: Loss and accuracy at step 2785: 0.0369009, 0.0097\n",
      "2017-11-10 16:07:22: Loss and accuracy at step 2786: 0.036851, 0.014\n",
      "2017-11-10 16:07:24: Loss and accuracy at step 2787: 0.0368872, 0.0077\n",
      "2017-11-10 16:07:26: Loss and accuracy at step 2788: 0.0368385, 0.0121\n",
      "2017-11-10 16:07:28: Loss and accuracy at step 2789: 0.0369279, 0.0116\n",
      "2017-11-10 16:07:31: Loss and accuracy at step 2790: 0.0368829, 0.011\n",
      "2017-11-10 16:07:33: Loss and accuracy at step 2791: 0.0368646, 0.0125\n",
      "2017-11-10 16:07:35: Loss and accuracy at step 2792: 0.0369192, 0.009\n",
      "2017-11-10 16:07:37: Loss and accuracy at step 2793: 0.0369448, 0.0161\n",
      "2017-11-10 16:07:39: Loss and accuracy at step 2794: 0.0369215, 0.0123\n",
      "2017-11-10 16:07:41: Loss and accuracy at step 2795: 0.0369441, 0.01\n",
      "2017-11-10 16:07:43: Loss and accuracy at step 2796: 0.0368798, 0.0128\n",
      "2017-11-10 16:07:45: Loss and accuracy at step 2797: 0.0369541, 0.0086\n",
      "2017-11-10 16:07:47: Loss and accuracy at step 2798: 0.0368885, 0.0103\n",
      "2017-11-10 16:07:49: Loss and accuracy at step 2799: 0.037019, 0.0205\n",
      "2017-11-10 16:07:51: Loss and accuracy at step 2800: 0.0369042, 0.0065\n",
      "2017-11-10 16:07:53: Loss and accuracy at step 2801: 0.0369142, 0.0161\n",
      "2017-11-10 16:07:56: Loss and accuracy at step 2802: 0.0369055, 0.0164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:07:58: Loss and accuracy at step 2803: 0.0369211, 0.0083\n",
      "2017-11-10 16:08:00: Loss and accuracy at step 2804: 0.0368021, 0.0165\n",
      "2017-11-10 16:08:02: Loss and accuracy at step 2805: 0.0368837, 0.0119\n",
      "2017-11-10 16:08:04: Loss and accuracy at step 2806: 0.0368549, 0.0136\n",
      "2017-11-10 16:08:06: Loss and accuracy at step 2807: 0.036768, 0.0101\n",
      "2017-11-10 16:08:08: Loss and accuracy at step 2808: 0.0368797, 0.0114\n",
      "2017-11-10 16:08:10: Loss and accuracy at step 2809: 0.0368182, 0.0135\n",
      "2017-11-10 16:08:12: Loss and accuracy at step 2810: 0.0368383, 0.012\n",
      "2017-11-10 16:08:15: Loss and accuracy at step 2811: 0.0368294, 0.0143\n",
      "2017-11-10 16:08:17: Loss and accuracy at step 2812: 0.0369005, 0.0102\n",
      "2017-11-10 16:08:19: Loss and accuracy at step 2813: 0.036831, 0.0166\n",
      "2017-11-10 16:08:21: Loss and accuracy at step 2814: 0.0369347, 0.0104\n",
      "2017-11-10 16:08:23: Loss and accuracy at step 2815: 0.0368834, 0.0118\n",
      "2017-11-10 16:08:25: Loss and accuracy at step 2816: 0.0368223, 0.0187\n",
      "2017-11-10 16:08:27: Loss and accuracy at step 2817: 0.0368408, 0.0101\n",
      "2017-11-10 16:08:29: Loss and accuracy at step 2818: 0.0368802, 0.0132\n",
      "2017-11-10 16:08:31: Loss and accuracy at step 2819: 0.0368171, 0.0174\n",
      "2017-11-10 16:08:33: Loss and accuracy at step 2820: 0.0369193, 0.0114\n",
      "2017-11-10 16:08:35: Loss and accuracy at step 2821: 0.0368614, 0.0141\n",
      "2017-11-10 16:08:37: Loss and accuracy at step 2822: 0.0368722, 0.0146\n",
      "2017-11-10 16:08:40: Loss and accuracy at step 2823: 0.0368729, 0.0082\n",
      "2017-11-10 16:08:42: Loss and accuracy at step 2824: 0.0368849, 0.0167\n",
      "2017-11-10 16:08:44: Loss and accuracy at step 2825: 0.0368657, 0.0096\n",
      "2017-11-10 16:08:46: Loss and accuracy at step 2826: 0.0368633, 0.0105\n",
      "2017-11-10 16:08:48: Loss and accuracy at step 2827: 0.0369158, 0.0185\n",
      "2017-11-10 16:08:50: Loss and accuracy at step 2828: 0.0368821, 0.0083\n",
      "2017-11-10 16:08:52: Loss and accuracy at step 2829: 0.0369193, 0.0084\n",
      "2017-11-10 16:08:54: Loss and accuracy at step 2830: 0.0368878, 0.0149\n",
      "2017-11-10 16:08:56: Loss and accuracy at step 2831: 0.0369034, 0.0124\n",
      "2017-11-10 16:08:58: Loss and accuracy at step 2832: 0.0368817, 0.0099\n",
      "2017-11-10 16:09:00: Loss and accuracy at step 2833: 0.0368638, 0.0085\n",
      "2017-11-10 16:09:03: Loss and accuracy at step 2834: 0.0368991, 0.0192\n",
      "2017-11-10 16:09:05: Loss and accuracy at step 2835: 0.0368862, 0.0117\n",
      "2017-11-10 16:09:07: Loss and accuracy at step 2836: 0.0368454, 0.0106\n",
      "2017-11-10 16:09:09: Loss and accuracy at step 2837: 0.0368467, 0.0137\n",
      "2017-11-10 16:09:11: Loss and accuracy at step 2838: 0.0369071, 0.0109\n",
      "2017-11-10 16:09:13: Loss and accuracy at step 2839: 0.036979, 0.0119\n",
      "2017-11-10 16:09:15: Loss and accuracy at step 2840: 0.0368203, 0.0128\n",
      "2017-11-10 16:09:17: Loss and accuracy at step 2841: 0.0368995, 0.0081\n",
      "2017-11-10 16:09:19: Loss and accuracy at step 2842: 0.0368798, 0.0142\n",
      "2017-11-10 16:09:21: Loss and accuracy at step 2843: 0.0369401, 0.0101\n",
      "2017-11-10 16:09:23: Loss and accuracy at step 2844: 0.0368567, 0.0118\n",
      "2017-11-10 16:09:25: Loss and accuracy at step 2845: 0.0369536, 0.0103\n",
      "2017-11-10 16:09:28: Loss and accuracy at step 2846: 0.0368807, 0.0083\n",
      "2017-11-10 16:09:30: Loss and accuracy at step 2847: 0.0369199, 0.0187\n",
      "2017-11-10 16:09:32: Loss and accuracy at step 2848: 0.0369177, 0.0057\n",
      "2017-11-10 16:09:34: Loss and accuracy at step 2849: 0.0368848, 0.0175\n",
      "2017-11-10 16:09:36: Loss and accuracy at step 2850: 0.0368907, 0.0103\n",
      "2017-11-10 16:09:38: Loss and accuracy at step 2851: 0.0368629, 0.0075\n",
      "2017-11-10 16:09:40: Loss and accuracy at step 2852: 0.0368641, 0.0186\n",
      "2017-11-10 16:09:42: Loss and accuracy at step 2853: 0.036839, 0.006\n",
      "2017-11-10 16:09:44: Loss and accuracy at step 2854: 0.036881, 0.0163\n",
      "2017-11-10 16:09:46: Loss and accuracy at step 2855: 0.0368624, 0.0096\n",
      "2017-11-10 16:09:48: Loss and accuracy at step 2856: 0.0368175, 0.0162\n",
      "2017-11-10 16:09:51: Loss and accuracy at step 2857: 0.0368127, 0.0161\n",
      "2017-11-10 16:09:53: Loss and accuracy at step 2858: 0.0369183, 0.007\n",
      "2017-11-10 16:09:55: Loss and accuracy at step 2859: 0.0369127, 0.0265\n",
      "2017-11-10 16:09:57: Loss and accuracy at step 2860: 0.0368714, 0.0075\n",
      "2017-11-10 16:09:59: Loss and accuracy at step 2861: 0.036755, 0.0108\n",
      "2017-11-10 16:10:01: Loss and accuracy at step 2862: 0.0368971, 0.018\n",
      "2017-11-10 16:10:03: Loss and accuracy at step 2863: 0.0369146, 0.0081\n",
      "2017-11-10 16:10:05: Loss and accuracy at step 2864: 0.0369535, 0.0131\n",
      "2017-11-10 16:10:07: Loss and accuracy at step 2865: 0.0368537, 0.0187\n",
      "2017-11-10 16:10:09: Loss and accuracy at step 2866: 0.0368486, 0.0108\n",
      "2017-11-10 16:10:11: Loss and accuracy at step 2867: 0.0368846, 0.0119\n",
      "2017-11-10 16:10:14: Loss and accuracy at step 2868: 0.0367928, 0.0142\n",
      "2017-11-10 16:10:16: Loss and accuracy at step 2869: 0.0369052, 0.0124\n",
      "2017-11-10 16:10:18: Loss and accuracy at step 2870: 0.0368677, 0.0111\n",
      "2017-11-10 16:10:20: Loss and accuracy at step 2871: 0.0368615, 0.0119\n",
      "2017-11-10 16:10:22: Loss and accuracy at step 2872: 0.0368462, 0.013\n",
      "2017-11-10 16:10:24: Loss and accuracy at step 2873: 0.0369084, 0.0109\n",
      "2017-11-10 16:10:26: Loss and accuracy at step 2874: 0.0368672, 0.0125\n",
      "2017-11-10 16:10:28: Loss and accuracy at step 2875: 0.0368896, 0.0096\n",
      "2017-11-10 16:10:30: Loss and accuracy at step 2876: 0.0368289, 0.0103\n",
      "2017-11-10 16:10:32: Loss and accuracy at step 2877: 0.036918, 0.0167\n",
      "2017-11-10 16:10:34: Loss and accuracy at step 2878: 0.0367575, 0.0084\n",
      "2017-11-10 16:10:37: Loss and accuracy at step 2879: 0.036857, 0.0121\n",
      "2017-11-10 16:10:39: Loss and accuracy at step 2880: 0.0368455, 0.0129\n",
      "2017-11-10 16:10:41: Loss and accuracy at step 2881: 0.036844, 0.0151\n",
      "2017-11-10 16:10:43: Loss and accuracy at step 2882: 0.03691, 0.0118\n",
      "2017-11-10 16:10:45: Loss and accuracy at step 2883: 0.0368976, 0.0143\n",
      "2017-11-10 16:10:47: Loss and accuracy at step 2884: 0.0369824, 0.0113\n",
      "2017-11-10 16:10:49: Loss and accuracy at step 2885: 0.0370291, 0.0096\n",
      "2017-11-10 16:10:51: Loss and accuracy at step 2886: 0.0369378, 0.0149\n",
      "2017-11-10 16:10:53: Loss and accuracy at step 2887: 0.0369026, 0.0108\n",
      "2017-11-10 16:10:55: Loss and accuracy at step 2888: 0.0369004, 0.0121\n",
      "2017-11-10 16:10:57: Loss and accuracy at step 2889: 0.0368478, 0.0128\n",
      "2017-11-10 16:10:59: Loss and accuracy at step 2890: 0.0368844, 0.0131\n",
      "2017-11-10 16:11:02: Loss and accuracy at step 2891: 0.0369768, 0.0103\n",
      "2017-11-10 16:11:04: Loss and accuracy at step 2892: 0.036825, 0.013\n",
      "2017-11-10 16:11:06: Loss and accuracy at step 2893: 0.0368748, 0.0134\n",
      "2017-11-10 16:11:08: Loss and accuracy at step 2894: 0.0368786, 0.0157\n",
      "2017-11-10 16:11:10: Loss and accuracy at step 2895: 0.0368472, 0.0123\n",
      "2017-11-10 16:11:12: Loss and accuracy at step 2896: 0.0369148, 0.0134\n",
      "2017-11-10 16:11:14: Loss and accuracy at step 2897: 0.036849, 0.0152\n",
      "2017-11-10 16:11:16: Loss and accuracy at step 2898: 0.036881, 0.0099\n",
      "2017-11-10 16:11:18: Loss and accuracy at step 2899: 0.0369272, 0.0115\n",
      "2017-11-10 16:11:20: Loss and accuracy at step 2900: 0.0368848, 0.0152\n",
      "2017-11-10 16:11:22: Loss and accuracy at step 2901: 0.0368472, 0.0109\n",
      "2017-11-10 16:11:25: Loss and accuracy at step 2902: 0.0368666, 0.0109\n",
      "2017-11-10 16:11:27: Loss and accuracy at step 2903: 0.0368004, 0.0162\n",
      "2017-11-10 16:11:29: Loss and accuracy at step 2904: 0.0368913, 0.0081\n",
      "2017-11-10 16:11:31: Loss and accuracy at step 2905: 0.0368058, 0.0137\n",
      "2017-11-10 16:11:33: Loss and accuracy at step 2906: 0.0368209, 0.0103\n",
      "2017-11-10 16:11:35: Loss and accuracy at step 2907: 0.0369061, 0.0094\n",
      "2017-11-10 16:11:37: Loss and accuracy at step 2908: 0.0369179, 0.0149\n",
      "2017-11-10 16:11:39: Loss and accuracy at step 2909: 0.0369057, 0.0086\n",
      "2017-11-10 16:11:41: Loss and accuracy at step 2910: 0.0368999, 0.0107\n",
      "2017-11-10 16:11:43: Loss and accuracy at step 2911: 0.0368001, 0.0112\n",
      "2017-11-10 16:11:45: Loss and accuracy at step 2912: 0.0369146, 0.0122\n",
      "2017-11-10 16:11:47: Loss and accuracy at step 2913: 0.0368726, 0.0105\n",
      "2017-11-10 16:11:50: Loss and accuracy at step 2914: 0.0368351, 0.0154\n",
      "2017-11-10 16:11:52: Loss and accuracy at step 2915: 0.0368265, 0.0102\n",
      "2017-11-10 16:11:54: Loss and accuracy at step 2916: 0.0368396, 0.0094\n",
      "2017-11-10 16:11:56: Loss and accuracy at step 2917: 0.0367972, 0.0145\n",
      "2017-11-10 16:11:58: Loss and accuracy at step 2918: 0.036828, 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:12:00: Loss and accuracy at step 2919: 0.0368818, 0.0132\n",
      "2017-11-10 16:12:02: Loss and accuracy at step 2920: 0.0368582, 0.0106\n",
      "2017-11-10 16:12:04: Loss and accuracy at step 2921: 0.0368015, 0.0135\n",
      "2017-11-10 16:12:06: Loss and accuracy at step 2922: 0.0368022, 0.0118\n",
      "2017-11-10 16:12:08: Loss and accuracy at step 2923: 0.0368593, 0.0133\n",
      "2017-11-10 16:12:11: Loss and accuracy at step 2924: 0.0368285, 0.014\n",
      "2017-11-10 16:12:13: Loss and accuracy at step 2925: 0.0368113, 0.0081\n",
      "2017-11-10 16:12:15: Loss and accuracy at step 2926: 0.0368168, 0.0133\n",
      "2017-11-10 16:12:17: Loss and accuracy at step 2927: 0.0368184, 0.0104\n",
      "2017-11-10 16:12:19: Loss and accuracy at step 2928: 0.036831, 0.0143\n",
      "2017-11-10 16:12:21: Loss and accuracy at step 2929: 0.0367782, 0.009\n",
      "2017-11-10 16:12:23: Loss and accuracy at step 2930: 0.0368363, 0.0162\n",
      "2017-11-10 16:12:25: Loss and accuracy at step 2931: 0.036805, 0.0114\n",
      "2017-11-10 16:12:27: Loss and accuracy at step 2932: 0.0369387, 0.0097\n",
      "2017-11-10 16:12:30: Loss and accuracy at step 2933: 0.0368657, 0.0145\n",
      "2017-11-10 16:12:32: Loss and accuracy at step 2934: 0.0369061, 0.0091\n",
      "2017-11-10 16:12:34: Loss and accuracy at step 2935: 0.0369342, 0.0134\n",
      "2017-11-10 16:12:36: Loss and accuracy at step 2936: 0.0369037, 0.0146\n",
      "2017-11-10 16:12:38: Loss and accuracy at step 2937: 0.036845, 0.008\n",
      "2017-11-10 16:12:40: Loss and accuracy at step 2938: 0.0369185, 0.0181\n",
      "2017-11-10 16:12:42: Loss and accuracy at step 2939: 0.036887, 0.0095\n",
      "2017-11-10 16:12:44: Loss and accuracy at step 2940: 0.036863, 0.0119\n",
      "2017-11-10 16:12:46: Loss and accuracy at step 2941: 0.0369441, 0.0144\n",
      "2017-11-10 16:12:48: Loss and accuracy at step 2942: 0.0368598, 0.0074\n",
      "2017-11-10 16:12:50: Loss and accuracy at step 2943: 0.0370045, 0.0133\n",
      "2017-11-10 16:12:52: Loss and accuracy at step 2944: 0.0369341, 0.0128\n",
      "2017-11-10 16:12:54: Loss and accuracy at step 2945: 0.036983, 0.0144\n",
      "2017-11-10 16:12:57: Loss and accuracy at step 2946: 0.0368803, 0.0075\n",
      "2017-11-10 16:12:59: Loss and accuracy at step 2947: 0.0368692, 0.0115\n",
      "2017-11-10 16:13:01: Loss and accuracy at step 2948: 0.0368594, 0.0116\n",
      "2017-11-10 16:13:03: Loss and accuracy at step 2949: 0.0369562, 0.0113\n",
      "2017-11-10 16:13:05: Loss and accuracy at step 2950: 0.0368411, 0.0114\n",
      "2017-11-10 16:13:07: Loss and accuracy at step 2951: 0.0368524, 0.0078\n",
      "2017-11-10 16:13:09: Loss and accuracy at step 2952: 0.0369532, 0.0228\n",
      "2017-11-10 16:13:11: Loss and accuracy at step 2953: 0.0368685, 0.0102\n",
      "2017-11-10 16:13:13: Loss and accuracy at step 2954: 0.0368405, 0.0091\n",
      "2017-11-10 16:13:15: Loss and accuracy at step 2955: 0.036851, 0.0218\n",
      "2017-11-10 16:13:17: Loss and accuracy at step 2956: 0.0368499, 0.0052\n",
      "2017-11-10 16:13:19: Loss and accuracy at step 2957: 0.0368661, 0.0164\n",
      "2017-11-10 16:13:22: Loss and accuracy at step 2958: 0.0368727, 0.0134\n",
      "2017-11-10 16:13:24: Loss and accuracy at step 2959: 0.0368891, 0.0112\n",
      "2017-11-10 16:13:26: Loss and accuracy at step 2960: 0.036874, 0.0121\n",
      "2017-11-10 16:13:28: Loss and accuracy at step 2961: 0.0368639, 0.0103\n",
      "2017-11-10 16:13:30: Loss and accuracy at step 2962: 0.0368332, 0.0152\n",
      "2017-11-10 16:13:32: Loss and accuracy at step 2963: 0.0368858, 0.0057\n",
      "2017-11-10 16:13:34: Loss and accuracy at step 2964: 0.0369541, 0.0111\n",
      "2017-11-10 16:13:36: Loss and accuracy at step 2965: 0.0368587, 0.0169\n",
      "2017-11-10 16:13:38: Loss and accuracy at step 2966: 0.0369415, 0.0092\n",
      "2017-11-10 16:13:40: Loss and accuracy at step 2967: 0.0367767, 0.0137\n",
      "2017-11-10 16:13:42: Loss and accuracy at step 2968: 0.0367818, 0.0114\n",
      "2017-11-10 16:13:45: Loss and accuracy at step 2969: 0.0368747, 0.0151\n",
      "2017-11-10 16:13:47: Loss and accuracy at step 2970: 0.0368938, 0.009\n",
      "2017-11-10 16:13:49: Loss and accuracy at step 2971: 0.0369436, 0.0102\n",
      "2017-11-10 16:13:51: Loss and accuracy at step 2972: 0.036825, 0.0175\n",
      "2017-11-10 16:13:53: Loss and accuracy at step 2973: 0.0368883, 0.0088\n",
      "2017-11-10 16:13:55: Loss and accuracy at step 2974: 0.036803, 0.0134\n",
      "2017-11-10 16:13:57: Loss and accuracy at step 2975: 0.0368664, 0.0105\n",
      "2017-11-10 16:13:59: Loss and accuracy at step 2976: 0.0369472, 0.0126\n",
      "2017-11-10 16:14:01: Loss and accuracy at step 2977: 0.0368463, 0.0126\n",
      "2017-11-10 16:14:03: Loss and accuracy at step 2978: 0.036915, 0.0111\n",
      "2017-11-10 16:14:05: Loss and accuracy at step 2979: 0.0367684, 0.0118\n",
      "2017-11-10 16:14:08: Loss and accuracy at step 2980: 0.0368738, 0.0114\n",
      "2017-11-10 16:14:10: Loss and accuracy at step 2981: 0.0368291, 0.0127\n",
      "2017-11-10 16:14:12: Loss and accuracy at step 2982: 0.0369338, 0.0118\n",
      "2017-11-10 16:14:14: Loss and accuracy at step 2983: 0.0369105, 0.0109\n",
      "2017-11-10 16:14:16: Loss and accuracy at step 2984: 0.0368462, 0.0152\n",
      "2017-11-10 16:14:18: Loss and accuracy at step 2985: 0.0368203, 0.0092\n",
      "2017-11-10 16:14:20: Loss and accuracy at step 2986: 0.0368099, 0.0126\n",
      "2017-11-10 16:14:22: Loss and accuracy at step 2987: 0.0367904, 0.0093\n",
      "2017-11-10 16:14:24: Loss and accuracy at step 2988: 0.0368558, 0.017\n",
      "2017-11-10 16:14:26: Loss and accuracy at step 2989: 0.0368186, 0.009\n",
      "2017-11-10 16:14:29: Loss and accuracy at step 2990: 0.0367959, 0.0123\n",
      "2017-11-10 16:14:31: Loss and accuracy at step 2991: 0.0367766, 0.0122\n",
      "2017-11-10 16:14:33: Loss and accuracy at step 2992: 0.0368049, 0.0102\n",
      "2017-11-10 16:14:35: Loss and accuracy at step 2993: 0.0369358, 0.0107\n",
      "2017-11-10 16:14:37: Loss and accuracy at step 2994: 0.0368808, 0.0105\n",
      "2017-11-10 16:14:39: Loss and accuracy at step 2995: 0.0368729, 0.0118\n",
      "2017-11-10 16:14:41: Loss and accuracy at step 2996: 0.0368231, 0.0105\n",
      "2017-11-10 16:14:43: Loss and accuracy at step 2997: 0.0367891, 0.0134\n",
      "2017-11-10 16:14:45: Loss and accuracy at step 2998: 0.0368219, 0.0077\n",
      "2017-11-10 16:14:47: Loss and accuracy at step 2999: 0.0368718, 0.0108\n",
      "2017-11-10 16:14:49: Loss and accuracy at step 3000: 0.0368924, 0.017\n",
      "2017-11-10 16:14:52: Loss and accuracy at step 3001: 0.0368934, 0.0089\n",
      "2017-11-10 16:14:54: Loss and accuracy at step 3002: 0.0368094, 0.0153\n",
      "2017-11-10 16:14:56: Loss and accuracy at step 3003: 0.0368628, 0.0121\n",
      "2017-11-10 16:14:58: Loss and accuracy at step 3004: 0.0368423, 0.0104\n",
      "2017-11-10 16:15:00: Loss and accuracy at step 3005: 0.0368359, 0.0136\n",
      "2017-11-10 16:15:02: Loss and accuracy at step 3006: 0.036897, 0.0096\n",
      "2017-11-10 16:15:04: Loss and accuracy at step 3007: 0.0368809, 0.011\n",
      "2017-11-10 16:15:06: Loss and accuracy at step 3008: 0.0369082, 0.0122\n",
      "2017-11-10 16:15:08: Loss and accuracy at step 3009: 0.0367967, 0.011\n",
      "2017-11-10 16:15:10: Loss and accuracy at step 3010: 0.0367538, 0.0122\n",
      "2017-11-10 16:15:12: Loss and accuracy at step 3011: 0.0369003, 0.0103\n",
      "2017-11-10 16:15:15: Loss and accuracy at step 3012: 0.0368606, 0.0094\n",
      "2017-11-10 16:15:17: Loss and accuracy at step 3013: 0.0368979, 0.0103\n",
      "2017-11-10 16:15:19: Loss and accuracy at step 3014: 0.0369223, 0.015\n",
      "2017-11-10 16:15:21: Loss and accuracy at step 3015: 0.0368328, 0.0095\n",
      "2017-11-10 16:15:23: Loss and accuracy at step 3016: 0.0368546, 0.0106\n",
      "2017-11-10 16:15:25: Loss and accuracy at step 3017: 0.0368907, 0.0123\n",
      "2017-11-10 16:15:27: Loss and accuracy at step 3018: 0.0368709, 0.0112\n",
      "2017-11-10 16:15:29: Loss and accuracy at step 3019: 0.0367677, 0.0101\n",
      "2017-11-10 16:15:31: Loss and accuracy at step 3020: 0.0369444, 0.0093\n",
      "2017-11-10 16:15:33: Loss and accuracy at step 3021: 0.0368613, 0.0212\n",
      "2017-11-10 16:15:35: Loss and accuracy at step 3022: 0.0367651, 0.0079\n",
      "2017-11-10 16:15:38: Loss and accuracy at step 3023: 0.0368466, 0.0103\n",
      "2017-11-10 16:15:40: Loss and accuracy at step 3024: 0.0368594, 0.0188\n",
      "2017-11-10 16:15:42: Loss and accuracy at step 3025: 0.0368649, 0.0075\n",
      "2017-11-10 16:15:44: Loss and accuracy at step 3026: 0.0368579, 0.0162\n",
      "2017-11-10 16:15:46: Loss and accuracy at step 3027: 0.0368681, 0.0118\n",
      "2017-11-10 16:15:48: Loss and accuracy at step 3028: 0.0368269, 0.0098\n",
      "2017-11-10 16:15:50: Loss and accuracy at step 3029: 0.0369404, 0.0141\n",
      "2017-11-10 16:15:52: Loss and accuracy at step 3030: 0.0368241, 0.0088\n",
      "2017-11-10 16:15:54: Loss and accuracy at step 3031: 0.0368599, 0.0204\n",
      "2017-11-10 16:15:56: Loss and accuracy at step 3032: 0.0368371, 0.0078\n",
      "2017-11-10 16:15:58: Loss and accuracy at step 3033: 0.0368886, 0.0156\n",
      "2017-11-10 16:16:00: Loss and accuracy at step 3034: 0.0369719, 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:16:03: Loss and accuracy at step 3035: 0.0369025, 0.0104\n",
      "2017-11-10 16:16:05: Loss and accuracy at step 3036: 0.0369864, 0.0105\n",
      "2017-11-10 16:16:07: Loss and accuracy at step 3037: 0.0368788, 0.0178\n",
      "2017-11-10 16:16:09: Loss and accuracy at step 3038: 0.0367887, 0.0119\n",
      "2017-11-10 16:16:11: Loss and accuracy at step 3039: 0.0368554, 0.0092\n",
      "2017-11-10 16:16:13: Loss and accuracy at step 3040: 0.0369137, 0.0167\n",
      "2017-11-10 16:16:15: Loss and accuracy at step 3041: 0.0369135, 0.0083\n",
      "2017-11-10 16:16:17: Loss and accuracy at step 3042: 0.0368644, 0.0145\n",
      "2017-11-10 16:16:19: Loss and accuracy at step 3043: 0.036948, 0.0103\n",
      "2017-11-10 16:16:22: Loss and accuracy at step 3044: 0.0368672, 0.0102\n",
      "2017-11-10 16:16:24: Loss and accuracy at step 3045: 0.0369317, 0.0158\n",
      "2017-11-10 16:16:26: Loss and accuracy at step 3046: 0.036777, 0.0071\n",
      "2017-11-10 16:16:28: Loss and accuracy at step 3047: 0.0368638, 0.0155\n",
      "2017-11-10 16:16:30: Loss and accuracy at step 3048: 0.036832, 0.0102\n",
      "2017-11-10 16:16:32: Loss and accuracy at step 3049: 0.0368347, 0.0095\n",
      "2017-11-10 16:16:34: Loss and accuracy at step 3050: 0.0369521, 0.0205\n",
      "2017-11-10 16:16:36: Loss and accuracy at step 3051: 0.0369698, 0.0061\n",
      "2017-11-10 16:16:38: Loss and accuracy at step 3052: 0.0368306, 0.0172\n",
      "2017-11-10 16:16:40: Loss and accuracy at step 3053: 0.0368946, 0.0112\n",
      "2017-11-10 16:16:42: Loss and accuracy at step 3054: 0.0368656, 0.0091\n",
      "2017-11-10 16:16:44: Loss and accuracy at step 3055: 0.0368913, 0.0158\n",
      "2017-11-10 16:16:47: Loss and accuracy at step 3056: 0.0368879, 0.0095\n",
      "2017-11-10 16:16:49: Loss and accuracy at step 3057: 0.0369112, 0.0118\n",
      "2017-11-10 16:16:51: Loss and accuracy at step 3058: 0.0368791, 0.0092\n",
      "2017-11-10 16:16:53: Loss and accuracy at step 3059: 0.0368428, 0.0081\n",
      "2017-11-10 16:16:55: Loss and accuracy at step 3060: 0.0369302, 0.0149\n",
      "2017-11-10 16:16:57: Loss and accuracy at step 3061: 0.0368203, 0.007\n",
      "2017-11-10 16:16:59: Loss and accuracy at step 3062: 0.0368143, 0.0165\n",
      "2017-11-10 16:17:01: Loss and accuracy at step 3063: 0.0368317, 0.0082\n",
      "2017-11-10 16:17:03: Loss and accuracy at step 3064: 0.0368395, 0.0144\n",
      "2017-11-10 16:17:05: Loss and accuracy at step 3065: 0.0368265, 0.0137\n",
      "2017-11-10 16:17:07: Loss and accuracy at step 3066: 0.0368566, 0.0057\n",
      "2017-11-10 16:17:09: Loss and accuracy at step 3067: 0.0368936, 0.0234\n",
      "2017-11-10 16:17:12: Loss and accuracy at step 3068: 0.0369739, 0.0078\n",
      "2017-11-10 16:17:14: Loss and accuracy at step 3069: 0.0369041, 0.0116\n",
      "2017-11-10 16:17:16: Loss and accuracy at step 3070: 0.0368483, 0.019\n",
      "2017-11-10 16:17:18: Loss and accuracy at step 3071: 0.0368291, 0.0071\n",
      "2017-11-10 16:17:20: Loss and accuracy at step 3072: 0.0369114, 0.0235\n",
      "2017-11-10 16:17:22: Loss and accuracy at step 3073: 0.0369057, 0.0077\n",
      "2017-11-10 16:17:24: Loss and accuracy at step 3074: 0.036802, 0.0101\n",
      "2017-11-10 16:17:26: Loss and accuracy at step 3075: 0.0368928, 0.0356\n",
      "2017-11-10 16:17:28: Loss and accuracy at step 3076: 0.0368135, 0.0069\n",
      "2017-11-10 16:17:30: Loss and accuracy at step 3077: 0.0368697, 0.0134\n",
      "2017-11-10 16:17:32: Loss and accuracy at step 3078: 0.0368975, 0.0302\n",
      "2017-11-10 16:17:35: Loss and accuracy at step 3079: 0.0369546, 0.0098\n",
      "2017-11-10 16:17:37: Loss and accuracy at step 3080: 0.0368781, 0.0108\n",
      "2017-11-10 16:17:39: Loss and accuracy at step 3081: 0.0369056, 0.0129\n",
      "2017-11-10 16:17:41: Loss and accuracy at step 3082: 0.0369096, 0.0153\n",
      "2017-11-10 16:17:43: Loss and accuracy at step 3083: 0.0369415, 0.0095\n",
      "2017-11-10 16:17:45: Loss and accuracy at step 3084: 0.0369757, 0.0091\n",
      "2017-11-10 16:17:47: Loss and accuracy at step 3085: 0.0368609, 0.0152\n",
      "2017-11-10 16:17:49: Loss and accuracy at step 3086: 0.0368956, 0.0078\n",
      "2017-11-10 16:17:51: Loss and accuracy at step 3087: 0.0368662, 0.0146\n",
      "2017-11-10 16:17:53: Loss and accuracy at step 3088: 0.0368303, 0.0117\n",
      "2017-11-10 16:17:55: Loss and accuracy at step 3089: 0.0367994, 0.0093\n",
      "2017-11-10 16:17:58: Loss and accuracy at step 3090: 0.0368658, 0.0148\n",
      "2017-11-10 16:18:00: Loss and accuracy at step 3091: 0.0368639, 0.008\n",
      "2017-11-10 16:18:02: Loss and accuracy at step 3092: 0.0369068, 0.023\n",
      "2017-11-10 16:18:04: Loss and accuracy at step 3093: 0.0368727, 0.0073\n",
      "2017-11-10 16:18:06: Loss and accuracy at step 3094: 0.0368141, 0.0112\n",
      "2017-11-10 16:18:08: Loss and accuracy at step 3095: 0.0368787, 0.0225\n",
      "2017-11-10 16:18:10: Loss and accuracy at step 3096: 0.0368699, 0.0089\n",
      "2017-11-10 16:18:12: Loss and accuracy at step 3097: 0.0369065, 0.0109\n",
      "2017-11-10 16:18:14: Loss and accuracy at step 3098: 0.0368746, 0.0265\n",
      "2017-11-10 16:18:16: Loss and accuracy at step 3099: 0.0368265, 0.0084\n",
      "2017-11-10 16:18:19: Loss and accuracy at step 3100: 0.0368393, 0.0123\n",
      "2017-11-10 16:18:21: Loss and accuracy at step 3101: 0.0367593, 0.0132\n",
      "2017-11-10 16:18:23: Loss and accuracy at step 3102: 0.0368641, 0.0127\n",
      "2017-11-10 16:18:25: Loss and accuracy at step 3103: 0.0368714, 0.0109\n",
      "2017-11-10 16:18:27: Loss and accuracy at step 3104: 0.0368806, 0.0104\n",
      "2017-11-10 16:18:29: Loss and accuracy at step 3105: 0.0368637, 0.0115\n",
      "2017-11-10 16:18:31: Loss and accuracy at step 3106: 0.0367951, 0.011\n",
      "2017-11-10 16:18:33: Loss and accuracy at step 3107: 0.0369115, 0.0093\n",
      "2017-11-10 16:18:35: Loss and accuracy at step 3108: 0.0369165, 0.0108\n",
      "2017-11-10 16:18:37: Loss and accuracy at step 3109: 0.0368522, 0.0146\n",
      "2017-11-10 16:18:39: Loss and accuracy at step 3110: 0.0368689, 0.0099\n",
      "2017-11-10 16:18:41: Loss and accuracy at step 3111: 0.0368504, 0.0082\n",
      "2017-11-10 16:18:44: Loss and accuracy at step 3112: 0.0369427, 0.0125\n",
      "2017-11-10 16:18:46: Loss and accuracy at step 3113: 0.0368461, 0.0098\n",
      "2017-11-10 16:18:48: Loss and accuracy at step 3114: 0.0369284, 0.0119\n",
      "2017-11-10 16:18:50: Loss and accuracy at step 3115: 0.0368503, 0.0168\n",
      "2017-11-10 16:18:52: Loss and accuracy at step 3116: 0.0368998, 0.0077\n",
      "2017-11-10 16:18:54: Loss and accuracy at step 3117: 0.0369798, 0.0107\n",
      "2017-11-10 16:18:56: Loss and accuracy at step 3118: 0.0368863, 0.0222\n",
      "2017-11-10 16:18:58: Loss and accuracy at step 3119: 0.0369735, 0.007\n",
      "2017-11-10 16:19:00: Loss and accuracy at step 3120: 0.0368886, 0.0159\n",
      "2017-11-10 16:19:02: Loss and accuracy at step 3121: 0.036854, 0.015\n",
      "2017-11-10 16:19:04: Loss and accuracy at step 3122: 0.0368265, 0.0136\n",
      "2017-11-10 16:19:07: Loss and accuracy at step 3123: 0.0368527, 0.0115\n",
      "2017-11-10 16:19:09: Loss and accuracy at step 3124: 0.0368574, 0.0122\n",
      "2017-11-10 16:19:11: Loss and accuracy at step 3125: 0.0368403, 0.0164\n",
      "2017-11-10 16:19:13: Loss and accuracy at step 3126: 0.0368507, 0.0077\n",
      "2017-11-10 16:19:15: Loss and accuracy at step 3127: 0.0368939, 0.0144\n",
      "2017-11-10 16:19:17: Loss and accuracy at step 3128: 0.0369021, 0.0129\n",
      "2017-11-10 16:19:19: Loss and accuracy at step 3129: 0.0368899, 0.01\n",
      "2017-11-10 16:19:21: Loss and accuracy at step 3130: 0.0368904, 0.0124\n",
      "2017-11-10 16:19:23: Loss and accuracy at step 3131: 0.0368979, 0.0105\n",
      "2017-11-10 16:19:25: Loss and accuracy at step 3132: 0.0367862, 0.0178\n",
      "2017-11-10 16:19:27: Loss and accuracy at step 3133: 0.0368982, 0.0058\n",
      "2017-11-10 16:19:29: Loss and accuracy at step 3134: 0.0369777, 0.0106\n",
      "2017-11-10 16:19:32: Loss and accuracy at step 3135: 0.0369712, 0.0161\n",
      "2017-11-10 16:19:34: Loss and accuracy at step 3136: 0.0368575, 0.0071\n",
      "2017-11-10 16:19:36: Loss and accuracy at step 3137: 0.0369185, 0.0128\n",
      "2017-11-10 16:19:38: Loss and accuracy at step 3138: 0.0369036, 0.0126\n",
      "2017-11-10 16:19:40: Loss and accuracy at step 3139: 0.0368911, 0.0113\n",
      "2017-11-10 16:19:42: Loss and accuracy at step 3140: 0.0368168, 0.0084\n",
      "2017-11-10 16:19:44: Loss and accuracy at step 3141: 0.0368401, 0.0121\n",
      "2017-11-10 16:19:46: Loss and accuracy at step 3142: 0.0368393, 0.0124\n",
      "2017-11-10 16:19:48: Loss and accuracy at step 3143: 0.0368115, 0.0072\n",
      "2017-11-10 16:19:50: Loss and accuracy at step 3144: 0.0368007, 0.0128\n",
      "2017-11-10 16:19:52: Loss and accuracy at step 3145: 0.0367492, 0.0148\n",
      "2017-11-10 16:19:55: Loss and accuracy at step 3146: 0.03688, 0.0111\n",
      "2017-11-10 16:19:57: Loss and accuracy at step 3147: 0.0368139, 0.0127\n",
      "2017-11-10 16:19:59: Loss and accuracy at step 3148: 0.0368642, 0.0126\n",
      "2017-11-10 16:20:01: Loss and accuracy at step 3149: 0.0368668, 0.0132\n",
      "2017-11-10 16:20:03: Loss and accuracy at step 3150: 0.0368638, 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:20:05: Loss and accuracy at step 3151: 0.0367799, 0.0086\n",
      "2017-11-10 16:20:07: Loss and accuracy at step 3152: 0.0369132, 0.013\n",
      "2017-11-10 16:20:09: Loss and accuracy at step 3153: 0.0368609, 0.0139\n",
      "2017-11-10 16:20:11: Loss and accuracy at step 3154: 0.0369212, 0.0081\n",
      "2017-11-10 16:20:13: Loss and accuracy at step 3155: 0.0368681, 0.0136\n",
      "2017-11-10 16:20:16: Loss and accuracy at step 3156: 0.0368481, 0.0099\n",
      "2017-11-10 16:20:18: Loss and accuracy at step 3157: 0.036941, 0.0079\n",
      "2017-11-10 16:20:20: Loss and accuracy at step 3158: 0.0368349, 0.015\n",
      "2017-11-10 16:20:22: Loss and accuracy at step 3159: 0.0368512, 0.0078\n",
      "2017-11-10 16:20:24: Loss and accuracy at step 3160: 0.0368146, 0.0112\n",
      "2017-11-10 16:20:26: Loss and accuracy at step 3161: 0.0368304, 0.0124\n",
      "2017-11-10 16:20:28: Loss and accuracy at step 3162: 0.0367991, 0.0091\n",
      "2017-11-10 16:20:30: Loss and accuracy at step 3163: 0.0368525, 0.0161\n",
      "2017-11-10 16:20:32: Loss and accuracy at step 3164: 0.0368864, 0.0076\n",
      "2017-11-10 16:20:34: Loss and accuracy at step 3165: 0.0368248, 0.0174\n",
      "2017-11-10 16:20:36: Loss and accuracy at step 3166: 0.0368443, 0.0096\n",
      "2017-11-10 16:20:38: Loss and accuracy at step 3167: 0.0368472, 0.0092\n",
      "2017-11-10 16:20:41: Loss and accuracy at step 3168: 0.0369324, 0.0165\n",
      "2017-11-10 16:20:43: Loss and accuracy at step 3169: 0.036867, 0.006\n",
      "2017-11-10 16:20:45: Loss and accuracy at step 3170: 0.0368548, 0.0135\n",
      "2017-11-10 16:20:47: Loss and accuracy at step 3171: 0.03688, 0.0097\n",
      "2017-11-10 16:20:49: Loss and accuracy at step 3172: 0.0369393, 0.011\n",
      "2017-11-10 16:20:51: Loss and accuracy at step 3173: 0.0368482, 0.0144\n",
      "2017-11-10 16:20:53: Loss and accuracy at step 3174: 0.036984, 0.0073\n",
      "2017-11-10 16:20:55: Loss and accuracy at step 3175: 0.0368908, 0.0125\n",
      "2017-11-10 16:20:57: Loss and accuracy at step 3176: 0.0368199, 0.0149\n",
      "2017-11-10 16:20:59: Loss and accuracy at step 3177: 0.0369207, 0.0074\n",
      "2017-11-10 16:21:01: Loss and accuracy at step 3178: 0.0369019, 0.0154\n",
      "2017-11-10 16:21:03: Loss and accuracy at step 3179: 0.0368437, 0.0149\n",
      "2017-11-10 16:21:06: Loss and accuracy at step 3180: 0.0367877, 0.01\n",
      "2017-11-10 16:21:08: Loss and accuracy at step 3181: 0.0368604, 0.0092\n",
      "2017-11-10 16:21:10: Loss and accuracy at step 3182: 0.0368055, 0.0139\n",
      "2017-11-10 16:21:12: Loss and accuracy at step 3183: 0.0369008, 0.0118\n",
      "2017-11-10 16:21:14: Loss and accuracy at step 3184: 0.0368202, 0.0115\n",
      "2017-11-10 16:21:16: Loss and accuracy at step 3185: 0.036843, 0.0098\n",
      "2017-11-10 16:21:18: Loss and accuracy at step 3186: 0.0369053, 0.012\n",
      "2017-11-10 16:21:20: Loss and accuracy at step 3187: 0.0369072, 0.0112\n",
      "2017-11-10 16:21:22: Loss and accuracy at step 3188: 0.0368774, 0.0094\n",
      "2017-11-10 16:21:24: Loss and accuracy at step 3189: 0.0368965, 0.0144\n",
      "2017-11-10 16:21:26: Loss and accuracy at step 3190: 0.0368497, 0.0093\n",
      "2017-11-10 16:21:28: Loss and accuracy at step 3191: 0.03683, 0.0133\n",
      "2017-11-10 16:21:31: Loss and accuracy at step 3192: 0.0369133, 0.0094\n",
      "2017-11-10 16:21:33: Loss and accuracy at step 3193: 0.0368608, 0.0103\n",
      "2017-11-10 16:21:35: Loss and accuracy at step 3194: 0.0368256, 0.0158\n",
      "2017-11-10 16:21:37: Loss and accuracy at step 3195: 0.0368609, 0.008\n",
      "2017-11-10 16:21:39: Loss and accuracy at step 3196: 0.0368951, 0.0158\n",
      "2017-11-10 16:21:41: Loss and accuracy at step 3197: 0.0368754, 0.0115\n",
      "2017-11-10 16:21:43: Loss and accuracy at step 3198: 0.0369682, 0.0106\n",
      "2017-11-10 16:21:45: Loss and accuracy at step 3199: 0.036854, 0.0172\n",
      "2017-11-10 16:21:47: Loss and accuracy at step 3200: 0.0368385, 0.0095\n",
      "2017-11-10 16:21:49: Loss and accuracy at step 3201: 0.0369621, 0.0118\n",
      "2017-11-10 16:21:51: Loss and accuracy at step 3202: 0.0369045, 0.0168\n",
      "2017-11-10 16:21:53: Loss and accuracy at step 3203: 0.0368309, 0.0117\n",
      "2017-11-10 16:21:56: Loss and accuracy at step 3204: 0.0368208, 0.012\n",
      "2017-11-10 16:21:58: Loss and accuracy at step 3205: 0.0369217, 0.0152\n",
      "2017-11-10 16:22:00: Loss and accuracy at step 3206: 0.0368795, 0.0153\n",
      "2017-11-10 16:22:02: Loss and accuracy at step 3207: 0.0368903, 0.0103\n",
      "2017-11-10 16:22:04: Loss and accuracy at step 3208: 0.0368918, 0.0115\n",
      "2017-11-10 16:22:06: Loss and accuracy at step 3209: 0.0368868, 0.0277\n",
      "2017-11-10 16:22:08: Loss and accuracy at step 3210: 0.0368598, 0.0106\n",
      "2017-11-10 16:22:10: Loss and accuracy at step 3211: 0.0369572, 0.0101\n",
      "2017-11-10 16:22:12: Loss and accuracy at step 3212: 0.0369136, 0.0511\n",
      "2017-11-10 16:22:15: Loss and accuracy at step 3213: 0.0370431, 0.0094\n",
      "2017-11-10 16:22:17: Loss and accuracy at step 3214: 0.0369101, 0.0111\n",
      "2017-11-10 16:22:19: Loss and accuracy at step 3215: 0.0369008, 0.052\n",
      "2017-11-10 16:22:21: Loss and accuracy at step 3216: 0.0368771, 0.008\n",
      "2017-11-10 16:22:23: Loss and accuracy at step 3217: 0.0368621, 0.0112\n",
      "2017-11-10 16:22:25: Loss and accuracy at step 3218: 0.0368374, 0.0379\n",
      "2017-11-10 16:22:27: Loss and accuracy at step 3219: 0.0369667, 0.0069\n",
      "2017-11-10 16:22:29: Loss and accuracy at step 3220: 0.0369046, 0.0139\n",
      "2017-11-10 16:22:31: Loss and accuracy at step 3221: 0.0368598, 0.024\n",
      "2017-11-10 16:22:33: Loss and accuracy at step 3222: 0.0368375, 0.0107\n",
      "2017-11-10 16:22:35: Loss and accuracy at step 3223: 0.0369439, 0.0165\n",
      "2017-11-10 16:22:38: Loss and accuracy at step 3224: 0.0369012, 0.0179\n",
      "2017-11-10 16:22:40: Loss and accuracy at step 3225: 0.0368709, 0.0085\n",
      "2017-11-10 16:22:42: Loss and accuracy at step 3226: 0.0368308, 0.0144\n",
      "2017-11-10 16:22:44: Loss and accuracy at step 3227: 0.0368414, 0.0136\n",
      "2017-11-10 16:22:46: Loss and accuracy at step 3228: 0.0367978, 0.0084\n",
      "2017-11-10 16:22:48: Loss and accuracy at step 3229: 0.0369012, 0.0158\n",
      "2017-11-10 16:22:50: Loss and accuracy at step 3230: 0.036931, 0.0089\n",
      "2017-11-10 16:22:52: Loss and accuracy at step 3231: 0.0368677, 0.0072\n",
      "2017-11-10 16:22:54: Loss and accuracy at step 3232: 0.0368713, 0.0167\n",
      "2017-11-10 16:22:56: Loss and accuracy at step 3233: 0.0368139, 0.0093\n",
      "2017-11-10 16:22:58: Loss and accuracy at step 3234: 0.0368224, 0.0088\n",
      "2017-11-10 16:23:00: Loss and accuracy at step 3235: 0.0369054, 0.0189\n",
      "2017-11-10 16:23:03: Loss and accuracy at step 3236: 0.0368603, 0.0088\n",
      "2017-11-10 16:23:05: Loss and accuracy at step 3237: 0.0369252, 0.0115\n",
      "2017-11-10 16:23:07: Loss and accuracy at step 3238: 0.036783, 0.0137\n",
      "2017-11-10 16:23:09: Loss and accuracy at step 3239: 0.0368446, 0.0086\n",
      "2017-11-10 16:23:11: Loss and accuracy at step 3240: 0.0368447, 0.0137\n",
      "2017-11-10 16:23:13: Loss and accuracy at step 3241: 0.0368411, 0.0093\n",
      "2017-11-10 16:23:15: Loss and accuracy at step 3242: 0.0367986, 0.0095\n",
      "2017-11-10 16:23:17: Loss and accuracy at step 3243: 0.0368389, 0.0175\n",
      "2017-11-10 16:23:19: Loss and accuracy at step 3244: 0.0369036, 0.0108\n",
      "2017-11-10 16:23:21: Loss and accuracy at step 3245: 0.0368306, 0.0119\n",
      "2017-11-10 16:23:23: Loss and accuracy at step 3246: 0.0368624, 0.0099\n",
      "2017-11-10 16:23:26: Loss and accuracy at step 3247: 0.0369331, 0.0126\n",
      "2017-11-10 16:23:28: Loss and accuracy at step 3248: 0.0368167, 0.0115\n",
      "2017-11-10 16:23:30: Loss and accuracy at step 3249: 0.0369178, 0.0072\n",
      "2017-11-10 16:23:32: Loss and accuracy at step 3250: 0.0368389, 0.0166\n",
      "2017-11-10 16:23:34: Loss and accuracy at step 3251: 0.0368675, 0.0119\n",
      "2017-11-10 16:23:36: Loss and accuracy at step 3252: 0.0368615, 0.0104\n",
      "2017-11-10 16:23:38: Loss and accuracy at step 3253: 0.0369233, 0.0122\n",
      "2017-11-10 16:23:40: Loss and accuracy at step 3254: 0.0368688, 0.0074\n",
      "2017-11-10 16:23:42: Loss and accuracy at step 3255: 0.036925, 0.0181\n",
      "2017-11-10 16:23:44: Loss and accuracy at step 3256: 0.0368602, 0.0088\n",
      "2017-11-10 16:23:46: Loss and accuracy at step 3257: 0.0368498, 0.011\n",
      "2017-11-10 16:23:49: Loss and accuracy at step 3258: 0.0367615, 0.024\n",
      "2017-11-10 16:23:51: Loss and accuracy at step 3259: 0.0368027, 0.0056\n",
      "2017-11-10 16:23:53: Loss and accuracy at step 3260: 0.036842, 0.0126\n",
      "2017-11-10 16:23:55: Loss and accuracy at step 3261: 0.0368065, 0.0165\n",
      "2017-11-10 16:23:57: Loss and accuracy at step 3262: 0.0368209, 0.0083\n",
      "2017-11-10 16:23:59: Loss and accuracy at step 3263: 0.0367933, 0.0155\n",
      "2017-11-10 16:24:01: Loss and accuracy at step 3264: 0.0368294, 0.0136\n",
      "2017-11-10 16:24:03: Loss and accuracy at step 3265: 0.0367107, 0.0091\n",
      "2017-11-10 16:24:05: Loss and accuracy at step 3266: 0.0367879, 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:24:07: Loss and accuracy at step 3267: 0.0368448, 0.0173\n",
      "2017-11-10 16:24:09: Loss and accuracy at step 3268: 0.0368019, 0.0107\n",
      "2017-11-10 16:24:12: Loss and accuracy at step 3269: 0.0368315, 0.0098\n",
      "2017-11-10 16:24:14: Loss and accuracy at step 3270: 0.0368122, 0.0146\n",
      "2017-11-10 16:24:16: Loss and accuracy at step 3271: 0.036864, 0.0114\n",
      "2017-11-10 16:24:18: Loss and accuracy at step 3272: 0.0368749, 0.008\n",
      "2017-11-10 16:24:20: Loss and accuracy at step 3273: 0.0368331, 0.0145\n",
      "2017-11-10 16:24:22: Loss and accuracy at step 3274: 0.0367909, 0.0099\n",
      "2017-11-10 16:24:24: Loss and accuracy at step 3275: 0.0368734, 0.0157\n",
      "2017-11-10 16:24:26: Loss and accuracy at step 3276: 0.0368337, 0.0116\n",
      "2017-11-10 16:24:28: Loss and accuracy at step 3277: 0.0368576, 0.0105\n",
      "2017-11-10 16:24:31: Loss and accuracy at step 3278: 0.036815, 0.0133\n",
      "2017-11-10 16:24:33: Loss and accuracy at step 3279: 0.0368418, 0.0118\n",
      "2017-11-10 16:24:35: Loss and accuracy at step 3280: 0.0368368, 0.0125\n",
      "2017-11-10 16:24:37: Loss and accuracy at step 3281: 0.0368959, 0.0076\n",
      "2017-11-10 16:24:39: Loss and accuracy at step 3282: 0.0368531, 0.0154\n",
      "2017-11-10 16:24:41: Loss and accuracy at step 3283: 0.0367912, 0.0092\n",
      "2017-11-10 16:24:43: Loss and accuracy at step 3284: 0.0369033, 0.0087\n",
      "2017-11-10 16:24:45: Loss and accuracy at step 3285: 0.0368911, 0.0166\n",
      "2017-11-10 16:24:47: Loss and accuracy at step 3286: 0.0368232, 0.0071\n",
      "2017-11-10 16:24:49: Loss and accuracy at step 3287: 0.0368593, 0.0121\n",
      "2017-11-10 16:24:51: Loss and accuracy at step 3288: 0.0368655, 0.0134\n",
      "2017-11-10 16:24:53: Loss and accuracy at step 3289: 0.036814, 0.0097\n",
      "2017-11-10 16:24:56: Loss and accuracy at step 3290: 0.036878, 0.0153\n",
      "2017-11-10 16:24:58: Loss and accuracy at step 3291: 0.0367981, 0.0095\n",
      "2017-11-10 16:25:00: Loss and accuracy at step 3292: 0.0367492, 0.0125\n",
      "2017-11-10 16:25:02: Loss and accuracy at step 3293: 0.0368285, 0.0078\n",
      "2017-11-10 16:25:04: Loss and accuracy at step 3294: 0.0368227, 0.0167\n",
      "2017-11-10 16:25:06: Loss and accuracy at step 3295: 0.0368907, 0.0133\n",
      "2017-11-10 16:25:08: Loss and accuracy at step 3296: 0.0369455, 0.0076\n",
      "2017-11-10 16:25:10: Loss and accuracy at step 3297: 0.0368966, 0.0143\n",
      "2017-11-10 16:25:12: Loss and accuracy at step 3298: 0.0368194, 0.0098\n",
      "2017-11-10 16:25:14: Loss and accuracy at step 3299: 0.0368476, 0.0116\n",
      "2017-11-10 16:25:16: Loss and accuracy at step 3300: 0.0368306, 0.0127\n",
      "2017-11-10 16:25:19: Loss and accuracy at step 3301: 0.036848, 0.0105\n",
      "2017-11-10 16:25:21: Loss and accuracy at step 3302: 0.0367922, 0.0105\n",
      "2017-11-10 16:25:23: Loss and accuracy at step 3303: 0.0367467, 0.0116\n",
      "2017-11-10 16:25:25: Loss and accuracy at step 3304: 0.0367837, 0.0136\n",
      "2017-11-10 16:25:27: Loss and accuracy at step 3305: 0.0368468, 0.0094\n",
      "2017-11-10 16:25:29: Loss and accuracy at step 3306: 0.0368696, 0.0103\n",
      "2017-11-10 16:25:31: Loss and accuracy at step 3307: 0.0368355, 0.0169\n",
      "2017-11-10 16:25:33: Loss and accuracy at step 3308: 0.0368333, 0.0099\n",
      "2017-11-10 16:25:35: Loss and accuracy at step 3309: 0.0369281, 0.0109\n",
      "2017-11-10 16:25:37: Loss and accuracy at step 3310: 0.0368886, 0.0165\n",
      "2017-11-10 16:25:39: Loss and accuracy at step 3311: 0.0369053, 0.0111\n",
      "2017-11-10 16:25:42: Loss and accuracy at step 3312: 0.0368118, 0.0088\n",
      "2017-11-10 16:25:44: Loss and accuracy at step 3313: 0.0368698, 0.0123\n",
      "2017-11-10 16:25:46: Loss and accuracy at step 3314: 0.0369452, 0.0099\n",
      "2017-11-10 16:25:48: Loss and accuracy at step 3315: 0.0369004, 0.0156\n",
      "2017-11-10 16:25:50: Loss and accuracy at step 3316: 0.0368645, 0.0085\n",
      "2017-11-10 16:25:52: Loss and accuracy at step 3317: 0.0368839, 0.0086\n",
      "2017-11-10 16:25:54: Loss and accuracy at step 3318: 0.036822, 0.0119\n",
      "2017-11-10 16:25:56: Loss and accuracy at step 3319: 0.0368658, 0.0102\n",
      "2017-11-10 16:25:58: Loss and accuracy at step 3320: 0.036748, 0.0125\n",
      "2017-11-10 16:26:00: Loss and accuracy at step 3321: 0.0368473, 0.0111\n",
      "2017-11-10 16:26:02: Loss and accuracy at step 3322: 0.0368994, 0.0081\n",
      "2017-11-10 16:26:04: Loss and accuracy at step 3323: 0.0369014, 0.0138\n",
      "2017-11-10 16:26:07: Loss and accuracy at step 3324: 0.0368354, 0.0113\n",
      "2017-11-10 16:26:09: Loss and accuracy at step 3325: 0.0368899, 0.0096\n",
      "2017-11-10 16:26:11: Loss and accuracy at step 3326: 0.0368556, 0.0122\n",
      "2017-11-10 16:26:13: Loss and accuracy at step 3327: 0.0368453, 0.0154\n",
      "2017-11-10 16:26:15: Loss and accuracy at step 3328: 0.0368831, 0.0094\n",
      "2017-11-10 16:26:17: Loss and accuracy at step 3329: 0.0367894, 0.0124\n",
      "2017-11-10 16:26:19: Loss and accuracy at step 3330: 0.03688, 0.0198\n",
      "2017-11-10 16:26:21: Loss and accuracy at step 3331: 0.0368472, 0.0085\n",
      "2017-11-10 16:26:23: Loss and accuracy at step 3332: 0.0368728, 0.0119\n",
      "2017-11-10 16:26:25: Loss and accuracy at step 3333: 0.0369116, 0.017\n",
      "2017-11-10 16:26:27: Loss and accuracy at step 3334: 0.0368383, 0.01\n",
      "2017-11-10 16:26:30: Loss and accuracy at step 3335: 0.0368245, 0.0113\n",
      "2017-11-10 16:26:32: Loss and accuracy at step 3336: 0.0368415, 0.0188\n",
      "2017-11-10 16:26:34: Loss and accuracy at step 3337: 0.0368559, 0.0085\n",
      "2017-11-10 16:26:36: Loss and accuracy at step 3338: 0.0368153, 0.0092\n",
      "2017-11-10 16:26:38: Loss and accuracy at step 3339: 0.0369063, 0.0204\n",
      "2017-11-10 16:26:40: Loss and accuracy at step 3340: 0.0368372, 0.0096\n",
      "2017-11-10 16:26:42: Loss and accuracy at step 3341: 0.0368473, 0.0164\n",
      "2017-11-10 16:26:44: Loss and accuracy at step 3342: 0.0367789, 0.0109\n",
      "2017-11-10 16:26:46: Loss and accuracy at step 3343: 0.0368379, 0.0102\n",
      "2017-11-10 16:26:48: Loss and accuracy at step 3344: 0.036884, 0.0207\n",
      "2017-11-10 16:26:50: Loss and accuracy at step 3345: 0.0367933, 0.008\n",
      "2017-11-10 16:26:53: Loss and accuracy at step 3346: 0.0369269, 0.0118\n",
      "2017-11-10 16:26:55: Loss and accuracy at step 3347: 0.0367976, 0.0155\n",
      "2017-11-10 16:26:57: Loss and accuracy at step 3348: 0.0368494, 0.008\n",
      "2017-11-10 16:26:59: Loss and accuracy at step 3349: 0.0368543, 0.0145\n",
      "2017-11-10 16:27:01: Loss and accuracy at step 3350: 0.0368413, 0.0101\n",
      "2017-11-10 16:27:03: Loss and accuracy at step 3351: 0.0368663, 0.0111\n",
      "2017-11-10 16:27:05: Loss and accuracy at step 3352: 0.0368501, 0.0091\n",
      "2017-11-10 16:27:07: Loss and accuracy at step 3353: 0.0368878, 0.0101\n",
      "2017-11-10 16:27:09: Loss and accuracy at step 3354: 0.0368625, 0.0152\n",
      "2017-11-10 16:27:11: Loss and accuracy at step 3355: 0.0367971, 0.01\n",
      "2017-11-10 16:27:13: Loss and accuracy at step 3356: 0.0368062, 0.0095\n",
      "2017-11-10 16:27:16: Loss and accuracy at step 3357: 0.0367852, 0.0148\n",
      "2017-11-10 16:27:18: Loss and accuracy at step 3358: 0.036814, 0.0127\n",
      "2017-11-10 16:27:20: Loss and accuracy at step 3359: 0.0368997, 0.0075\n",
      "2017-11-10 16:27:22: Loss and accuracy at step 3360: 0.0369579, 0.0171\n",
      "2017-11-10 16:27:24: Loss and accuracy at step 3361: 0.0369169, 0.0075\n",
      "2017-11-10 16:27:26: Loss and accuracy at step 3362: 0.036888, 0.015\n",
      "2017-11-10 16:27:28: Loss and accuracy at step 3363: 0.0369727, 0.0123\n",
      "2017-11-10 16:27:30: Loss and accuracy at step 3364: 0.0368465, 0.012\n",
      "2017-11-10 16:27:32: Loss and accuracy at step 3365: 0.0369026, 0.0134\n",
      "2017-11-10 16:27:34: Loss and accuracy at step 3366: 0.0368736, 0.0115\n",
      "2017-11-10 16:27:36: Loss and accuracy at step 3367: 0.0369399, 0.0133\n",
      "2017-11-10 16:27:38: Loss and accuracy at step 3368: 0.0368861, 0.0097\n",
      "2017-11-10 16:27:41: Loss and accuracy at step 3369: 0.0368762, 0.0163\n",
      "2017-11-10 16:27:43: Loss and accuracy at step 3370: 0.0368295, 0.0101\n",
      "2017-11-10 16:27:45: Loss and accuracy at step 3371: 0.0369244, 0.0128\n",
      "2017-11-10 16:27:47: Loss and accuracy at step 3372: 0.0368808, 0.0122\n",
      "2017-11-10 16:27:49: Loss and accuracy at step 3373: 0.0368726, 0.0096\n",
      "2017-11-10 16:27:51: Loss and accuracy at step 3374: 0.0368687, 0.017\n",
      "2017-11-10 16:27:53: Loss and accuracy at step 3375: 0.0369409, 0.0065\n",
      "2017-11-10 16:27:55: Loss and accuracy at step 3376: 0.036969, 0.0127\n",
      "2017-11-10 16:27:57: Loss and accuracy at step 3377: 0.0368876, 0.0158\n",
      "2017-11-10 16:27:59: Loss and accuracy at step 3378: 0.0368315, 0.0075\n",
      "2017-11-10 16:28:01: Loss and accuracy at step 3379: 0.0369174, 0.0144\n",
      "2017-11-10 16:28:03: Loss and accuracy at step 3380: 0.0369411, 0.011\n",
      "2017-11-10 16:28:06: Loss and accuracy at step 3381: 0.0369154, 0.0141\n",
      "2017-11-10 16:28:08: Loss and accuracy at step 3382: 0.0369023, 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:28:10: Loss and accuracy at step 3383: 0.0368353, 0.0169\n",
      "2017-11-10 16:28:12: Loss and accuracy at step 3384: 0.0369004, 0.013\n",
      "2017-11-10 16:28:14: Loss and accuracy at step 3385: 0.0368799, 0.0071\n",
      "2017-11-10 16:28:16: Loss and accuracy at step 3386: 0.0368802, 0.0151\n",
      "2017-11-10 16:28:18: Loss and accuracy at step 3387: 0.0367757, 0.0111\n",
      "2017-11-10 16:28:20: Loss and accuracy at step 3388: 0.0369192, 0.0118\n",
      "2017-11-10 16:28:22: Loss and accuracy at step 3389: 0.0368422, 0.0112\n",
      "2017-11-10 16:28:24: Loss and accuracy at step 3390: 0.0368329, 0.0116\n",
      "2017-11-10 16:28:27: Loss and accuracy at step 3391: 0.0369268, 0.013\n",
      "2017-11-10 16:28:29: Loss and accuracy at step 3392: 0.036799, 0.0105\n",
      "2017-11-10 16:28:31: Loss and accuracy at step 3393: 0.0368241, 0.0163\n",
      "2017-11-10 16:28:33: Loss and accuracy at step 3394: 0.0368637, 0.0065\n",
      "2017-11-10 16:28:35: Loss and accuracy at step 3395: 0.0368974, 0.0205\n",
      "2017-11-10 16:28:37: Loss and accuracy at step 3396: 0.0368435, 0.014\n",
      "2017-11-10 16:28:39: Loss and accuracy at step 3397: 0.0368713, 0.008\n",
      "2017-11-10 16:28:41: Loss and accuracy at step 3398: 0.0368518, 0.0171\n",
      "2017-11-10 16:28:43: Loss and accuracy at step 3399: 0.0368153, 0.0103\n",
      "2017-11-10 16:28:45: Loss and accuracy at step 3400: 0.0368609, 0.0101\n",
      "2017-11-10 16:28:47: Loss and accuracy at step 3401: 0.0368518, 0.016\n",
      "2017-11-10 16:28:49: Loss and accuracy at step 3402: 0.0368766, 0.012\n",
      "2017-11-10 16:28:52: Loss and accuracy at step 3403: 0.0368627, 0.0082\n",
      "2017-11-10 16:28:54: Loss and accuracy at step 3404: 0.0367969, 0.0183\n",
      "2017-11-10 16:28:56: Loss and accuracy at step 3405: 0.0368089, 0.0117\n",
      "2017-11-10 16:28:58: Loss and accuracy at step 3406: 0.0367672, 0.0088\n",
      "2017-11-10 16:29:00: Loss and accuracy at step 3407: 0.0368479, 0.0199\n",
      "2017-11-10 16:29:02: Loss and accuracy at step 3408: 0.0368681, 0.0077\n",
      "2017-11-10 16:29:04: Loss and accuracy at step 3409: 0.0367984, 0.011\n",
      "2017-11-10 16:29:06: Loss and accuracy at step 3410: 0.036886, 0.0135\n",
      "2017-11-10 16:29:08: Loss and accuracy at step 3411: 0.0368323, 0.0101\n",
      "2017-11-10 16:29:10: Loss and accuracy at step 3412: 0.0368932, 0.0117\n",
      "2017-11-10 16:29:12: Loss and accuracy at step 3413: 0.0368214, 0.0117\n",
      "2017-11-10 16:29:15: Loss and accuracy at step 3414: 0.0367547, 0.0094\n",
      "2017-11-10 16:29:17: Loss and accuracy at step 3415: 0.0368058, 0.0168\n",
      "2017-11-10 16:29:19: Loss and accuracy at step 3416: 0.0368248, 0.0091\n",
      "2017-11-10 16:29:21: Loss and accuracy at step 3417: 0.0369041, 0.0123\n",
      "2017-11-10 16:29:23: Loss and accuracy at step 3418: 0.036889, 0.0135\n",
      "2017-11-10 16:29:25: Loss and accuracy at step 3419: 0.0368199, 0.0098\n",
      "2017-11-10 16:29:27: Loss and accuracy at step 3420: 0.0369085, 0.0105\n",
      "2017-11-10 16:29:29: Loss and accuracy at step 3421: 0.0369033, 0.0104\n",
      "2017-11-10 16:29:31: Loss and accuracy at step 3422: 0.0369091, 0.0156\n",
      "2017-11-10 16:29:33: Loss and accuracy at step 3423: 0.0369338, 0.0093\n",
      "2017-11-10 16:29:35: Loss and accuracy at step 3424: 0.0368869, 0.0169\n",
      "2017-11-10 16:29:38: Loss and accuracy at step 3425: 0.0367757, 0.0103\n",
      "2017-11-10 16:29:40: Loss and accuracy at step 3426: 0.0367717, 0.0092\n",
      "2017-11-10 16:29:42: Loss and accuracy at step 3427: 0.036945, 0.0171\n",
      "2017-11-10 16:29:44: Loss and accuracy at step 3428: 0.0367982, 0.0092\n",
      "2017-11-10 16:29:46: Loss and accuracy at step 3429: 0.0367869, 0.0113\n",
      "2017-11-10 16:29:48: Loss and accuracy at step 3430: 0.0368672, 0.0187\n",
      "2017-11-10 16:29:50: Loss and accuracy at step 3431: 0.0368711, 0.0077\n",
      "2017-11-10 16:29:52: Loss and accuracy at step 3432: 0.0368901, 0.0108\n",
      "2017-11-10 16:29:54: Loss and accuracy at step 3433: 0.0367992, 0.0144\n",
      "2017-11-10 16:29:56: Loss and accuracy at step 3434: 0.0368834, 0.011\n",
      "2017-11-10 16:29:58: Loss and accuracy at step 3435: 0.0368631, 0.0129\n",
      "2017-11-10 16:30:00: Loss and accuracy at step 3436: 0.0369401, 0.0102\n",
      "2017-11-10 16:30:03: Loss and accuracy at step 3437: 0.0368048, 0.0143\n",
      "2017-11-10 16:30:05: Loss and accuracy at step 3438: 0.036844, 0.0062\n",
      "2017-11-10 16:30:07: Loss and accuracy at step 3439: 0.0368403, 0.0172\n",
      "2017-11-10 16:30:09: Loss and accuracy at step 3440: 0.0368096, 0.0106\n",
      "2017-11-10 16:30:11: Loss and accuracy at step 3441: 0.0367982, 0.008\n",
      "2017-11-10 16:30:13: Loss and accuracy at step 3442: 0.0369054, 0.0131\n",
      "2017-11-10 16:30:15: Loss and accuracy at step 3443: 0.0368127, 0.0156\n",
      "2017-11-10 16:30:17: Loss and accuracy at step 3444: 0.0368698, 0.0089\n",
      "2017-11-10 16:30:19: Loss and accuracy at step 3445: 0.0368722, 0.0121\n",
      "2017-11-10 16:30:21: Loss and accuracy at step 3446: 0.0370167, 0.0138\n",
      "2017-11-10 16:30:24: Loss and accuracy at step 3447: 0.0368816, 0.0055\n",
      "2017-11-10 16:30:26: Loss and accuracy at step 3448: 0.0369483, 0.0166\n",
      "2017-11-10 16:30:28: Loss and accuracy at step 3449: 0.0368915, 0.0096\n",
      "2017-11-10 16:30:30: Loss and accuracy at step 3450: 0.0368481, 0.0125\n",
      "2017-11-10 16:30:32: Loss and accuracy at step 3451: 0.0368809, 0.0107\n",
      "2017-11-10 16:30:34: Loss and accuracy at step 3452: 0.0368733, 0.0097\n",
      "2017-11-10 16:30:36: Loss and accuracy at step 3453: 0.0369557, 0.0141\n",
      "2017-11-10 16:30:38: Loss and accuracy at step 3454: 0.0369522, 0.0074\n",
      "2017-11-10 16:30:40: Loss and accuracy at step 3455: 0.0368793, 0.0141\n",
      "2017-11-10 16:30:42: Loss and accuracy at step 3456: 0.0369317, 0.0079\n",
      "2017-11-10 16:30:44: Loss and accuracy at step 3457: 0.0369101, 0.0121\n",
      "2017-11-10 16:30:47: Loss and accuracy at step 3458: 0.0368045, 0.014\n",
      "2017-11-10 16:30:49: Loss and accuracy at step 3459: 0.0368605, 0.0097\n",
      "2017-11-10 16:30:51: Loss and accuracy at step 3460: 0.0368459, 0.0179\n",
      "2017-11-10 16:30:53: Loss and accuracy at step 3461: 0.0368917, 0.0071\n",
      "2017-11-10 16:30:55: Loss and accuracy at step 3462: 0.0368134, 0.0111\n",
      "2017-11-10 16:30:57: Loss and accuracy at step 3463: 0.0368191, 0.0179\n",
      "2017-11-10 16:30:59: Loss and accuracy at step 3464: 0.03688, 0.0065\n",
      "2017-11-10 16:31:01: Loss and accuracy at step 3465: 0.0367833, 0.0196\n",
      "2017-11-10 16:31:03: Loss and accuracy at step 3466: 0.0367677, 0.0086\n",
      "2017-11-10 16:31:05: Loss and accuracy at step 3467: 0.0368652, 0.0114\n",
      "2017-11-10 16:31:07: Loss and accuracy at step 3468: 0.036852, 0.0115\n",
      "2017-11-10 16:31:10: Loss and accuracy at step 3469: 0.0368576, 0.0083\n",
      "2017-11-10 16:31:12: Loss and accuracy at step 3470: 0.036943, 0.0213\n",
      "2017-11-10 16:31:14: Loss and accuracy at step 3471: 0.0368784, 0.009\n",
      "2017-11-10 16:31:16: Loss and accuracy at step 3472: 0.0368358, 0.0182\n",
      "2017-11-10 16:31:18: Loss and accuracy at step 3473: 0.0367566, 0.0116\n",
      "2017-11-10 16:31:20: Loss and accuracy at step 3474: 0.0368999, 0.0102\n",
      "2017-11-10 16:31:22: Loss and accuracy at step 3475: 0.036791, 0.0174\n",
      "2017-11-10 16:31:24: Loss and accuracy at step 3476: 0.0368207, 0.007\n",
      "2017-11-10 16:31:26: Loss and accuracy at step 3477: 0.0367936, 0.0143\n",
      "2017-11-10 16:31:28: Loss and accuracy at step 3478: 0.0368262, 0.0141\n",
      "2017-11-10 16:31:30: Loss and accuracy at step 3479: 0.0368641, 0.0101\n",
      "2017-11-10 16:31:33: Loss and accuracy at step 3480: 0.0368178, 0.0207\n",
      "2017-11-10 16:31:35: Loss and accuracy at step 3481: 0.0369564, 0.0061\n",
      "2017-11-10 16:31:37: Loss and accuracy at step 3482: 0.0368109, 0.0213\n",
      "2017-11-10 16:31:39: Loss and accuracy at step 3483: 0.03682, 0.01\n",
      "2017-11-10 16:31:41: Loss and accuracy at step 3484: 0.0368742, 0.0083\n",
      "2017-11-10 16:31:43: Loss and accuracy at step 3485: 0.0368266, 0.0277\n",
      "2017-11-10 16:31:45: Loss and accuracy at step 3486: 0.0369092, 0.0039\n",
      "2017-11-10 16:31:47: Loss and accuracy at step 3487: 0.036864, 0.0217\n",
      "2017-11-10 16:31:49: Loss and accuracy at step 3488: 0.0369415, 0.0109\n",
      "2017-11-10 16:31:51: Loss and accuracy at step 3489: 0.036933, 0.0082\n",
      "2017-11-10 16:31:53: Loss and accuracy at step 3490: 0.0368919, 0.0237\n",
      "2017-11-10 16:31:55: Loss and accuracy at step 3491: 0.0368459, 0.0102\n",
      "2017-11-10 16:31:58: Loss and accuracy at step 3492: 0.0368886, 0.0165\n",
      "2017-11-10 16:32:00: Loss and accuracy at step 3493: 0.0368166, 0.011\n",
      "2017-11-10 16:32:02: Loss and accuracy at step 3494: 0.0368515, 0.015\n",
      "2017-11-10 16:32:04: Loss and accuracy at step 3495: 0.0368893, 0.0101\n",
      "2017-11-10 16:32:06: Loss and accuracy at step 3496: 0.0369634, 0.0103\n",
      "2017-11-10 16:32:08: Loss and accuracy at step 3497: 0.0367936, 0.0168\n",
      "2017-11-10 16:32:10: Loss and accuracy at step 3498: 0.0368337, 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:32:12: Loss and accuracy at step 3499: 0.0368269, 0.0145\n",
      "2017-11-10 16:32:14: Loss and accuracy at step 3500: 0.0367987, 0.0134\n",
      "2017-11-10 16:32:17: Loss and accuracy at step 3501: 0.036834, 0.0101\n",
      "2017-11-10 16:32:19: Loss and accuracy at step 3502: 0.0368417, 0.0145\n",
      "2017-11-10 16:32:21: Loss and accuracy at step 3503: 0.036793, 0.0091\n",
      "2017-11-10 16:32:23: Loss and accuracy at step 3504: 0.0368074, 0.0099\n",
      "2017-11-10 16:32:25: Loss and accuracy at step 3505: 0.0368602, 0.0147\n",
      "2017-11-10 16:32:27: Loss and accuracy at step 3506: 0.0368921, 0.0123\n",
      "2017-11-10 16:32:29: Loss and accuracy at step 3507: 0.0368795, 0.0115\n",
      "2017-11-10 16:32:31: Loss and accuracy at step 3508: 0.0368777, 0.012\n",
      "2017-11-10 16:32:33: Loss and accuracy at step 3509: 0.0368379, 0.0176\n",
      "2017-11-10 16:32:35: Loss and accuracy at step 3510: 0.0369227, 0.0079\n",
      "2017-11-10 16:32:37: Loss and accuracy at step 3511: 0.0368457, 0.014\n",
      "2017-11-10 16:32:39: Loss and accuracy at step 3512: 0.0369148, 0.0143\n",
      "2017-11-10 16:32:42: Loss and accuracy at step 3513: 0.0368907, 0.0069\n",
      "2017-11-10 16:32:44: Loss and accuracy at step 3514: 0.0369195, 0.0164\n",
      "2017-11-10 16:32:46: Loss and accuracy at step 3515: 0.0368128, 0.0105\n",
      "2017-11-10 16:32:48: Loss and accuracy at step 3516: 0.0368616, 0.0125\n",
      "2017-11-10 16:32:50: Loss and accuracy at step 3517: 0.0368825, 0.0137\n",
      "2017-11-10 16:32:52: Loss and accuracy at step 3518: 0.0368401, 0.0102\n",
      "2017-11-10 16:32:54: Loss and accuracy at step 3519: 0.0368371, 0.0168\n",
      "2017-11-10 16:32:56: Loss and accuracy at step 3520: 0.0368727, 0.0117\n",
      "2017-11-10 16:32:58: Loss and accuracy at step 3521: 0.0368597, 0.0132\n",
      "2017-11-10 16:33:00: Loss and accuracy at step 3522: 0.036809, 0.0211\n",
      "2017-11-10 16:33:02: Loss and accuracy at step 3523: 0.0368094, 0.0062\n",
      "2017-11-10 16:33:05: Loss and accuracy at step 3524: 0.0368736, 0.0151\n",
      "2017-11-10 16:33:07: Loss and accuracy at step 3525: 0.0368755, 0.02\n",
      "2017-11-10 16:33:09: Loss and accuracy at step 3526: 0.0368266, 0.0098\n",
      "2017-11-10 16:33:11: Loss and accuracy at step 3527: 0.0368678, 0.017\n",
      "2017-11-10 16:33:13: Loss and accuracy at step 3528: 0.0369046, 0.0104\n",
      "2017-11-10 16:33:15: Loss and accuracy at step 3529: 0.0368518, 0.0128\n",
      "2017-11-10 16:33:17: Loss and accuracy at step 3530: 0.0368253, 0.0126\n",
      "2017-11-10 16:33:19: Loss and accuracy at step 3531: 0.0367954, 0.0076\n",
      "2017-11-10 16:33:21: Loss and accuracy at step 3532: 0.0368006, 0.0171\n",
      "2017-11-10 16:33:23: Loss and accuracy at step 3533: 0.0368911, 0.009\n",
      "2017-11-10 16:33:25: Loss and accuracy at step 3534: 0.0368804, 0.0097\n",
      "2017-11-10 16:33:27: Loss and accuracy at step 3535: 0.0368997, 0.0249\n",
      "2017-11-10 16:33:30: Loss and accuracy at step 3536: 0.0370005, 0.0058\n",
      "2017-11-10 16:33:32: Loss and accuracy at step 3537: 0.0368334, 0.0132\n",
      "2017-11-10 16:33:34: Loss and accuracy at step 3538: 0.0368582, 0.0167\n",
      "2017-11-10 16:33:36: Loss and accuracy at step 3539: 0.0369223, 0.0082\n",
      "2017-11-10 16:33:38: Loss and accuracy at step 3540: 0.0368165, 0.0154\n",
      "2017-11-10 16:33:40: Loss and accuracy at step 3541: 0.0368369, 0.0121\n",
      "2017-11-10 16:33:42: Loss and accuracy at step 3542: 0.0369087, 0.0104\n",
      "2017-11-10 16:33:44: Loss and accuracy at step 3543: 0.0368283, 0.0158\n",
      "2017-11-10 16:33:46: Loss and accuracy at step 3544: 0.0368882, 0.0067\n",
      "2017-11-10 16:33:48: Loss and accuracy at step 3545: 0.0368818, 0.0178\n",
      "2017-11-10 16:33:50: Loss and accuracy at step 3546: 0.0368648, 0.0135\n",
      "2017-11-10 16:33:53: Loss and accuracy at step 3547: 0.0369344, 0.0068\n",
      "2017-11-10 16:33:55: Loss and accuracy at step 3548: 0.0368962, 0.0213\n",
      "2017-11-10 16:33:57: Loss and accuracy at step 3549: 0.0367983, 0.0061\n",
      "2017-11-10 16:33:59: Loss and accuracy at step 3550: 0.0368912, 0.0116\n",
      "2017-11-10 16:34:01: Loss and accuracy at step 3551: 0.0368625, 0.0113\n",
      "2017-11-10 16:34:03: Loss and accuracy at step 3552: 0.0368199, 0.0079\n",
      "2017-11-10 16:34:05: Loss and accuracy at step 3553: 0.0369248, 0.017\n",
      "2017-11-10 16:34:07: Loss and accuracy at step 3554: 0.0368454, 0.0084\n",
      "2017-11-10 16:34:09: Loss and accuracy at step 3555: 0.0368596, 0.0102\n",
      "2017-11-10 16:34:11: Loss and accuracy at step 3556: 0.0368795, 0.0172\n",
      "2017-11-10 16:34:14: Loss and accuracy at step 3557: 0.0368316, 0.006\n",
      "2017-11-10 16:34:16: Loss and accuracy at step 3558: 0.036773, 0.014\n",
      "2017-11-10 16:34:18: Loss and accuracy at step 3559: 0.0367535, 0.015\n",
      "2017-11-10 16:34:20: Loss and accuracy at step 3560: 0.0368535, 0.0053\n",
      "2017-11-10 16:34:22: Loss and accuracy at step 3561: 0.0368452, 0.0204\n",
      "2017-11-10 16:34:24: Loss and accuracy at step 3562: 0.0367952, 0.0088\n",
      "2017-11-10 16:34:26: Loss and accuracy at step 3563: 0.0368717, 0.0111\n",
      "2017-11-10 16:34:28: Loss and accuracy at step 3564: 0.0369222, 0.0103\n",
      "2017-11-10 16:34:30: Loss and accuracy at step 3565: 0.0368848, 0.0083\n",
      "2017-11-10 16:34:32: Loss and accuracy at step 3566: 0.0368548, 0.0155\n",
      "2017-11-10 16:34:34: Loss and accuracy at step 3567: 0.0367732, 0.0065\n",
      "2017-11-10 16:34:37: Loss and accuracy at step 3568: 0.0368317, 0.0101\n",
      "2017-11-10 16:34:39: Loss and accuracy at step 3569: 0.0367616, 0.0184\n",
      "2017-11-10 16:34:41: Loss and accuracy at step 3570: 0.0369104, 0.0074\n",
      "2017-11-10 16:34:43: Loss and accuracy at step 3571: 0.0367678, 0.0165\n",
      "2017-11-10 16:34:45: Loss and accuracy at step 3572: 0.0367235, 0.0114\n",
      "2017-11-10 16:34:47: Loss and accuracy at step 3573: 0.0367755, 0.0098\n",
      "2017-11-10 16:34:49: Loss and accuracy at step 3574: 0.0367975, 0.0152\n",
      "2017-11-10 16:34:51: Loss and accuracy at step 3575: 0.0368157, 0.0062\n",
      "2017-11-10 16:34:53: Loss and accuracy at step 3576: 0.0368089, 0.0153\n",
      "2017-11-10 16:34:55: Loss and accuracy at step 3577: 0.0367857, 0.0138\n",
      "2017-11-10 16:34:57: Loss and accuracy at step 3578: 0.0367229, 0.0085\n",
      "2017-11-10 16:35:00: Loss and accuracy at step 3579: 0.036858, 0.0152\n",
      "2017-11-10 16:35:02: Loss and accuracy at step 3580: 0.0367758, 0.0117\n",
      "2017-11-10 16:35:04: Loss and accuracy at step 3581: 0.0367941, 0.0107\n",
      "2017-11-10 16:35:06: Loss and accuracy at step 3582: 0.0368808, 0.0113\n",
      "2017-11-10 16:35:08: Loss and accuracy at step 3583: 0.0367362, 0.007\n",
      "2017-11-10 16:35:10: Loss and accuracy at step 3584: 0.0368126, 0.0146\n",
      "2017-11-10 16:35:12: Loss and accuracy at step 3585: 0.0367749, 0.0101\n",
      "2017-11-10 16:35:14: Loss and accuracy at step 3586: 0.0368298, 0.0092\n",
      "2017-11-10 16:35:16: Loss and accuracy at step 3587: 0.0367978, 0.0146\n",
      "2017-11-10 16:35:18: Loss and accuracy at step 3588: 0.036809, 0.0115\n",
      "2017-11-10 16:35:20: Loss and accuracy at step 3589: 0.0367859, 0.0099\n",
      "2017-11-10 16:35:23: Loss and accuracy at step 3590: 0.0367501, 0.0151\n",
      "2017-11-10 16:35:25: Loss and accuracy at step 3591: 0.036784, 0.0091\n",
      "2017-11-10 16:35:27: Loss and accuracy at step 3592: 0.0368491, 0.0124\n",
      "2017-11-10 16:35:29: Loss and accuracy at step 3593: 0.0367895, 0.0134\n",
      "2017-11-10 16:35:31: Loss and accuracy at step 3594: 0.0368176, 0.0071\n",
      "2017-11-10 16:35:33: Loss and accuracy at step 3595: 0.0368143, 0.0159\n",
      "2017-11-10 16:35:35: Loss and accuracy at step 3596: 0.036846, 0.0121\n",
      "2017-11-10 16:35:37: Loss and accuracy at step 3597: 0.0368321, 0.008\n",
      "2017-11-10 16:35:39: Loss and accuracy at step 3598: 0.0368803, 0.0163\n",
      "2017-11-10 16:35:41: Loss and accuracy at step 3599: 0.0368467, 0.006\n",
      "2017-11-10 16:35:44: Loss and accuracy at step 3600: 0.0368249, 0.0162\n",
      "2017-11-10 16:35:46: Loss and accuracy at step 3601: 0.0367846, 0.0175\n",
      "2017-11-10 16:35:48: Loss and accuracy at step 3602: 0.0369189, 0.0069\n",
      "2017-11-10 16:35:50: Loss and accuracy at step 3603: 0.0368457, 0.0208\n",
      "2017-11-10 16:35:52: Loss and accuracy at step 3604: 0.0369474, 0.0093\n",
      "2017-11-10 16:35:54: Loss and accuracy at step 3605: 0.0368884, 0.0091\n",
      "2017-11-10 16:35:56: Loss and accuracy at step 3606: 0.0368595, 0.0126\n",
      "2017-11-10 16:35:58: Loss and accuracy at step 3607: 0.0368171, 0.01\n",
      "2017-11-10 16:36:00: Loss and accuracy at step 3608: 0.0369582, 0.0119\n",
      "2017-11-10 16:36:02: Loss and accuracy at step 3609: 0.0368714, 0.0102\n",
      "2017-11-10 16:36:04: Loss and accuracy at step 3610: 0.0367502, 0.0116\n",
      "2017-11-10 16:36:06: Loss and accuracy at step 3611: 0.0367967, 0.0123\n",
      "2017-11-10 16:36:09: Loss and accuracy at step 3612: 0.0368335, 0.0114\n",
      "2017-11-10 16:36:11: Loss and accuracy at step 3613: 0.0368421, 0.0101\n",
      "2017-11-10 16:36:13: Loss and accuracy at step 3614: 0.0368415, 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:36:15: Loss and accuracy at step 3615: 0.0368598, 0.0111\n",
      "2017-11-10 16:36:17: Loss and accuracy at step 3616: 0.0367861, 0.0109\n",
      "2017-11-10 16:36:19: Loss and accuracy at step 3617: 0.0367513, 0.0114\n",
      "2017-11-10 16:36:21: Loss and accuracy at step 3618: 0.0368279, 0.0125\n",
      "2017-11-10 16:36:23: Loss and accuracy at step 3619: 0.0368019, 0.0089\n",
      "2017-11-10 16:36:25: Loss and accuracy at step 3620: 0.0367205, 0.0113\n",
      "2017-11-10 16:36:28: Loss and accuracy at step 3621: 0.0368224, 0.0114\n",
      "2017-11-10 16:36:30: Loss and accuracy at step 3622: 0.0368403, 0.0112\n",
      "2017-11-10 16:36:32: Loss and accuracy at step 3623: 0.0368117, 0.0136\n",
      "2017-11-10 16:36:34: Loss and accuracy at step 3624: 0.0368101, 0.0109\n",
      "2017-11-10 16:36:36: Loss and accuracy at step 3625: 0.0368513, 0.0078\n",
      "2017-11-10 16:36:38: Loss and accuracy at step 3626: 0.0368776, 0.0144\n",
      "2017-11-10 16:36:40: Loss and accuracy at step 3627: 0.0368523, 0.0076\n",
      "2017-11-10 16:36:42: Loss and accuracy at step 3628: 0.0367769, 0.0116\n",
      "2017-11-10 16:36:44: Loss and accuracy at step 3629: 0.0368811, 0.0119\n",
      "2017-11-10 16:36:46: Loss and accuracy at step 3630: 0.0367704, 0.012\n",
      "2017-11-10 16:36:48: Loss and accuracy at step 3631: 0.0367765, 0.0105\n",
      "2017-11-10 16:36:50: Loss and accuracy at step 3632: 0.0368449, 0.0072\n",
      "2017-11-10 16:36:53: Loss and accuracy at step 3633: 0.0367392, 0.0154\n",
      "2017-11-10 16:36:55: Loss and accuracy at step 3634: 0.036773, 0.0144\n",
      "2017-11-10 16:36:57: Loss and accuracy at step 3635: 0.0368249, 0.0103\n",
      "2017-11-10 16:36:59: Loss and accuracy at step 3636: 0.0368132, 0.0138\n",
      "2017-11-10 16:37:01: Loss and accuracy at step 3637: 0.036841, 0.0158\n",
      "2017-11-10 16:37:03: Loss and accuracy at step 3638: 0.0367779, 0.0141\n",
      "2017-11-10 16:37:05: Loss and accuracy at step 3639: 0.0368342, 0.0112\n",
      "2017-11-10 16:37:07: Loss and accuracy at step 3640: 0.036802, 0.0165\n",
      "2017-11-10 16:37:09: Loss and accuracy at step 3641: 0.036865, 0.0119\n",
      "2017-11-10 16:37:11: Loss and accuracy at step 3642: 0.0368565, 0.0111\n",
      "2017-11-10 16:37:13: Loss and accuracy at step 3643: 0.0367963, 0.0218\n",
      "2017-11-10 16:37:16: Loss and accuracy at step 3644: 0.0369075, 0.0083\n",
      "2017-11-10 16:37:18: Loss and accuracy at step 3645: 0.0368045, 0.0158\n",
      "2017-11-10 16:37:20: Loss and accuracy at step 3646: 0.0367666, 0.0201\n",
      "2017-11-10 16:37:22: Loss and accuracy at step 3647: 0.0367577, 0.0069\n",
      "2017-11-10 16:37:24: Loss and accuracy at step 3648: 0.0368058, 0.0142\n",
      "2017-11-10 16:37:26: Loss and accuracy at step 3649: 0.0367552, 0.0094\n",
      "2017-11-10 16:37:28: Loss and accuracy at step 3650: 0.0368386, 0.0134\n",
      "2017-11-10 16:37:30: Loss and accuracy at step 3651: 0.0367725, 0.0116\n",
      "2017-11-10 16:37:32: Loss and accuracy at step 3652: 0.0368161, 0.0125\n",
      "2017-11-10 16:37:34: Loss and accuracy at step 3653: 0.0367328, 0.0251\n",
      "2017-11-10 16:37:36: Loss and accuracy at step 3654: 0.0367525, 0.0107\n",
      "2017-11-10 16:37:39: Loss and accuracy at step 3655: 0.0367483, 0.0107\n",
      "2017-11-10 16:37:41: Loss and accuracy at step 3656: 0.0367762, 0.0222\n",
      "2017-11-10 16:37:43: Loss and accuracy at step 3657: 0.0368009, 0.009\n",
      "2017-11-10 16:37:45: Loss and accuracy at step 3658: 0.0368249, 0.0106\n",
      "2017-11-10 16:37:47: Loss and accuracy at step 3659: 0.036942, 0.0205\n",
      "2017-11-10 16:37:49: Loss and accuracy at step 3660: 0.0368444, 0.0103\n",
      "2017-11-10 16:37:51: Loss and accuracy at step 3661: 0.036784, 0.0129\n",
      "2017-11-10 16:37:53: Loss and accuracy at step 3662: 0.0367681, 0.0195\n",
      "2017-11-10 16:37:55: Loss and accuracy at step 3663: 0.0367641, 0.0118\n",
      "2017-11-10 16:37:57: Loss and accuracy at step 3664: 0.0368123, 0.0125\n",
      "2017-11-10 16:37:59: Loss and accuracy at step 3665: 0.0367919, 0.0108\n",
      "2017-11-10 16:38:02: Loss and accuracy at step 3666: 0.0368285, 0.0127\n",
      "2017-11-10 16:38:04: Loss and accuracy at step 3667: 0.0368844, 0.0125\n",
      "2017-11-10 16:38:06: Loss and accuracy at step 3668: 0.0368467, 0.0106\n",
      "2017-11-10 16:38:08: Loss and accuracy at step 3669: 0.0368193, 0.0142\n",
      "2017-11-10 16:38:10: Loss and accuracy at step 3670: 0.0368336, 0.0107\n",
      "2017-11-10 16:38:12: Loss and accuracy at step 3671: 0.0368039, 0.0127\n",
      "2017-11-10 16:38:14: Loss and accuracy at step 3672: 0.0368251, 0.0163\n",
      "2017-11-10 16:38:16: Loss and accuracy at step 3673: 0.0367943, 0.0115\n",
      "2017-11-10 16:38:18: Loss and accuracy at step 3674: 0.0368417, 0.0083\n",
      "2017-11-10 16:38:21: Loss and accuracy at step 3675: 0.0368704, 0.0185\n",
      "2017-11-10 16:38:23: Loss and accuracy at step 3676: 0.0367901, 0.0095\n",
      "2017-11-10 16:38:25: Loss and accuracy at step 3677: 0.036835, 0.012\n",
      "2017-11-10 16:38:27: Loss and accuracy at step 3678: 0.036795, 0.0147\n",
      "2017-11-10 16:38:29: Loss and accuracy at step 3679: 0.0367587, 0.0116\n",
      "2017-11-10 16:38:31: Loss and accuracy at step 3680: 0.036728, 0.0109\n",
      "2017-11-10 16:38:33: Loss and accuracy at step 3681: 0.0366763, 0.0122\n",
      "2017-11-10 16:38:35: Loss and accuracy at step 3682: 0.0368343, 0.0127\n",
      "2017-11-10 16:38:37: Loss and accuracy at step 3683: 0.0368348, 0.0135\n",
      "2017-11-10 16:38:39: Loss and accuracy at step 3684: 0.0368315, 0.0101\n",
      "2017-11-10 16:38:41: Loss and accuracy at step 3685: 0.0368602, 0.0093\n",
      "2017-11-10 16:38:44: Loss and accuracy at step 3686: 0.0367395, 0.0154\n",
      "2017-11-10 16:38:46: Loss and accuracy at step 3687: 0.036844, 0.007\n",
      "2017-11-10 16:38:48: Loss and accuracy at step 3688: 0.036784, 0.013\n",
      "2017-11-10 16:38:50: Loss and accuracy at step 3689: 0.0368139, 0.0114\n",
      "2017-11-10 16:38:52: Loss and accuracy at step 3690: 0.0368818, 0.0118\n",
      "2017-11-10 16:38:54: Loss and accuracy at step 3691: 0.0367582, 0.0144\n",
      "2017-11-10 16:38:56: Loss and accuracy at step 3692: 0.0367743, 0.0081\n",
      "2017-11-10 16:38:58: Loss and accuracy at step 3693: 0.0368616, 0.0175\n",
      "2017-11-10 16:39:00: Loss and accuracy at step 3694: 0.0368953, 0.0123\n",
      "2017-11-10 16:39:02: Loss and accuracy at step 3695: 0.0369017, 0.0106\n",
      "2017-11-10 16:39:04: Loss and accuracy at step 3696: 0.0368236, 0.0184\n",
      "2017-11-10 16:39:06: Loss and accuracy at step 3697: 0.0368806, 0.0077\n",
      "2017-11-10 16:39:09: Loss and accuracy at step 3698: 0.0368678, 0.0205\n",
      "2017-11-10 16:39:11: Loss and accuracy at step 3699: 0.0368817, 0.0117\n",
      "2017-11-10 16:39:13: Loss and accuracy at step 3700: 0.0368674, 0.0092\n",
      "2017-11-10 16:39:15: Loss and accuracy at step 3701: 0.0368395, 0.0186\n",
      "2017-11-10 16:39:17: Loss and accuracy at step 3702: 0.0368296, 0.0089\n",
      "2017-11-10 16:39:19: Loss and accuracy at step 3703: 0.0368367, 0.0117\n",
      "2017-11-10 16:39:21: Loss and accuracy at step 3704: 0.0368339, 0.0193\n",
      "2017-11-10 16:39:23: Loss and accuracy at step 3705: 0.0368326, 0.008\n",
      "2017-11-10 16:39:25: Loss and accuracy at step 3706: 0.0368019, 0.0101\n",
      "2017-11-10 16:39:27: Loss and accuracy at step 3707: 0.0367733, 0.0127\n",
      "2017-11-10 16:39:29: Loss and accuracy at step 3708: 0.0368507, 0.0102\n",
      "2017-11-10 16:39:32: Loss and accuracy at step 3709: 0.0368357, 0.0089\n",
      "2017-11-10 16:39:34: Loss and accuracy at step 3710: 0.0368868, 0.0137\n",
      "2017-11-10 16:39:36: Loss and accuracy at step 3711: 0.0369261, 0.0152\n",
      "2017-11-10 16:39:38: Loss and accuracy at step 3712: 0.0368821, 0.0087\n",
      "2017-11-10 16:39:40: Loss and accuracy at step 3713: 0.036823, 0.0119\n",
      "2017-11-10 16:39:42: Loss and accuracy at step 3714: 0.0369208, 0.0186\n",
      "2017-11-10 16:39:44: Loss and accuracy at step 3715: 0.0368433, 0.0086\n",
      "2017-11-10 16:39:46: Loss and accuracy at step 3716: 0.0369371, 0.0126\n",
      "2017-11-10 16:39:48: Loss and accuracy at step 3717: 0.0367677, 0.0109\n",
      "2017-11-10 16:39:50: Loss and accuracy at step 3718: 0.0368523, 0.0098\n",
      "2017-11-10 16:39:52: Loss and accuracy at step 3719: 0.03679, 0.0236\n",
      "2017-11-10 16:39:54: Loss and accuracy at step 3720: 0.0367806, 0.0085\n",
      "2017-11-10 16:39:57: Loss and accuracy at step 3721: 0.0368395, 0.0111\n",
      "2017-11-10 16:39:59: Loss and accuracy at step 3722: 0.0368292, 0.0126\n",
      "2017-11-10 16:40:01: Loss and accuracy at step 3723: 0.036784, 0.0109\n",
      "2017-11-10 16:40:03: Loss and accuracy at step 3724: 0.0368277, 0.0124\n",
      "2017-11-10 16:40:05: Loss and accuracy at step 3725: 0.0367818, 0.0106\n",
      "2017-11-10 16:40:07: Loss and accuracy at step 3726: 0.0368382, 0.0142\n",
      "2017-11-10 16:40:09: Loss and accuracy at step 3727: 0.0367998, 0.008\n",
      "2017-11-10 16:40:11: Loss and accuracy at step 3728: 0.0368504, 0.0122\n",
      "2017-11-10 16:40:13: Loss and accuracy at step 3729: 0.0368494, 0.0173\n",
      "2017-11-10 16:40:15: Loss and accuracy at step 3730: 0.0367454, 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:40:18: Loss and accuracy at step 3731: 0.0368825, 0.0141\n",
      "2017-11-10 16:40:20: Loss and accuracy at step 3732: 0.0368325, 0.0124\n",
      "2017-11-10 16:40:22: Loss and accuracy at step 3733: 0.036857, 0.0121\n",
      "2017-11-10 16:40:24: Loss and accuracy at step 3734: 0.0368722, 0.0109\n",
      "2017-11-10 16:40:26: Loss and accuracy at step 3735: 0.036795, 0.0211\n",
      "2017-11-10 16:40:28: Loss and accuracy at step 3736: 0.0367871, 0.0125\n",
      "2017-11-10 16:40:30: Loss and accuracy at step 3737: 0.0368699, 0.0115\n",
      "2017-11-10 16:40:32: Loss and accuracy at step 3738: 0.0368402, 0.0204\n",
      "2017-11-10 16:40:34: Loss and accuracy at step 3739: 0.0368197, 0.0102\n",
      "2017-11-10 16:40:36: Loss and accuracy at step 3740: 0.0367927, 0.0138\n",
      "2017-11-10 16:40:38: Loss and accuracy at step 3741: 0.0367898, 0.0152\n",
      "2017-11-10 16:40:41: Loss and accuracy at step 3742: 0.0368004, 0.0102\n",
      "2017-11-10 16:40:43: Loss and accuracy at step 3743: 0.0368084, 0.0138\n",
      "2017-11-10 16:40:45: Loss and accuracy at step 3744: 0.0368017, 0.0103\n",
      "2017-11-10 16:40:47: Loss and accuracy at step 3745: 0.0367796, 0.012\n",
      "2017-11-10 16:40:49: Loss and accuracy at step 3746: 0.0368802, 0.0208\n",
      "2017-11-10 16:40:51: Loss and accuracy at step 3747: 0.0369034, 0.0094\n",
      "2017-11-10 16:40:53: Loss and accuracy at step 3748: 0.0367449, 0.013\n",
      "2017-11-10 16:40:55: Loss and accuracy at step 3749: 0.0367978, 0.0213\n",
      "2017-11-10 16:40:57: Loss and accuracy at step 3750: 0.0368013, 0.0074\n",
      "2017-11-10 16:40:59: Loss and accuracy at step 3751: 0.0367557, 0.0154\n",
      "2017-11-10 16:41:01: Loss and accuracy at step 3752: 0.0368365, 0.0176\n",
      "2017-11-10 16:41:04: Loss and accuracy at step 3753: 0.0368017, 0.0101\n",
      "2017-11-10 16:41:06: Loss and accuracy at step 3754: 0.0368281, 0.014\n",
      "2017-11-10 16:41:08: Loss and accuracy at step 3755: 0.0368342, 0.0261\n",
      "2017-11-10 16:41:10: Loss and accuracy at step 3756: 0.0368719, 0.0101\n",
      "2017-11-10 16:41:12: Loss and accuracy at step 3757: 0.0368843, 0.0106\n",
      "2017-11-10 16:41:14: Loss and accuracy at step 3758: 0.0368838, 0.0224\n",
      "2017-11-10 16:41:16: Loss and accuracy at step 3759: 0.0367877, 0.0069\n",
      "2017-11-10 16:41:18: Loss and accuracy at step 3760: 0.0367779, 0.0126\n",
      "2017-11-10 16:41:20: Loss and accuracy at step 3761: 0.0368447, 0.0141\n",
      "2017-11-10 16:41:22: Loss and accuracy at step 3762: 0.0369024, 0.0068\n",
      "2017-11-10 16:41:24: Loss and accuracy at step 3763: 0.0368635, 0.015\n",
      "2017-11-10 16:41:27: Loss and accuracy at step 3764: 0.0368506, 0.0135\n",
      "2017-11-10 16:41:29: Loss and accuracy at step 3765: 0.0368398, 0.0095\n",
      "2017-11-10 16:41:31: Loss and accuracy at step 3766: 0.036829, 0.0173\n",
      "2017-11-10 16:41:33: Loss and accuracy at step 3767: 0.0367629, 0.009\n",
      "2017-11-10 16:41:35: Loss and accuracy at step 3768: 0.0368391, 0.0083\n",
      "2017-11-10 16:41:37: Loss and accuracy at step 3769: 0.0368631, 0.0175\n",
      "2017-11-10 16:41:39: Loss and accuracy at step 3770: 0.0369377, 0.0085\n",
      "2017-11-10 16:41:41: Loss and accuracy at step 3771: 0.0368083, 0.0149\n",
      "2017-11-10 16:41:43: Loss and accuracy at step 3772: 0.0367666, 0.0174\n",
      "2017-11-10 16:41:45: Loss and accuracy at step 3773: 0.0368174, 0.0094\n",
      "2017-11-10 16:41:47: Loss and accuracy at step 3774: 0.0368319, 0.0122\n",
      "2017-11-10 16:41:49: Loss and accuracy at step 3775: 0.036823, 0.0152\n",
      "2017-11-10 16:41:52: Loss and accuracy at step 3776: 0.0367505, 0.0102\n",
      "2017-11-10 16:41:54: Loss and accuracy at step 3777: 0.0368269, 0.0067\n",
      "2017-11-10 16:41:56: Loss and accuracy at step 3778: 0.0368091, 0.0297\n",
      "2017-11-10 16:41:58: Loss and accuracy at step 3779: 0.0368897, 0.0088\n",
      "2017-11-10 16:42:00: Loss and accuracy at step 3780: 0.0368628, 0.0098\n",
      "2017-11-10 16:42:02: Loss and accuracy at step 3781: 0.0368218, 0.0354\n",
      "2017-11-10 16:42:04: Loss and accuracy at step 3782: 0.0368449, 0.0061\n",
      "2017-11-10 16:42:06: Loss and accuracy at step 3783: 0.036806, 0.0142\n",
      "2017-11-10 16:42:08: Loss and accuracy at step 3784: 0.0368345, 0.0299\n",
      "2017-11-10 16:42:10: Loss and accuracy at step 3785: 0.036868, 0.0082\n",
      "2017-11-10 16:42:13: Loss and accuracy at step 3786: 0.0369093, 0.0136\n",
      "2017-11-10 16:42:15: Loss and accuracy at step 3787: 0.0368753, 0.0101\n",
      "2017-11-10 16:42:17: Loss and accuracy at step 3788: 0.0368725, 0.0146\n",
      "2017-11-10 16:42:19: Loss and accuracy at step 3789: 0.0368731, 0.0114\n",
      "2017-11-10 16:42:21: Loss and accuracy at step 3790: 0.0369018, 0.0086\n",
      "2017-11-10 16:42:23: Loss and accuracy at step 3791: 0.0368595, 0.0176\n",
      "2017-11-10 16:42:25: Loss and accuracy at step 3792: 0.0367934, 0.0102\n",
      "2017-11-10 16:42:27: Loss and accuracy at step 3793: 0.0368093, 0.012\n",
      "2017-11-10 16:42:29: Loss and accuracy at step 3794: 0.0368473, 0.0126\n",
      "2017-11-10 16:42:31: Loss and accuracy at step 3795: 0.0367302, 0.0088\n",
      "2017-11-10 16:42:33: Loss and accuracy at step 3796: 0.0369052, 0.0152\n",
      "2017-11-10 16:42:35: Loss and accuracy at step 3797: 0.036846, 0.0128\n",
      "2017-11-10 16:42:38: Loss and accuracy at step 3798: 0.0367521, 0.0114\n",
      "2017-11-10 16:42:40: Loss and accuracy at step 3799: 0.036828, 0.0123\n",
      "2017-11-10 16:42:42: Loss and accuracy at step 3800: 0.0367676, 0.0086\n",
      "2017-11-10 16:42:44: Loss and accuracy at step 3801: 0.03678, 0.0148\n",
      "2017-11-10 16:42:46: Loss and accuracy at step 3802: 0.0368621, 0.0112\n",
      "2017-11-10 16:42:48: Loss and accuracy at step 3803: 0.0368575, 0.0111\n",
      "2017-11-10 16:42:50: Loss and accuracy at step 3804: 0.0368297, 0.0139\n",
      "2017-11-10 16:42:52: Loss and accuracy at step 3805: 0.036759, 0.0086\n",
      "2017-11-10 16:42:54: Loss and accuracy at step 3806: 0.0366673, 0.0165\n",
      "2017-11-10 16:42:56: Loss and accuracy at step 3807: 0.0367849, 0.0088\n",
      "2017-11-10 16:42:58: Loss and accuracy at step 3808: 0.0368298, 0.0114\n",
      "2017-11-10 16:43:01: Loss and accuracy at step 3809: 0.0367546, 0.0102\n",
      "2017-11-10 16:43:03: Loss and accuracy at step 3810: 0.0367594, 0.0118\n",
      "2017-11-10 16:43:05: Loss and accuracy at step 3811: 0.0368513, 0.0085\n",
      "2017-11-10 16:43:07: Loss and accuracy at step 3812: 0.0368232, 0.0099\n",
      "2017-11-10 16:43:09: Loss and accuracy at step 3813: 0.0367745, 0.0101\n",
      "2017-11-10 16:43:11: Loss and accuracy at step 3814: 0.0368834, 0.0143\n",
      "2017-11-10 16:43:13: Loss and accuracy at step 3815: 0.0368021, 0.009\n",
      "2017-11-10 16:43:15: Loss and accuracy at step 3816: 0.0368152, 0.0111\n",
      "2017-11-10 16:43:17: Loss and accuracy at step 3817: 0.0368666, 0.0143\n",
      "2017-11-10 16:43:19: Loss and accuracy at step 3818: 0.0367435, 0.0087\n",
      "2017-11-10 16:43:21: Loss and accuracy at step 3819: 0.0368166, 0.012\n",
      "2017-11-10 16:43:24: Loss and accuracy at step 3820: 0.0367808, 0.0116\n",
      "2017-11-10 16:43:26: Loss and accuracy at step 3821: 0.0367976, 0.0108\n",
      "2017-11-10 16:43:28: Loss and accuracy at step 3822: 0.0367743, 0.0117\n",
      "2017-11-10 16:43:30: Loss and accuracy at step 3823: 0.0368377, 0.0129\n",
      "2017-11-10 16:43:32: Loss and accuracy at step 3824: 0.036806, 0.0131\n",
      "2017-11-10 16:43:34: Loss and accuracy at step 3825: 0.0368618, 0.0118\n",
      "2017-11-10 16:43:36: Loss and accuracy at step 3826: 0.03685, 0.0108\n",
      "2017-11-10 16:43:38: Loss and accuracy at step 3827: 0.0368148, 0.0145\n",
      "2017-11-10 16:43:40: Loss and accuracy at step 3828: 0.0368643, 0.0129\n",
      "2017-11-10 16:43:42: Loss and accuracy at step 3829: 0.0368564, 0.0111\n",
      "2017-11-10 16:43:44: Loss and accuracy at step 3830: 0.0367633, 0.0118\n",
      "2017-11-10 16:43:47: Loss and accuracy at step 3831: 0.0369093, 0.0115\n",
      "2017-11-10 16:43:49: Loss and accuracy at step 3832: 0.0368159, 0.0122\n",
      "2017-11-10 16:43:51: Loss and accuracy at step 3833: 0.0368336, 0.0133\n",
      "2017-11-10 16:43:53: Loss and accuracy at step 3834: 0.0368528, 0.0117\n",
      "2017-11-10 16:43:55: Loss and accuracy at step 3835: 0.0368125, 0.0115\n",
      "2017-11-10 16:43:57: Loss and accuracy at step 3836: 0.0368125, 0.0169\n",
      "2017-11-10 16:43:59: Loss and accuracy at step 3837: 0.036866, 0.0078\n",
      "2017-11-10 16:44:01: Loss and accuracy at step 3838: 0.0367952, 0.0127\n",
      "2017-11-10 16:44:03: Loss and accuracy at step 3839: 0.0368904, 0.0251\n",
      "2017-11-10 16:44:05: Loss and accuracy at step 3840: 0.0368178, 0.0098\n",
      "2017-11-10 16:44:07: Loss and accuracy at step 3841: 0.03689, 0.0113\n",
      "2017-11-10 16:44:09: Loss and accuracy at step 3842: 0.0368192, 0.0225\n",
      "2017-11-10 16:44:12: Loss and accuracy at step 3843: 0.0368019, 0.0075\n",
      "2017-11-10 16:44:14: Loss and accuracy at step 3844: 0.0368899, 0.0125\n",
      "2017-11-10 16:44:16: Loss and accuracy at step 3845: 0.0368209, 0.0117\n",
      "2017-11-10 16:44:18: Loss and accuracy at step 3846: 0.0368529, 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:44:20: Loss and accuracy at step 3847: 0.0368173, 0.0174\n",
      "2017-11-10 16:44:22: Loss and accuracy at step 3848: 0.0368591, 0.0127\n",
      "2017-11-10 16:44:24: Loss and accuracy at step 3849: 0.0368063, 0.0096\n",
      "2017-11-10 16:44:26: Loss and accuracy at step 3850: 0.0367747, 0.0146\n",
      "2017-11-10 16:44:28: Loss and accuracy at step 3851: 0.0368334, 0.0125\n",
      "2017-11-10 16:44:31: Loss and accuracy at step 3852: 0.036822, 0.007\n",
      "2017-11-10 16:44:33: Loss and accuracy at step 3853: 0.0367922, 0.0167\n",
      "2017-11-10 16:44:35: Loss and accuracy at step 3854: 0.0367535, 0.0121\n",
      "2017-11-10 16:44:37: Loss and accuracy at step 3855: 0.0368426, 0.0096\n",
      "2017-11-10 16:44:39: Loss and accuracy at step 3856: 0.036828, 0.0158\n",
      "2017-11-10 16:44:41: Loss and accuracy at step 3857: 0.036786, 0.0098\n",
      "2017-11-10 16:44:43: Loss and accuracy at step 3858: 0.0368063, 0.0157\n",
      "2017-11-10 16:44:45: Loss and accuracy at step 3859: 0.0367798, 0.0106\n",
      "2017-11-10 16:44:47: Loss and accuracy at step 3860: 0.0368013, 0.0109\n",
      "2017-11-10 16:44:49: Loss and accuracy at step 3861: 0.0368404, 0.0104\n",
      "2017-11-10 16:44:51: Loss and accuracy at step 3862: 0.0367609, 0.0104\n",
      "2017-11-10 16:44:54: Loss and accuracy at step 3863: 0.0367651, 0.0128\n",
      "2017-11-10 16:44:56: Loss and accuracy at step 3864: 0.0367535, 0.0107\n",
      "2017-11-10 16:44:58: Loss and accuracy at step 3865: 0.0367556, 0.0117\n",
      "2017-11-10 16:45:00: Loss and accuracy at step 3866: 0.0367961, 0.0125\n",
      "2017-11-10 16:45:02: Loss and accuracy at step 3867: 0.0368201, 0.0099\n",
      "2017-11-10 16:45:04: Loss and accuracy at step 3868: 0.0368994, 0.0098\n",
      "2017-11-10 16:45:06: Loss and accuracy at step 3869: 0.0368286, 0.0217\n",
      "2017-11-10 16:45:08: Loss and accuracy at step 3870: 0.0369485, 0.0068\n",
      "2017-11-10 16:45:10: Loss and accuracy at step 3871: 0.0368232, 0.0144\n",
      "2017-11-10 16:45:12: Loss and accuracy at step 3872: 0.0367839, 0.021\n",
      "2017-11-10 16:45:14: Loss and accuracy at step 3873: 0.0367319, 0.0069\n",
      "2017-11-10 16:45:16: Loss and accuracy at step 3874: 0.0367133, 0.0193\n",
      "2017-11-10 16:45:19: Loss and accuracy at step 3875: 0.0367791, 0.0127\n",
      "2017-11-10 16:45:21: Loss and accuracy at step 3876: 0.0368285, 0.0069\n",
      "2017-11-10 16:45:23: Loss and accuracy at step 3877: 0.0367729, 0.0321\n",
      "2017-11-10 16:45:25: Loss and accuracy at step 3878: 0.0367864, 0.0054\n",
      "2017-11-10 16:45:27: Loss and accuracy at step 3879: 0.0367976, 0.0123\n",
      "2017-11-10 16:45:29: Loss and accuracy at step 3880: 0.0368617, 0.0229\n",
      "2017-11-10 16:45:31: Loss and accuracy at step 3881: 0.0368331, 0.0113\n",
      "2017-11-10 16:45:33: Loss and accuracy at step 3882: 0.0367512, 0.011\n",
      "2017-11-10 16:45:35: Loss and accuracy at step 3883: 0.0368181, 0.0213\n",
      "2017-11-10 16:45:37: Loss and accuracy at step 3884: 0.0368992, 0.0134\n",
      "2017-11-10 16:45:40: Loss and accuracy at step 3885: 0.0368827, 0.008\n",
      "2017-11-10 16:45:42: Loss and accuracy at step 3886: 0.0368608, 0.0432\n",
      "2017-11-10 16:45:44: Loss and accuracy at step 3887: 0.0368408, 0.012\n",
      "2017-11-10 16:45:46: Loss and accuracy at step 3888: 0.0368304, 0.0065\n",
      "2017-11-10 16:45:48: Loss and accuracy at step 3889: 0.0368797, 0.077\n",
      "2017-11-10 16:45:50: Loss and accuracy at step 3890: 0.036837, 0.0081\n",
      "2017-11-10 16:45:52: Loss and accuracy at step 3891: 0.036915, 0.0098\n",
      "2017-11-10 16:45:54: Loss and accuracy at step 3892: 0.0368686, 0.0856\n",
      "2017-11-10 16:45:56: Loss and accuracy at step 3893: 0.0368672, 0.0064\n",
      "2017-11-10 16:45:58: Loss and accuracy at step 3894: 0.0368803, 0.0176\n",
      "2017-11-10 16:46:00: Loss and accuracy at step 3895: 0.0369014, 0.0833\n",
      "2017-11-10 16:46:03: Loss and accuracy at step 3896: 0.0368813, 0.0082\n",
      "2017-11-10 16:46:05: Loss and accuracy at step 3897: 0.0368377, 0.0178\n",
      "2017-11-10 16:46:07: Loss and accuracy at step 3898: 0.0368682, 0.0368\n",
      "2017-11-10 16:46:09: Loss and accuracy at step 3899: 0.0368837, 0.0152\n",
      "2017-11-10 16:46:11: Loss and accuracy at step 3900: 0.036904, 0.0119\n",
      "2017-11-10 16:46:13: Loss and accuracy at step 3901: 0.0369356, 0.0169\n",
      "2017-11-10 16:46:15: Loss and accuracy at step 3902: 0.0368846, 0.0156\n",
      "2017-11-10 16:46:17: Loss and accuracy at step 3903: 0.0369136, 0.0104\n",
      "2017-11-10 16:46:19: Loss and accuracy at step 3904: 0.0368636, 0.0147\n",
      "2017-11-10 16:46:22: Loss and accuracy at step 3905: 0.0368726, 0.0152\n",
      "2017-11-10 16:46:24: Loss and accuracy at step 3906: 0.0369098, 0.0151\n",
      "2017-11-10 16:46:26: Loss and accuracy at step 3907: 0.0368653, 0.0137\n",
      "2017-11-10 16:46:28: Loss and accuracy at step 3908: 0.0368481, 0.0079\n",
      "2017-11-10 16:46:30: Loss and accuracy at step 3909: 0.0369207, 0.0252\n",
      "2017-11-10 16:46:32: Loss and accuracy at step 3910: 0.0368417, 0.0123\n",
      "2017-11-10 16:46:34: Loss and accuracy at step 3911: 0.0368771, 0.0091\n",
      "2017-11-10 16:46:36: Loss and accuracy at step 3912: 0.0369208, 0.0181\n",
      "2017-11-10 16:46:38: Loss and accuracy at step 3913: 0.0368957, 0.017\n",
      "2017-11-10 16:46:40: Loss and accuracy at step 3914: 0.036833, 0.0085\n",
      "2017-11-10 16:46:42: Loss and accuracy at step 3915: 0.0367595, 0.0139\n",
      "2017-11-10 16:46:44: Loss and accuracy at step 3916: 0.0368096, 0.0168\n",
      "2017-11-10 16:46:47: Loss and accuracy at step 3917: 0.0369111, 0.009\n",
      "2017-11-10 16:46:49: Loss and accuracy at step 3918: 0.0368349, 0.0123\n",
      "2017-11-10 16:46:51: Loss and accuracy at step 3919: 0.0367461, 0.0169\n",
      "2017-11-10 16:46:53: Loss and accuracy at step 3920: 0.036859, 0.0073\n",
      "2017-11-10 16:46:55: Loss and accuracy at step 3921: 0.0368524, 0.0138\n",
      "2017-11-10 16:46:57: Loss and accuracy at step 3922: 0.0368189, 0.0133\n",
      "2017-11-10 16:46:59: Loss and accuracy at step 3923: 0.0368282, 0.0079\n",
      "2017-11-10 16:47:01: Loss and accuracy at step 3924: 0.036801, 0.0131\n",
      "2017-11-10 16:47:03: Loss and accuracy at step 3925: 0.0368762, 0.0202\n",
      "2017-11-10 16:47:05: Loss and accuracy at step 3926: 0.0369718, 0.0094\n",
      "2017-11-10 16:47:07: Loss and accuracy at step 3927: 0.0368089, 0.0104\n",
      "2017-11-10 16:47:09: Loss and accuracy at step 3928: 0.0368734, 0.0148\n",
      "2017-11-10 16:47:12: Loss and accuracy at step 3929: 0.036881, 0.0101\n",
      "2017-11-10 16:47:14: Loss and accuracy at step 3930: 0.0368645, 0.0089\n",
      "2017-11-10 16:47:16: Loss and accuracy at step 3931: 0.0369333, 0.0163\n",
      "2017-11-10 16:47:18: Loss and accuracy at step 3932: 0.0368881, 0.0114\n",
      "2017-11-10 16:47:20: Loss and accuracy at step 3933: 0.0368362, 0.01\n",
      "2017-11-10 16:47:22: Loss and accuracy at step 3934: 0.0368197, 0.0105\n",
      "2017-11-10 16:47:24: Loss and accuracy at step 3935: 0.0368016, 0.0142\n",
      "2017-11-10 16:47:26: Loss and accuracy at step 3936: 0.0368683, 0.009\n",
      "2017-11-10 16:47:28: Loss and accuracy at step 3937: 0.0368105, 0.0121\n",
      "2017-11-10 16:47:30: Loss and accuracy at step 3938: 0.0367731, 0.0122\n",
      "2017-11-10 16:47:32: Loss and accuracy at step 3939: 0.0366754, 0.0105\n",
      "2017-11-10 16:47:35: Loss and accuracy at step 3940: 0.0368639, 0.0124\n",
      "2017-11-10 16:47:37: Loss and accuracy at step 3941: 0.0367838, 0.011\n",
      "2017-11-10 16:47:39: Loss and accuracy at step 3942: 0.0367376, 0.0093\n",
      "2017-11-10 16:47:41: Loss and accuracy at step 3943: 0.0367688, 0.0151\n",
      "2017-11-10 16:47:43: Loss and accuracy at step 3944: 0.0367125, 0.0133\n",
      "2017-11-10 16:47:45: Loss and accuracy at step 3945: 0.0367152, 0.01\n",
      "2017-11-10 16:47:47: Loss and accuracy at step 3946: 0.0368116, 0.0137\n",
      "2017-11-10 16:47:49: Loss and accuracy at step 3947: 0.0368076, 0.0099\n",
      "2017-11-10 16:47:51: Loss and accuracy at step 3948: 0.0367237, 0.0113\n",
      "2017-11-10 16:47:53: Loss and accuracy at step 3949: 0.0368216, 0.015\n",
      "2017-11-10 16:47:56: Loss and accuracy at step 3950: 0.0368825, 0.008\n",
      "2017-11-10 16:47:58: Loss and accuracy at step 3951: 0.0368195, 0.0179\n",
      "2017-11-10 16:48:00: Loss and accuracy at step 3952: 0.0367424, 0.0153\n",
      "2017-11-10 16:48:02: Loss and accuracy at step 3953: 0.0369063, 0.0099\n",
      "2017-11-10 16:48:04: Loss and accuracy at step 3954: 0.0368291, 0.0131\n",
      "2017-11-10 16:48:06: Loss and accuracy at step 3955: 0.0368076, 0.0137\n",
      "2017-11-10 16:48:08: Loss and accuracy at step 3956: 0.036798, 0.009\n",
      "2017-11-10 16:48:10: Loss and accuracy at step 3957: 0.0368737, 0.0092\n",
      "2017-11-10 16:48:12: Loss and accuracy at step 3958: 0.0368088, 0.0158\n",
      "2017-11-10 16:48:14: Loss and accuracy at step 3959: 0.0368287, 0.0109\n",
      "2017-11-10 16:48:17: Loss and accuracy at step 3960: 0.036717, 0.0121\n",
      "2017-11-10 16:48:19: Loss and accuracy at step 3961: 0.036803, 0.0124\n",
      "2017-11-10 16:48:21: Loss and accuracy at step 3962: 0.0368033, 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:48:23: Loss and accuracy at step 3963: 0.0369257, 0.0113\n",
      "2017-11-10 16:48:25: Loss and accuracy at step 3964: 0.0367204, 0.0125\n",
      "2017-11-10 16:48:27: Loss and accuracy at step 3965: 0.0367755, 0.0096\n",
      "2017-11-10 16:48:29: Loss and accuracy at step 3966: 0.0368036, 0.0112\n",
      "2017-11-10 16:48:31: Loss and accuracy at step 3967: 0.0367949, 0.0134\n",
      "2017-11-10 16:48:33: Loss and accuracy at step 3968: 0.0368314, 0.0089\n",
      "2017-11-10 16:48:35: Loss and accuracy at step 3969: 0.0368194, 0.0148\n",
      "2017-11-10 16:48:37: Loss and accuracy at step 3970: 0.0367718, 0.0094\n",
      "2017-11-10 16:48:40: Loss and accuracy at step 3971: 0.036765, 0.0138\n",
      "2017-11-10 16:48:42: Loss and accuracy at step 3972: 0.036838, 0.0119\n",
      "2017-11-10 16:48:44: Loss and accuracy at step 3973: 0.0367317, 0.0121\n",
      "2017-11-10 16:48:46: Loss and accuracy at step 3974: 0.0368031, 0.0113\n",
      "2017-11-10 16:48:48: Loss and accuracy at step 3975: 0.0369223, 0.0187\n",
      "2017-11-10 16:48:50: Loss and accuracy at step 3976: 0.0368702, 0.0078\n",
      "2017-11-10 16:48:52: Loss and accuracy at step 3977: 0.0368287, 0.0096\n",
      "2017-11-10 16:48:54: Loss and accuracy at step 3978: 0.0368472, 0.0196\n",
      "2017-11-10 16:48:56: Loss and accuracy at step 3979: 0.0368713, 0.0092\n",
      "2017-11-10 16:48:58: Loss and accuracy at step 3980: 0.0368134, 0.0086\n",
      "2017-11-10 16:49:00: Loss and accuracy at step 3981: 0.0368338, 0.0148\n",
      "2017-11-10 16:49:02: Loss and accuracy at step 3982: 0.0368347, 0.0132\n",
      "2017-11-10 16:49:05: Loss and accuracy at step 3983: 0.0367779, 0.0086\n",
      "2017-11-10 16:49:07: Loss and accuracy at step 3984: 0.0368071, 0.0152\n",
      "2017-11-10 16:49:09: Loss and accuracy at step 3985: 0.0366906, 0.0178\n",
      "2017-11-10 16:49:11: Loss and accuracy at step 3986: 0.0368685, 0.0092\n",
      "2017-11-10 16:49:13: Loss and accuracy at step 3987: 0.0368875, 0.0195\n",
      "2017-11-10 16:49:15: Loss and accuracy at step 3988: 0.0368637, 0.0113\n",
      "2017-11-10 16:49:17: Loss and accuracy at step 3989: 0.0368219, 0.012\n",
      "2017-11-10 16:49:19: Loss and accuracy at step 3990: 0.0368344, 0.0202\n",
      "2017-11-10 16:49:21: Loss and accuracy at step 3991: 0.0368659, 0.0078\n",
      "2017-11-10 16:49:23: Loss and accuracy at step 3992: 0.0368484, 0.0162\n",
      "2017-11-10 16:49:25: Loss and accuracy at step 3993: 0.0368323, 0.0103\n",
      "2017-11-10 16:49:28: Loss and accuracy at step 3994: 0.0368315, 0.0166\n",
      "2017-11-10 16:49:30: Loss and accuracy at step 3995: 0.0368562, 0.0116\n",
      "2017-11-10 16:49:32: Loss and accuracy at step 3996: 0.0368465, 0.0085\n",
      "2017-11-10 16:49:34: Loss and accuracy at step 3997: 0.0368246, 0.0174\n",
      "2017-11-10 16:49:36: Loss and accuracy at step 3998: 0.0367903, 0.0075\n",
      "2017-11-10 16:49:38: Loss and accuracy at step 3999: 0.0368673, 0.0109\n",
      "2017-11-10 16:49:40: Loss and accuracy at step 4000: 0.0368771, 0.0167\n",
      "2017-11-10 16:49:42: Loss and accuracy at step 4001: 0.0368211, 0.0092\n",
      "2017-11-10 16:49:44: Loss and accuracy at step 4002: 0.0368112, 0.0107\n",
      "2017-11-10 16:49:46: Loss and accuracy at step 4003: 0.0368624, 0.0157\n",
      "2017-11-10 16:49:48: Loss and accuracy at step 4004: 0.0367694, 0.01\n",
      "2017-11-10 16:49:50: Loss and accuracy at step 4005: 0.0367852, 0.008\n",
      "2017-11-10 16:49:53: Loss and accuracy at step 4006: 0.0367685, 0.0152\n",
      "2017-11-10 16:49:55: Loss and accuracy at step 4007: 0.0367921, 0.0108\n",
      "2017-11-10 16:49:57: Loss and accuracy at step 4008: 0.036848, 0.0091\n",
      "2017-11-10 16:49:59: Loss and accuracy at step 4009: 0.0368265, 0.0159\n",
      "2017-11-10 16:50:01: Loss and accuracy at step 4010: 0.0369054, 0.0088\n",
      "2017-11-10 16:50:03: Loss and accuracy at step 4011: 0.0368461, 0.0129\n",
      "2017-11-10 16:50:05: Loss and accuracy at step 4012: 0.0368144, 0.0112\n",
      "2017-11-10 16:50:07: Loss and accuracy at step 4013: 0.0369604, 0.0103\n",
      "2017-11-10 16:50:09: Loss and accuracy at step 4014: 0.0368259, 0.0124\n",
      "2017-11-10 16:50:11: Loss and accuracy at step 4015: 0.0368764, 0.0102\n",
      "2017-11-10 16:50:14: Loss and accuracy at step 4016: 0.036775, 0.0117\n",
      "2017-11-10 16:50:16: Loss and accuracy at step 4017: 0.0368072, 0.009\n",
      "2017-11-10 16:50:18: Loss and accuracy at step 4018: 0.036828, 0.012\n",
      "2017-11-10 16:50:20: Loss and accuracy at step 4019: 0.0367802, 0.0147\n",
      "2017-11-10 16:50:22: Loss and accuracy at step 4020: 0.0368141, 0.0093\n",
      "2017-11-10 16:50:24: Loss and accuracy at step 4021: 0.036789, 0.0149\n",
      "2017-11-10 16:50:26: Loss and accuracy at step 4022: 0.0368909, 0.0119\n",
      "2017-11-10 16:50:28: Loss and accuracy at step 4023: 0.0368706, 0.0098\n",
      "2017-11-10 16:50:30: Loss and accuracy at step 4024: 0.0368236, 0.0128\n",
      "2017-11-10 16:50:32: Loss and accuracy at step 4025: 0.0368426, 0.0159\n",
      "2017-11-10 16:50:34: Loss and accuracy at step 4026: 0.036819, 0.0116\n",
      "2017-11-10 16:50:36: Loss and accuracy at step 4027: 0.0367343, 0.0086\n",
      "2017-11-10 16:50:39: Loss and accuracy at step 4028: 0.0368391, 0.0185\n",
      "2017-11-10 16:50:41: Loss and accuracy at step 4029: 0.0368557, 0.0115\n",
      "2017-11-10 16:50:43: Loss and accuracy at step 4030: 0.0368361, 0.0096\n",
      "2017-11-10 16:50:45: Loss and accuracy at step 4031: 0.0368523, 0.0145\n",
      "2017-11-10 16:50:47: Loss and accuracy at step 4032: 0.0368614, 0.0098\n",
      "2017-11-10 16:50:49: Loss and accuracy at step 4033: 0.0368624, 0.0119\n",
      "2017-11-10 16:50:51: Loss and accuracy at step 4034: 0.0369353, 0.0145\n",
      "2017-11-10 16:50:53: Loss and accuracy at step 4035: 0.0368254, 0.0095\n",
      "2017-11-10 16:50:55: Loss and accuracy at step 4036: 0.0368062, 0.0121\n",
      "2017-11-10 16:50:57: Loss and accuracy at step 4037: 0.0368612, 0.0129\n",
      "2017-11-10 16:50:59: Loss and accuracy at step 4038: 0.0369121, 0.0114\n",
      "2017-11-10 16:51:01: Loss and accuracy at step 4039: 0.0368613, 0.0106\n",
      "2017-11-10 16:51:04: Loss and accuracy at step 4040: 0.0368892, 0.0106\n",
      "2017-11-10 16:51:06: Loss and accuracy at step 4041: 0.0367827, 0.0136\n",
      "2017-11-10 16:51:08: Loss and accuracy at step 4042: 0.0367697, 0.0111\n",
      "2017-11-10 16:51:10: Loss and accuracy at step 4043: 0.0368946, 0.0095\n",
      "2017-11-10 16:51:12: Loss and accuracy at step 4044: 0.0368523, 0.0123\n",
      "2017-11-10 16:51:14: Loss and accuracy at step 4045: 0.0369175, 0.0094\n",
      "2017-11-10 16:51:16: Loss and accuracy at step 4046: 0.0369241, 0.0112\n",
      "2017-11-10 16:51:18: Loss and accuracy at step 4047: 0.0367797, 0.0095\n",
      "2017-11-10 16:51:20: Loss and accuracy at step 4048: 0.0369027, 0.0127\n",
      "2017-11-10 16:51:22: Loss and accuracy at step 4049: 0.0368088, 0.0126\n",
      "2017-11-10 16:51:24: Loss and accuracy at step 4050: 0.0368099, 0.0094\n",
      "2017-11-10 16:51:26: Loss and accuracy at step 4051: 0.0368594, 0.0131\n",
      "2017-11-10 16:51:29: Loss and accuracy at step 4052: 0.0367662, 0.0113\n",
      "2017-11-10 16:51:31: Loss and accuracy at step 4053: 0.0369123, 0.0147\n",
      "2017-11-10 16:51:33: Loss and accuracy at step 4054: 0.0368516, 0.0106\n",
      "2017-11-10 16:51:35: Loss and accuracy at step 4055: 0.0367814, 0.011\n",
      "2017-11-10 16:51:37: Loss and accuracy at step 4056: 0.0367934, 0.0148\n",
      "2017-11-10 16:51:39: Loss and accuracy at step 4057: 0.0368277, 0.0058\n",
      "2017-11-10 16:51:41: Loss and accuracy at step 4058: 0.0367785, 0.0227\n",
      "2017-11-10 16:51:43: Loss and accuracy at step 4059: 0.0368526, 0.0078\n",
      "2017-11-10 16:51:45: Loss and accuracy at step 4060: 0.0368727, 0.0112\n",
      "2017-11-10 16:51:47: Loss and accuracy at step 4061: 0.0368803, 0.0147\n",
      "2017-11-10 16:51:49: Loss and accuracy at step 4062: 0.0368346, 0.01\n",
      "2017-11-10 16:51:52: Loss and accuracy at step 4063: 0.0368263, 0.0101\n",
      "2017-11-10 16:51:54: Loss and accuracy at step 4064: 0.0367186, 0.0139\n",
      "2017-11-10 16:51:56: Loss and accuracy at step 4065: 0.03683, 0.0107\n",
      "2017-11-10 16:51:58: Loss and accuracy at step 4066: 0.0367655, 0.0097\n",
      "2017-11-10 16:52:00: Loss and accuracy at step 4067: 0.0367433, 0.014\n",
      "2017-11-10 16:52:02: Loss and accuracy at step 4068: 0.0367961, 0.0096\n",
      "2017-11-10 16:52:04: Loss and accuracy at step 4069: 0.0368201, 0.011\n",
      "2017-11-10 16:52:06: Loss and accuracy at step 4070: 0.0368739, 0.0138\n",
      "2017-11-10 16:52:08: Loss and accuracy at step 4071: 0.0367928, 0.0081\n",
      "2017-11-10 16:52:10: Loss and accuracy at step 4072: 0.0368327, 0.019\n",
      "2017-11-10 16:52:13: Loss and accuracy at step 4073: 0.0367435, 0.0115\n",
      "2017-11-10 16:52:15: Loss and accuracy at step 4074: 0.0368283, 0.0096\n",
      "2017-11-10 16:52:17: Loss and accuracy at step 4075: 0.03681, 0.017\n",
      "2017-11-10 16:52:19: Loss and accuracy at step 4076: 0.0366886, 0.0091\n",
      "2017-11-10 16:52:21: Loss and accuracy at step 4077: 0.0367731, 0.0146\n",
      "2017-11-10 16:52:23: Loss and accuracy at step 4078: 0.0368563, 0.007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:52:25: Loss and accuracy at step 4079: 0.0367928, 0.014\n",
      "2017-11-10 16:52:27: Loss and accuracy at step 4080: 0.0368191, 0.0084\n",
      "2017-11-10 16:52:29: Loss and accuracy at step 4081: 0.0368141, 0.0108\n",
      "2017-11-10 16:52:31: Loss and accuracy at step 4082: 0.0367998, 0.0122\n",
      "2017-11-10 16:52:33: Loss and accuracy at step 4083: 0.0367979, 0.0093\n",
      "2017-11-10 16:52:36: Loss and accuracy at step 4084: 0.0368539, 0.0133\n",
      "2017-11-10 16:52:38: Loss and accuracy at step 4085: 0.0368349, 0.0098\n",
      "2017-11-10 16:52:40: Loss and accuracy at step 4086: 0.0368211, 0.0134\n",
      "2017-11-10 16:52:42: Loss and accuracy at step 4087: 0.0367957, 0.0162\n",
      "2017-11-10 16:52:44: Loss and accuracy at step 4088: 0.0368065, 0.0081\n",
      "2017-11-10 16:52:46: Loss and accuracy at step 4089: 0.0368742, 0.0118\n",
      "2017-11-10 16:52:48: Loss and accuracy at step 4090: 0.036836, 0.0139\n",
      "2017-11-10 16:52:50: Loss and accuracy at step 4091: 0.0367671, 0.0098\n",
      "2017-11-10 16:52:52: Loss and accuracy at step 4092: 0.036747, 0.0117\n",
      "2017-11-10 16:52:54: Loss and accuracy at step 4093: 0.0367957, 0.0114\n",
      "2017-11-10 16:52:56: Loss and accuracy at step 4094: 0.0367765, 0.0158\n",
      "2017-11-10 16:52:58: Loss and accuracy at step 4095: 0.0368515, 0.0081\n",
      "2017-11-10 16:53:01: Loss and accuracy at step 4096: 0.0368773, 0.0127\n",
      "2017-11-10 16:53:03: Loss and accuracy at step 4097: 0.0367827, 0.0109\n",
      "2017-11-10 16:53:05: Loss and accuracy at step 4098: 0.0368749, 0.0098\n",
      "2017-11-10 16:53:07: Loss and accuracy at step 4099: 0.0368071, 0.01\n",
      "2017-11-10 16:53:09: Loss and accuracy at step 4100: 0.036832, 0.0146\n",
      "2017-11-10 16:53:11: Loss and accuracy at step 4101: 0.0368602, 0.011\n",
      "2017-11-10 16:53:13: Loss and accuracy at step 4102: 0.0368316, 0.0085\n",
      "2017-11-10 16:53:15: Loss and accuracy at step 4103: 0.036778, 0.0137\n",
      "2017-11-10 16:53:17: Loss and accuracy at step 4104: 0.0367046, 0.0092\n",
      "2017-11-10 16:53:19: Loss and accuracy at step 4105: 0.0367964, 0.02\n",
      "2017-11-10 16:53:21: Loss and accuracy at step 4106: 0.0368289, 0.0111\n",
      "2017-11-10 16:53:24: Loss and accuracy at step 4107: 0.0367412, 0.0083\n",
      "2017-11-10 16:53:26: Loss and accuracy at step 4108: 0.036821, 0.0155\n",
      "2017-11-10 16:53:28: Loss and accuracy at step 4109: 0.0369095, 0.0102\n",
      "2017-11-10 16:53:30: Loss and accuracy at step 4110: 0.0368351, 0.0095\n",
      "2017-11-10 16:53:32: Loss and accuracy at step 4111: 0.0368694, 0.0108\n",
      "2017-11-10 16:53:34: Loss and accuracy at step 4112: 0.0368327, 0.0134\n",
      "2017-11-10 16:53:36: Loss and accuracy at step 4113: 0.0367663, 0.0112\n",
      "2017-11-10 16:53:38: Loss and accuracy at step 4114: 0.0368643, 0.0111\n",
      "2017-11-10 16:53:40: Loss and accuracy at step 4115: 0.0367562, 0.015\n",
      "2017-11-10 16:53:42: Loss and accuracy at step 4116: 0.0368681, 0.0121\n",
      "2017-11-10 16:53:44: Loss and accuracy at step 4117: 0.0368534, 0.0094\n",
      "2017-11-10 16:53:47: Loss and accuracy at step 4118: 0.0368333, 0.0117\n",
      "2017-11-10 16:53:49: Loss and accuracy at step 4119: 0.03683, 0.0127\n",
      "2017-11-10 16:53:51: Loss and accuracy at step 4120: 0.0368546, 0.0148\n",
      "2017-11-10 16:53:53: Loss and accuracy at step 4121: 0.0368274, 0.0078\n",
      "2017-11-10 16:53:55: Loss and accuracy at step 4122: 0.0367682, 0.0256\n",
      "2017-11-10 16:53:57: Loss and accuracy at step 4123: 0.0368243, 0.012\n",
      "2017-11-10 16:53:59: Loss and accuracy at step 4124: 0.0368382, 0.008\n",
      "2017-11-10 16:54:01: Loss and accuracy at step 4125: 0.0367773, 0.0219\n",
      "2017-11-10 16:54:03: Loss and accuracy at step 4126: 0.0368088, 0.0073\n",
      "2017-11-10 16:54:05: Loss and accuracy at step 4127: 0.0367877, 0.0121\n",
      "2017-11-10 16:54:07: Loss and accuracy at step 4128: 0.0367516, 0.0111\n",
      "2017-11-10 16:54:09: Loss and accuracy at step 4129: 0.0368484, 0.0109\n",
      "2017-11-10 16:54:12: Loss and accuracy at step 4130: 0.0368394, 0.0156\n",
      "2017-11-10 16:54:14: Loss and accuracy at step 4131: 0.0368195, 0.0089\n",
      "2017-11-10 16:54:16: Loss and accuracy at step 4132: 0.0368103, 0.0201\n",
      "2017-11-10 16:54:18: Loss and accuracy at step 4133: 0.0369102, 0.0069\n",
      "2017-11-10 16:54:20: Loss and accuracy at step 4134: 0.0368442, 0.0132\n",
      "2017-11-10 16:54:22: Loss and accuracy at step 4135: 0.0368401, 0.0141\n",
      "2017-11-10 16:54:24: Loss and accuracy at step 4136: 0.036881, 0.0096\n",
      "2017-11-10 16:54:26: Loss and accuracy at step 4137: 0.0368827, 0.0139\n",
      "2017-11-10 16:54:28: Loss and accuracy at step 4138: 0.036928, 0.0076\n",
      "2017-11-10 16:54:30: Loss and accuracy at step 4139: 0.0368907, 0.0159\n",
      "2017-11-10 16:54:33: Loss and accuracy at step 4140: 0.036878, 0.0077\n",
      "2017-11-10 16:54:35: Loss and accuracy at step 4141: 0.0368124, 0.0118\n",
      "2017-11-10 16:54:37: Loss and accuracy at step 4142: 0.0368355, 0.0247\n",
      "2017-11-10 16:54:39: Loss and accuracy at step 4143: 0.0369337, 0.0073\n",
      "2017-11-10 16:54:41: Loss and accuracy at step 4144: 0.0368495, 0.0144\n",
      "2017-11-10 16:54:43: Loss and accuracy at step 4145: 0.036874, 0.0145\n",
      "2017-11-10 16:54:45: Loss and accuracy at step 4146: 0.0367745, 0.0089\n",
      "2017-11-10 16:54:47: Loss and accuracy at step 4147: 0.0368417, 0.0163\n",
      "2017-11-10 16:54:49: Loss and accuracy at step 4148: 0.036826, 0.0104\n",
      "2017-11-10 16:54:51: Loss and accuracy at step 4149: 0.0369124, 0.0102\n",
      "2017-11-10 16:54:53: Loss and accuracy at step 4150: 0.0368916, 0.011\n",
      "2017-11-10 16:54:55: Loss and accuracy at step 4151: 0.036894, 0.0116\n",
      "2017-11-10 16:54:58: Loss and accuracy at step 4152: 0.0367745, 0.0098\n",
      "2017-11-10 16:55:00: Loss and accuracy at step 4153: 0.036879, 0.0098\n",
      "2017-11-10 16:55:02: Loss and accuracy at step 4154: 0.0367762, 0.0137\n",
      "2017-11-10 16:55:04: Loss and accuracy at step 4155: 0.0367944, 0.0077\n",
      "2017-11-10 16:55:06: Loss and accuracy at step 4156: 0.0368317, 0.0163\n",
      "2017-11-10 16:55:08: Loss and accuracy at step 4157: 0.036835, 0.0134\n",
      "2017-11-10 16:55:10: Loss and accuracy at step 4158: 0.0368233, 0.0072\n",
      "2017-11-10 16:55:12: Loss and accuracy at step 4159: 0.0368286, 0.0222\n",
      "2017-11-10 16:55:14: Loss and accuracy at step 4160: 0.0368178, 0.0092\n",
      "2017-11-10 16:55:16: Loss and accuracy at step 4161: 0.0367266, 0.0101\n",
      "2017-11-10 16:55:18: Loss and accuracy at step 4162: 0.0367896, 0.0268\n",
      "2017-11-10 16:55:20: Loss and accuracy at step 4163: 0.0368293, 0.0048\n",
      "2017-11-10 16:55:23: Loss and accuracy at step 4164: 0.0368002, 0.0219\n",
      "2017-11-10 16:55:25: Loss and accuracy at step 4165: 0.0368593, 0.0314\n",
      "2017-11-10 16:55:27: Loss and accuracy at step 4166: 0.0368434, 0.009\n",
      "2017-11-10 16:55:29: Loss and accuracy at step 4167: 0.0368424, 0.0153\n",
      "2017-11-10 16:55:31: Loss and accuracy at step 4168: 0.0367276, 0.0313\n",
      "2017-11-10 16:55:33: Loss and accuracy at step 4169: 0.0368533, 0.011\n",
      "2017-11-10 16:55:35: Loss and accuracy at step 4170: 0.0369094, 0.012\n",
      "2017-11-10 16:55:37: Loss and accuracy at step 4171: 0.0367845, 0.0186\n",
      "2017-11-10 16:55:39: Loss and accuracy at step 4172: 0.0367976, 0.0139\n",
      "2017-11-10 16:55:41: Loss and accuracy at step 4173: 0.0368249, 0.008\n",
      "2017-11-10 16:55:43: Loss and accuracy at step 4174: 0.0367411, 0.0193\n",
      "2017-11-10 16:55:46: Loss and accuracy at step 4175: 0.0367884, 0.0108\n",
      "2017-11-10 16:55:48: Loss and accuracy at step 4176: 0.0368986, 0.008\n",
      "2017-11-10 16:55:50: Loss and accuracy at step 4177: 0.0367755, 0.0196\n",
      "2017-11-10 16:55:52: Loss and accuracy at step 4178: 0.0368361, 0.017\n",
      "2017-11-10 16:55:54: Loss and accuracy at step 4179: 0.0367953, 0.0081\n",
      "2017-11-10 16:55:56: Loss and accuracy at step 4180: 0.0368396, 0.0127\n",
      "2017-11-10 16:55:58: Loss and accuracy at step 4181: 0.0368787, 0.0189\n",
      "2017-11-10 16:56:00: Loss and accuracy at step 4182: 0.0368738, 0.0077\n",
      "2017-11-10 16:56:02: Loss and accuracy at step 4183: 0.0368112, 0.0097\n",
      "2017-11-10 16:56:04: Loss and accuracy at step 4184: 0.0367854, 0.0224\n",
      "2017-11-10 16:56:06: Loss and accuracy at step 4185: 0.0367961, 0.0088\n",
      "2017-11-10 16:56:08: Loss and accuracy at step 4186: 0.0368325, 0.0081\n",
      "2017-11-10 16:56:11: Loss and accuracy at step 4187: 0.0367385, 0.018\n",
      "2017-11-10 16:56:13: Loss and accuracy at step 4188: 0.0367447, 0.0103\n",
      "2017-11-10 16:56:15: Loss and accuracy at step 4189: 0.0367883, 0.0105\n",
      "2017-11-10 16:56:17: Loss and accuracy at step 4190: 0.0368086, 0.0167\n",
      "2017-11-10 16:56:19: Loss and accuracy at step 4191: 0.0367982, 0.0079\n",
      "2017-11-10 16:56:21: Loss and accuracy at step 4192: 0.0368355, 0.017\n",
      "2017-11-10 16:56:23: Loss and accuracy at step 4193: 0.0367968, 0.0084\n",
      "2017-11-10 16:56:25: Loss and accuracy at step 4194: 0.0369038, 0.0104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 16:56:27: Loss and accuracy at step 4195: 0.0368094, 0.0155\n",
      "2017-11-10 16:56:30: Loss and accuracy at step 4196: 0.0367774, 0.0077\n",
      "2017-11-10 16:56:32: Loss and accuracy at step 4197: 0.0369062, 0.0171\n",
      "2017-11-10 16:56:34: Loss and accuracy at step 4198: 0.036871, 0.0074\n",
      "2017-11-10 16:56:36: Loss and accuracy at step 4199: 0.0368671, 0.011\n",
      "2017-11-10 16:56:38: Loss and accuracy at step 4200: 0.0367947, 0.0109\n",
      "2017-11-10 16:56:40: Loss and accuracy at step 4201: 0.0367712, 0.0081\n",
      "2017-11-10 16:56:42: Loss and accuracy at step 4202: 0.0368221, 0.0138\n",
      "2017-11-10 16:56:44: Loss and accuracy at step 4203: 0.0367988, 0.0106\n",
      "2017-11-10 16:56:46: Loss and accuracy at step 4204: 0.0368292, 0.01\n",
      "2017-11-10 16:56:48: Loss and accuracy at step 4205: 0.0368101, 0.0116\n",
      "2017-11-10 16:56:50: Loss and accuracy at step 4206: 0.0367958, 0.0152\n",
      "2017-11-10 16:56:52: Loss and accuracy at step 4207: 0.0369012, 0.0108\n",
      "2017-11-10 16:56:54: Loss and accuracy at step 4208: 0.0367937, 0.0066\n",
      "2017-11-10 16:56:57: Loss and accuracy at step 4209: 0.0368624, 0.0227\n",
      "2017-11-10 16:56:59: Loss and accuracy at step 4210: 0.0367536, 0.0073\n",
      "2017-11-10 16:57:01: Loss and accuracy at step 4211: 0.0368194, 0.0132\n",
      "2017-11-10 16:57:03: Loss and accuracy at step 4212: 0.0367605, 0.0158\n",
      "2017-11-10 16:57:05: Loss and accuracy at step 4213: 0.0367809, 0.0117\n",
      "2017-11-10 16:57:07: Loss and accuracy at step 4214: 0.0368229, 0.0161\n",
      "2017-11-10 16:57:09: Loss and accuracy at step 4215: 0.0367629, 0.0085\n",
      "2017-11-10 16:57:11: Loss and accuracy at step 4216: 0.0368003, 0.0127\n",
      "2017-11-10 16:57:13: Loss and accuracy at step 4217: 0.0368551, 0.0161\n",
      "2017-11-10 16:57:15: Loss and accuracy at step 4218: 0.0368979, 0.0102\n",
      "2017-11-10 16:57:17: Loss and accuracy at step 4219: 0.0367949, 0.0123\n",
      "2017-11-10 16:57:20: Loss and accuracy at step 4220: 0.0368033, 0.0145\n",
      "2017-11-10 16:57:22: Loss and accuracy at step 4221: 0.0367814, 0.0085\n",
      "2017-11-10 16:57:24: Loss and accuracy at step 4222: 0.0368029, 0.0168\n",
      "2017-11-10 16:57:26: Loss and accuracy at step 4223: 0.0367509, 0.0091\n",
      "2017-11-10 16:57:28: Loss and accuracy at step 4224: 0.0368189, 0.0117\n",
      "2017-11-10 16:57:30: Loss and accuracy at step 4225: 0.0367845, 0.0139\n",
      "2017-11-10 16:57:32: Loss and accuracy at step 4226: 0.0368235, 0.0098\n",
      "2017-11-10 16:57:34: Loss and accuracy at step 4227: 0.0368309, 0.0117\n",
      "2017-11-10 16:57:36: Loss and accuracy at step 4228: 0.0367969, 0.0152\n",
      "2017-11-10 16:57:38: Loss and accuracy at step 4229: 0.0368112, 0.0136\n",
      "2017-11-10 16:57:41: Loss and accuracy at step 4230: 0.0368432, 0.0087\n",
      "2017-11-10 16:57:43: Loss and accuracy at step 4231: 0.0368448, 0.0148\n",
      "2017-11-10 16:57:45: Loss and accuracy at step 4232: 0.0367765, 0.0138\n",
      "2017-11-10 16:57:47: Loss and accuracy at step 4233: 0.036727, 0.0127\n",
      "2017-11-10 16:57:49: Loss and accuracy at step 4234: 0.0367865, 0.0125\n",
      "2017-11-10 16:57:51: Loss and accuracy at step 4235: 0.0368148, 0.0136\n",
      "2017-11-10 16:57:53: Loss and accuracy at step 4236: 0.0369222, 0.0117\n",
      "2017-11-10 16:57:55: Loss and accuracy at step 4237: 0.0368532, 0.0143\n",
      "2017-11-10 16:57:57: Loss and accuracy at step 4238: 0.0368433, 0.0147\n",
      "2017-11-10 16:57:59: Loss and accuracy at step 4239: 0.0368346, 0.0145\n",
      "2017-11-10 16:58:01: Loss and accuracy at step 4240: 0.0367933, 0.0118\n",
      "2017-11-10 16:58:03: Loss and accuracy at step 4241: 0.0367912, 0.0162\n",
      "2017-11-10 16:58:06: Loss and accuracy at step 4242: 0.0368421, 0.0134\n",
      "2017-11-10 16:58:08: Loss and accuracy at step 4243: 0.0368723, 0.0114\n",
      "2017-11-10 16:58:10: Loss and accuracy at step 4244: 0.0368638, 0.018\n",
      "2017-11-10 16:58:12: Loss and accuracy at step 4245: 0.0368367, 0.0136\n",
      "2017-11-10 16:58:14: Loss and accuracy at step 4246: 0.0369027, 0.0092\n",
      "2017-11-10 16:58:16: Loss and accuracy at step 4247: 0.0368768, 0.0133\n",
      "2017-11-10 16:58:18: Loss and accuracy at step 4248: 0.0367976, 0.0188\n",
      "2017-11-10 16:58:20: Loss and accuracy at step 4249: 0.0368349, 0.0092\n",
      "2017-11-10 16:58:22: Loss and accuracy at step 4250: 0.0367629, 0.0126\n",
      "2017-11-10 16:58:25: Loss and accuracy at step 4251: 0.0368158, 0.0181\n",
      "2017-11-10 16:58:27: Loss and accuracy at step 4252: 0.0368936, 0.0117\n",
      "2017-11-10 16:58:29: Loss and accuracy at step 4253: 0.0368097, 0.0121\n",
      "2017-11-10 16:58:31: Loss and accuracy at step 4254: 0.036866, 0.0184\n",
      "2017-11-10 16:58:33: Loss and accuracy at step 4255: 0.0368128, 0.0134\n",
      "2017-11-10 16:58:35: Loss and accuracy at step 4256: 0.0368416, 0.0096\n",
      "2017-11-10 16:58:37: Loss and accuracy at step 4257: 0.0368358, 0.0188\n",
      "2017-11-10 16:58:39: Loss and accuracy at step 4258: 0.0368796, 0.0146\n",
      "2017-11-10 16:58:41: Loss and accuracy at step 4259: 0.0367806, 0.0081\n",
      "2017-11-10 16:58:43: Loss and accuracy at step 4260: 0.0367432, 0.0175\n",
      "2017-11-10 16:58:45: Loss and accuracy at step 4261: 0.0367948, 0.0112\n",
      "2017-11-10 16:58:47: Loss and accuracy at step 4262: 0.0367764, 0.0137\n",
      "2017-11-10 16:58:50: Loss and accuracy at step 4263: 0.0367927, 0.0126\n",
      "2017-11-10 16:58:52: Loss and accuracy at step 4264: 0.0367763, 0.0105\n",
      "2017-11-10 16:58:54: Loss and accuracy at step 4265: 0.0368097, 0.0222\n",
      "2017-11-10 16:58:56: Loss and accuracy at step 4266: 0.0368769, 0.0061\n",
      "2017-11-10 16:58:58: Loss and accuracy at step 4267: 0.0368242, 0.0142\n",
      "2017-11-10 16:59:00: Loss and accuracy at step 4268: 0.0368342, 0.0253\n",
      "2017-11-10 16:59:02: Loss and accuracy at step 4269: 0.0368688, 0.0076\n",
      "2017-11-10 16:59:04: Loss and accuracy at step 4270: 0.036848, 0.0192\n",
      "2017-11-10 16:59:06: Loss and accuracy at step 4271: 0.0368984, 0.0167\n",
      "2017-11-10 16:59:08: Loss and accuracy at step 4272: 0.0368831, 0.0109\n",
      "2017-11-10 16:59:10: Loss and accuracy at step 4273: 0.0368499, 0.0196\n",
      "2017-11-10 16:59:13: Loss and accuracy at step 4274: 0.0369048, 0.0135\n",
      "2017-11-10 16:59:15: Loss and accuracy at step 4275: 0.0367932, 0.0121\n",
      "2017-11-10 16:59:17: Loss and accuracy at step 4276: 0.0368485, 0.0218\n",
      "2017-11-10 16:59:19: Loss and accuracy at step 4277: 0.0368722, 0.0189\n",
      "2017-11-10 16:59:21: Loss and accuracy at step 4278: 0.036824, 0.0102\n",
      "2017-11-10 16:59:23: Loss and accuracy at step 4279: 0.0368223, 0.0231\n",
      "2017-11-10 16:59:25: Loss and accuracy at step 4280: 0.0368052, 0.0155\n",
      "2017-11-10 16:59:27: Loss and accuracy at step 4281: 0.0368475, 0.0066\n",
      "2017-11-10 16:59:29: Loss and accuracy at step 4282: 0.0368552, 0.0185\n",
      "2017-11-10 16:59:31: Loss and accuracy at step 4283: 0.0368106, 0.0174\n",
      "2017-11-10 16:59:33: Loss and accuracy at step 4284: 0.0367735, 0.0121\n",
      "2017-11-10 16:59:36: Loss and accuracy at step 4285: 0.0368329, 0.012\n",
      "2017-11-10 16:59:38: Loss and accuracy at step 4286: 0.0367511, 0.018\n",
      "2017-11-10 16:59:40: Loss and accuracy at step 4287: 0.0368581, 0.0121\n",
      "2017-11-10 16:59:42: Loss and accuracy at step 4288: 0.0367945, 0.0098\n",
      "2017-11-10 16:59:44: Loss and accuracy at step 4289: 0.0368436, 0.0168\n",
      "2017-11-10 16:59:46: Loss and accuracy at step 4290: 0.0368162, 0.0161\n",
      "2017-11-10 16:59:48: Loss and accuracy at step 4291: 0.0368876, 0.0061\n",
      "2017-11-10 16:59:50: Loss and accuracy at step 4292: 0.0368095, 0.0276\n",
      "2017-11-10 16:59:52: Loss and accuracy at step 4293: 0.0368225, 0.0104\n",
      "2017-11-10 16:59:54: Loss and accuracy at step 4294: 0.0367865, 0.0104\n",
      "2017-11-10 16:59:56: Loss and accuracy at step 4295: 0.0367901, 0.0342\n",
      "2017-11-10 16:59:58: Loss and accuracy at step 4296: 0.0368449, 0.0114\n",
      "2017-11-10 17:00:01: Loss and accuracy at step 4297: 0.0368088, 0.0094\n",
      "2017-11-10 17:00:03: Loss and accuracy at step 4298: 0.0368722, 0.0304\n",
      "2017-11-10 17:00:05: Loss and accuracy at step 4299: 0.0367769, 0.012\n",
      "2017-11-10 17:00:07: Loss and accuracy at step 4300: 0.0368015, 0.0091\n",
      "2017-11-10 17:00:09: Loss and accuracy at step 4301: 0.0367747, 0.0221\n",
      "2017-11-10 17:00:11: Loss and accuracy at step 4302: 0.0368808, 0.0101\n",
      "2017-11-10 17:00:13: Loss and accuracy at step 4303: 0.0368517, 0.0124\n",
      "2017-11-10 17:00:15: Loss and accuracy at step 4304: 0.0367601, 0.0132\n",
      "2017-11-10 17:00:17: Loss and accuracy at step 4305: 0.0368657, 0.0133\n",
      "2017-11-10 17:00:19: Loss and accuracy at step 4306: 0.0369273, 0.0091\n",
      "2017-11-10 17:00:22: Loss and accuracy at step 4307: 0.0368516, 0.0131\n",
      "2017-11-10 17:00:24: Loss and accuracy at step 4308: 0.0368653, 0.017\n",
      "2017-11-10 17:00:26: Loss and accuracy at step 4309: 0.0367857, 0.0102\n",
      "2017-11-10 17:00:28: Loss and accuracy at step 4310: 0.0367679, 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:00:30: Loss and accuracy at step 4311: 0.0368099, 0.012\n",
      "2017-11-10 17:00:32: Loss and accuracy at step 4312: 0.0367368, 0.0099\n",
      "2017-11-10 17:00:34: Loss and accuracy at step 4313: 0.0368286, 0.0156\n",
      "2017-11-10 17:00:36: Loss and accuracy at step 4314: 0.0367967, 0.0129\n",
      "2017-11-10 17:00:38: Loss and accuracy at step 4315: 0.0367413, 0.0129\n",
      "2017-11-10 17:00:40: Loss and accuracy at step 4316: 0.0368873, 0.0169\n",
      "2017-11-10 17:00:42: Loss and accuracy at step 4317: 0.0367146, 0.006\n",
      "2017-11-10 17:00:45: Loss and accuracy at step 4318: 0.0368396, 0.0148\n",
      "2017-11-10 17:00:47: Loss and accuracy at step 4319: 0.0368465, 0.0129\n",
      "2017-11-10 17:00:49: Loss and accuracy at step 4320: 0.0367716, 0.0094\n",
      "2017-11-10 17:00:51: Loss and accuracy at step 4321: 0.0368203, 0.0172\n",
      "2017-11-10 17:00:53: Loss and accuracy at step 4322: 0.036786, 0.0109\n",
      "2017-11-10 17:00:55: Loss and accuracy at step 4323: 0.0367662, 0.0142\n",
      "2017-11-10 17:00:57: Loss and accuracy at step 4324: 0.0368345, 0.011\n",
      "2017-11-10 17:00:59: Loss and accuracy at step 4325: 0.0367639, 0.0091\n",
      "2017-11-10 17:01:01: Loss and accuracy at step 4326: 0.0368695, 0.0161\n",
      "2017-11-10 17:01:03: Loss and accuracy at step 4327: 0.0368071, 0.0084\n",
      "2017-11-10 17:01:05: Loss and accuracy at step 4328: 0.0368165, 0.0118\n",
      "2017-11-10 17:01:08: Loss and accuracy at step 4329: 0.0367891, 0.0163\n",
      "2017-11-10 17:01:10: Loss and accuracy at step 4330: 0.0367956, 0.0093\n",
      "2017-11-10 17:01:12: Loss and accuracy at step 4331: 0.0367657, 0.0121\n",
      "2017-11-10 17:01:14: Loss and accuracy at step 4332: 0.0367882, 0.0129\n",
      "2017-11-10 17:01:16: Loss and accuracy at step 4333: 0.0368413, 0.0136\n",
      "2017-11-10 17:01:18: Loss and accuracy at step 4334: 0.036774, 0.008\n",
      "2017-11-10 17:01:20: Loss and accuracy at step 4335: 0.0368599, 0.0114\n",
      "2017-11-10 17:01:22: Loss and accuracy at step 4336: 0.036878, 0.0182\n",
      "2017-11-10 17:01:24: Loss and accuracy at step 4337: 0.0368445, 0.007\n",
      "2017-11-10 17:01:26: Loss and accuracy at step 4338: 0.0368626, 0.0127\n",
      "2017-11-10 17:01:28: Loss and accuracy at step 4339: 0.0367399, 0.012\n",
      "2017-11-10 17:01:31: Loss and accuracy at step 4340: 0.0368182, 0.0128\n",
      "2017-11-10 17:01:33: Loss and accuracy at step 4341: 0.0368202, 0.0167\n",
      "2017-11-10 17:01:35: Loss and accuracy at step 4342: 0.036804, 0.0073\n",
      "2017-11-10 17:01:37: Loss and accuracy at step 4343: 0.0367558, 0.0141\n",
      "2017-11-10 17:01:39: Loss and accuracy at step 4344: 0.0368327, 0.0127\n",
      "2017-11-10 17:01:41: Loss and accuracy at step 4345: 0.0367685, 0.0066\n",
      "2017-11-10 17:01:43: Loss and accuracy at step 4346: 0.0367525, 0.0148\n",
      "2017-11-10 17:01:45: Loss and accuracy at step 4347: 0.0367139, 0.0165\n",
      "2017-11-10 17:01:47: Loss and accuracy at step 4348: 0.0367505, 0.0101\n",
      "2017-11-10 17:01:49: Loss and accuracy at step 4349: 0.0367709, 0.0136\n",
      "2017-11-10 17:01:52: Loss and accuracy at step 4350: 0.0368296, 0.0141\n",
      "2017-11-10 17:01:54: Loss and accuracy at step 4351: 0.0368022, 0.0142\n",
      "2017-11-10 17:01:56: Loss and accuracy at step 4352: 0.0367822, 0.0089\n",
      "2017-11-10 17:01:58: Loss and accuracy at step 4353: 0.0368682, 0.0112\n",
      "2017-11-10 17:02:00: Loss and accuracy at step 4354: 0.0368444, 0.0113\n",
      "2017-11-10 17:02:02: Loss and accuracy at step 4355: 0.0367821, 0.0149\n",
      "2017-11-10 17:02:04: Loss and accuracy at step 4356: 0.036751, 0.0112\n",
      "2017-11-10 17:02:06: Loss and accuracy at step 4357: 0.0368555, 0.0081\n",
      "2017-11-10 17:02:08: Loss and accuracy at step 4358: 0.0366788, 0.0152\n",
      "2017-11-10 17:02:10: Loss and accuracy at step 4359: 0.0367434, 0.0132\n",
      "2017-11-10 17:02:12: Loss and accuracy at step 4360: 0.0368831, 0.0083\n",
      "2017-11-10 17:02:15: Loss and accuracy at step 4361: 0.036813, 0.0135\n",
      "2017-11-10 17:02:17: Loss and accuracy at step 4362: 0.0368329, 0.0136\n",
      "2017-11-10 17:02:19: Loss and accuracy at step 4363: 0.0368121, 0.0115\n",
      "2017-11-10 17:02:21: Loss and accuracy at step 4364: 0.0368619, 0.0121\n",
      "2017-11-10 17:02:23: Loss and accuracy at step 4365: 0.0367362, 0.01\n",
      "2017-11-10 17:02:25: Loss and accuracy at step 4366: 0.0368919, 0.0131\n",
      "2017-11-10 17:02:27: Loss and accuracy at step 4367: 0.0368191, 0.0082\n",
      "2017-11-10 17:02:29: Loss and accuracy at step 4368: 0.0367624, 0.0141\n",
      "2017-11-10 17:02:31: Loss and accuracy at step 4369: 0.036865, 0.0155\n",
      "2017-11-10 17:02:33: Loss and accuracy at step 4370: 0.0368287, 0.0088\n",
      "2017-11-10 17:02:35: Loss and accuracy at step 4371: 0.0368265, 0.014\n",
      "2017-11-10 17:02:38: Loss and accuracy at step 4372: 0.0367761, 0.0114\n",
      "2017-11-10 17:02:40: Loss and accuracy at step 4373: 0.0368718, 0.0127\n",
      "2017-11-10 17:02:42: Loss and accuracy at step 4374: 0.0368069, 0.0116\n",
      "2017-11-10 17:02:44: Loss and accuracy at step 4375: 0.0368425, 0.0105\n",
      "2017-11-10 17:02:46: Loss and accuracy at step 4376: 0.0368119, 0.0154\n",
      "2017-11-10 17:02:48: Loss and accuracy at step 4377: 0.0368201, 0.0087\n",
      "2017-11-10 17:02:50: Loss and accuracy at step 4378: 0.0368306, 0.0098\n",
      "2017-11-10 17:02:52: Loss and accuracy at step 4379: 0.0368269, 0.0205\n",
      "2017-11-10 17:02:54: Loss and accuracy at step 4380: 0.0368688, 0.0062\n",
      "2017-11-10 17:02:56: Loss and accuracy at step 4381: 0.0368523, 0.0105\n",
      "2017-11-10 17:02:58: Loss and accuracy at step 4382: 0.0368433, 0.0119\n",
      "2017-11-10 17:03:00: Loss and accuracy at step 4383: 0.0368101, 0.0134\n",
      "2017-11-10 17:03:02: Loss and accuracy at step 4384: 0.0367911, 0.0106\n",
      "2017-11-10 17:03:05: Loss and accuracy at step 4385: 0.0368241, 0.0111\n",
      "2017-11-10 17:03:07: Loss and accuracy at step 4386: 0.0368165, 0.0117\n",
      "2017-11-10 17:03:09: Loss and accuracy at step 4387: 0.0367814, 0.0149\n",
      "2017-11-10 17:03:11: Loss and accuracy at step 4388: 0.0368038, 0.0104\n",
      "2017-11-10 17:03:13: Loss and accuracy at step 4389: 0.036828, 0.012\n",
      "2017-11-10 17:03:15: Loss and accuracy at step 4390: 0.0368402, 0.0135\n",
      "2017-11-10 17:03:17: Loss and accuracy at step 4391: 0.0367527, 0.0098\n",
      "2017-11-10 17:03:19: Loss and accuracy at step 4392: 0.0368437, 0.014\n",
      "2017-11-10 17:03:21: Loss and accuracy at step 4393: 0.0367546, 0.0112\n",
      "2017-11-10 17:03:23: Loss and accuracy at step 4394: 0.0367876, 0.0079\n",
      "2017-11-10 17:03:25: Loss and accuracy at step 4395: 0.03681, 0.0106\n",
      "2017-11-10 17:03:27: Loss and accuracy at step 4396: 0.0367758, 0.0143\n",
      "2017-11-10 17:03:30: Loss and accuracy at step 4397: 0.0368301, 0.0086\n",
      "2017-11-10 17:03:32: Loss and accuracy at step 4398: 0.0368615, 0.0124\n",
      "2017-11-10 17:03:34: Loss and accuracy at step 4399: 0.0368251, 0.0182\n",
      "2017-11-10 17:03:36: Loss and accuracy at step 4400: 0.0368508, 0.0097\n",
      "2017-11-10 17:03:38: Loss and accuracy at step 4401: 0.0368483, 0.013\n",
      "2017-11-10 17:03:40: Loss and accuracy at step 4402: 0.0368317, 0.0102\n",
      "2017-11-10 17:03:42: Loss and accuracy at step 4403: 0.0368727, 0.0134\n",
      "2017-11-10 17:03:44: Loss and accuracy at step 4404: 0.0368277, 0.0123\n",
      "2017-11-10 17:03:46: Loss and accuracy at step 4405: 0.0368118, 0.0113\n",
      "2017-11-10 17:03:48: Loss and accuracy at step 4406: 0.0367722, 0.0118\n",
      "2017-11-10 17:03:50: Loss and accuracy at step 4407: 0.0368128, 0.0158\n",
      "2017-11-10 17:03:52: Loss and accuracy at step 4408: 0.0368081, 0.0102\n",
      "2017-11-10 17:03:55: Loss and accuracy at step 4409: 0.0368973, 0.0102\n",
      "2017-11-10 17:03:57: Loss and accuracy at step 4410: 0.0366928, 0.0204\n",
      "2017-11-10 17:03:59: Loss and accuracy at step 4411: 0.0367907, 0.0098\n",
      "2017-11-10 17:04:01: Loss and accuracy at step 4412: 0.0368104, 0.0121\n",
      "2017-11-10 17:04:03: Loss and accuracy at step 4413: 0.0368068, 0.0125\n",
      "2017-11-10 17:04:05: Loss and accuracy at step 4414: 0.0367256, 0.0087\n",
      "2017-11-10 17:04:07: Loss and accuracy at step 4415: 0.0367814, 0.0138\n",
      "2017-11-10 17:04:09: Loss and accuracy at step 4416: 0.0367607, 0.0078\n",
      "2017-11-10 17:04:11: Loss and accuracy at step 4417: 0.0368209, 0.017\n",
      "2017-11-10 17:04:14: Loss and accuracy at step 4418: 0.0368418, 0.0128\n",
      "2017-11-10 17:04:16: Loss and accuracy at step 4419: 0.0368535, 0.0127\n",
      "2017-11-10 17:04:18: Loss and accuracy at step 4420: 0.0368753, 0.0123\n",
      "2017-11-10 17:04:20: Loss and accuracy at step 4421: 0.0369054, 0.0078\n",
      "2017-11-10 17:04:22: Loss and accuracy at step 4422: 0.0368594, 0.013\n",
      "2017-11-10 17:04:24: Loss and accuracy at step 4423: 0.0368882, 0.0115\n",
      "2017-11-10 17:04:26: Loss and accuracy at step 4424: 0.0368631, 0.0114\n",
      "2017-11-10 17:04:28: Loss and accuracy at step 4425: 0.0368838, 0.0095\n",
      "2017-11-10 17:04:30: Loss and accuracy at step 4426: 0.0368325, 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:04:32: Loss and accuracy at step 4427: 0.0368265, 0.0131\n",
      "2017-11-10 17:04:34: Loss and accuracy at step 4428: 0.0368765, 0.0092\n",
      "2017-11-10 17:04:36: Loss and accuracy at step 4429: 0.0368519, 0.0143\n",
      "2017-11-10 17:04:38: Loss and accuracy at step 4430: 0.0369029, 0.011\n",
      "2017-11-10 17:04:41: Loss and accuracy at step 4431: 0.0369006, 0.0096\n",
      "2017-11-10 17:04:43: Loss and accuracy at step 4432: 0.0367807, 0.0123\n",
      "2017-11-10 17:04:45: Loss and accuracy at step 4433: 0.0367609, 0.0162\n",
      "2017-11-10 17:04:47: Loss and accuracy at step 4434: 0.0368313, 0.0112\n",
      "2017-11-10 17:04:49: Loss and accuracy at step 4435: 0.0367931, 0.0113\n",
      "2017-11-10 17:04:51: Loss and accuracy at step 4436: 0.0368631, 0.014\n",
      "2017-11-10 17:04:53: Loss and accuracy at step 4437: 0.0368192, 0.0089\n",
      "2017-11-10 17:04:55: Loss and accuracy at step 4438: 0.0367172, 0.0104\n",
      "2017-11-10 17:04:57: Loss and accuracy at step 4439: 0.0368511, 0.0139\n",
      "2017-11-10 17:04:59: Loss and accuracy at step 4440: 0.0368405, 0.0057\n",
      "2017-11-10 17:05:01: Loss and accuracy at step 4441: 0.0368375, 0.0195\n",
      "2017-11-10 17:05:03: Loss and accuracy at step 4442: 0.0368258, 0.0124\n",
      "2017-11-10 17:05:06: Loss and accuracy at step 4443: 0.0368601, 0.0072\n",
      "2017-11-10 17:05:08: Loss and accuracy at step 4444: 0.0368467, 0.0209\n",
      "2017-11-10 17:05:10: Loss and accuracy at step 4445: 0.0367985, 0.0085\n",
      "2017-11-10 17:05:12: Loss and accuracy at step 4446: 0.0368718, 0.0107\n",
      "2017-11-10 17:05:14: Loss and accuracy at step 4447: 0.036875, 0.0166\n",
      "2017-11-10 17:05:16: Loss and accuracy at step 4448: 0.0367087, 0.0078\n",
      "2017-11-10 17:05:18: Loss and accuracy at step 4449: 0.0367161, 0.0157\n",
      "2017-11-10 17:05:20: Loss and accuracy at step 4450: 0.0367061, 0.0093\n",
      "2017-11-10 17:05:22: Loss and accuracy at step 4451: 0.0368091, 0.012\n",
      "2017-11-10 17:05:24: Loss and accuracy at step 4452: 0.0368716, 0.0115\n",
      "2017-11-10 17:05:26: Loss and accuracy at step 4453: 0.0368219, 0.0113\n",
      "2017-11-10 17:05:28: Loss and accuracy at step 4454: 0.0368217, 0.0139\n",
      "2017-11-10 17:05:31: Loss and accuracy at step 4455: 0.0368424, 0.0069\n",
      "2017-11-10 17:05:33: Loss and accuracy at step 4456: 0.0367431, 0.019\n",
      "2017-11-10 17:05:35: Loss and accuracy at step 4457: 0.0367437, 0.0102\n",
      "2017-11-10 17:05:37: Loss and accuracy at step 4458: 0.0367201, 0.0119\n",
      "2017-11-10 17:05:39: Loss and accuracy at step 4459: 0.0367968, 0.017\n",
      "2017-11-10 17:05:41: Loss and accuracy at step 4460: 0.0367539, 0.0134\n",
      "2017-11-10 17:05:43: Loss and accuracy at step 4461: 0.0367663, 0.0127\n",
      "2017-11-10 17:05:45: Loss and accuracy at step 4462: 0.036831, 0.0095\n",
      "2017-11-10 17:05:47: Loss and accuracy at step 4463: 0.0368622, 0.0183\n",
      "2017-11-10 17:05:49: Loss and accuracy at step 4464: 0.0367921, 0.0098\n",
      "2017-11-10 17:05:52: Loss and accuracy at step 4465: 0.0369159, 0.0078\n",
      "2017-11-10 17:05:54: Loss and accuracy at step 4466: 0.0367845, 0.0214\n",
      "2017-11-10 17:05:56: Loss and accuracy at step 4467: 0.0369079, 0.0083\n",
      "2017-11-10 17:05:58: Loss and accuracy at step 4468: 0.0368674, 0.0121\n",
      "2017-11-10 17:06:00: Loss and accuracy at step 4469: 0.0368679, 0.0168\n",
      "2017-11-10 17:06:02: Loss and accuracy at step 4470: 0.0369285, 0.008\n",
      "2017-11-10 17:06:04: Loss and accuracy at step 4471: 0.0368205, 0.0156\n",
      "2017-11-10 17:06:06: Loss and accuracy at step 4472: 0.0367796, 0.0124\n",
      "2017-11-10 17:06:08: Loss and accuracy at step 4473: 0.0369121, 0.0067\n",
      "2017-11-10 17:06:10: Loss and accuracy at step 4474: 0.0368726, 0.0172\n",
      "2017-11-10 17:06:12: Loss and accuracy at step 4475: 0.0367708, 0.0081\n",
      "2017-11-10 17:06:15: Loss and accuracy at step 4476: 0.0369109, 0.0148\n",
      "2017-11-10 17:06:17: Loss and accuracy at step 4477: 0.0368005, 0.0087\n",
      "2017-11-10 17:06:19: Loss and accuracy at step 4478: 0.0368068, 0.0094\n",
      "2017-11-10 17:06:21: Loss and accuracy at step 4479: 0.0368324, 0.0112\n",
      "2017-11-10 17:06:23: Loss and accuracy at step 4480: 0.0368345, 0.0093\n",
      "2017-11-10 17:06:25: Loss and accuracy at step 4481: 0.0368151, 0.0144\n",
      "2017-11-10 17:06:27: Loss and accuracy at step 4482: 0.0367075, 0.0103\n",
      "2017-11-10 17:06:29: Loss and accuracy at step 4483: 0.0368323, 0.0157\n",
      "2017-11-10 17:06:31: Loss and accuracy at step 4484: 0.0366851, 0.0095\n",
      "2017-11-10 17:06:33: Loss and accuracy at step 4485: 0.0367092, 0.0194\n",
      "2017-11-10 17:06:35: Loss and accuracy at step 4486: 0.0367832, 0.011\n",
      "2017-11-10 17:06:38: Loss and accuracy at step 4487: 0.0367297, 0.0097\n",
      "2017-11-10 17:06:40: Loss and accuracy at step 4488: 0.0367516, 0.02\n",
      "2017-11-10 17:06:42: Loss and accuracy at step 4489: 0.0367596, 0.0095\n",
      "2017-11-10 17:06:44: Loss and accuracy at step 4490: 0.0367552, 0.0137\n",
      "2017-11-10 17:06:46: Loss and accuracy at step 4491: 0.0367036, 0.0111\n",
      "2017-11-10 17:06:48: Loss and accuracy at step 4492: 0.0368651, 0.0092\n",
      "2017-11-10 17:06:50: Loss and accuracy at step 4493: 0.0368825, 0.0127\n",
      "2017-11-10 17:06:52: Loss and accuracy at step 4494: 0.0367955, 0.0091\n",
      "2017-11-10 17:06:54: Loss and accuracy at step 4495: 0.036852, 0.0156\n",
      "2017-11-10 17:06:56: Loss and accuracy at step 4496: 0.0368501, 0.0106\n",
      "2017-11-10 17:06:58: Loss and accuracy at step 4497: 0.0367919, 0.0112\n",
      "2017-11-10 17:07:00: Loss and accuracy at step 4498: 0.0368809, 0.0117\n",
      "2017-11-10 17:07:03: Loss and accuracy at step 4499: 0.0368573, 0.0103\n",
      "2017-11-10 17:07:05: Loss and accuracy at step 4500: 0.0368206, 0.0154\n",
      "2017-11-10 17:07:07: Loss and accuracy at step 4501: 0.036878, 0.0068\n",
      "2017-11-10 17:07:09: Loss and accuracy at step 4502: 0.0368593, 0.0169\n",
      "2017-11-10 17:07:11: Loss and accuracy at step 4503: 0.0368452, 0.0133\n",
      "2017-11-10 17:07:13: Loss and accuracy at step 4504: 0.036746, 0.0095\n",
      "2017-11-10 17:07:15: Loss and accuracy at step 4505: 0.0368298, 0.0181\n",
      "2017-11-10 17:07:17: Loss and accuracy at step 4506: 0.0368403, 0.008\n",
      "2017-11-10 17:07:19: Loss and accuracy at step 4507: 0.0368016, 0.0137\n",
      "2017-11-10 17:07:21: Loss and accuracy at step 4508: 0.0367778, 0.0089\n",
      "2017-11-10 17:07:23: Loss and accuracy at step 4509: 0.03688, 0.0146\n",
      "2017-11-10 17:07:25: Loss and accuracy at step 4510: 0.0367599, 0.0144\n",
      "2017-11-10 17:07:28: Loss and accuracy at step 4511: 0.0368271, 0.0089\n",
      "2017-11-10 17:07:30: Loss and accuracy at step 4512: 0.0368005, 0.0155\n",
      "2017-11-10 17:07:32: Loss and accuracy at step 4513: 0.0368648, 0.0108\n",
      "2017-11-10 17:07:34: Loss and accuracy at step 4514: 0.0367915, 0.0119\n",
      "2017-11-10 17:07:36: Loss and accuracy at step 4515: 0.0367891, 0.0114\n",
      "2017-11-10 17:07:38: Loss and accuracy at step 4516: 0.0367914, 0.0104\n",
      "2017-11-10 17:07:40: Loss and accuracy at step 4517: 0.0368091, 0.0185\n",
      "2017-11-10 17:07:42: Loss and accuracy at step 4518: 0.0368129, 0.0086\n",
      "2017-11-10 17:07:44: Loss and accuracy at step 4519: 0.0368627, 0.0142\n",
      "2017-11-10 17:07:46: Loss and accuracy at step 4520: 0.0367822, 0.0128\n",
      "2017-11-10 17:07:48: Loss and accuracy at step 4521: 0.0368238, 0.0094\n",
      "2017-11-10 17:07:51: Loss and accuracy at step 4522: 0.0367777, 0.0134\n",
      "2017-11-10 17:07:53: Loss and accuracy at step 4523: 0.0368638, 0.0141\n",
      "2017-11-10 17:07:55: Loss and accuracy at step 4524: 0.0368183, 0.0104\n",
      "2017-11-10 17:07:57: Loss and accuracy at step 4525: 0.0368113, 0.01\n",
      "2017-11-10 17:07:59: Loss and accuracy at step 4526: 0.0367277, 0.0136\n",
      "2017-11-10 17:08:01: Loss and accuracy at step 4527: 0.0368082, 0.0114\n",
      "2017-11-10 17:08:03: Loss and accuracy at step 4528: 0.036732, 0.0115\n",
      "2017-11-10 17:08:05: Loss and accuracy at step 4529: 0.0367332, 0.0084\n",
      "2017-11-10 17:08:07: Loss and accuracy at step 4530: 0.0367383, 0.0157\n",
      "2017-11-10 17:08:09: Loss and accuracy at step 4531: 0.0367332, 0.0122\n",
      "2017-11-10 17:08:12: Loss and accuracy at step 4532: 0.0368216, 0.01\n",
      "2017-11-10 17:08:14: Loss and accuracy at step 4533: 0.0368233, 0.0229\n",
      "2017-11-10 17:08:16: Loss and accuracy at step 4534: 0.0367816, 0.0065\n",
      "2017-11-10 17:08:18: Loss and accuracy at step 4535: 0.0368339, 0.0127\n",
      "2017-11-10 17:08:20: Loss and accuracy at step 4536: 0.0367896, 0.0168\n",
      "2017-11-10 17:08:22: Loss and accuracy at step 4537: 0.0368545, 0.0085\n",
      "2017-11-10 17:08:24: Loss and accuracy at step 4538: 0.0368428, 0.0157\n",
      "2017-11-10 17:08:26: Loss and accuracy at step 4539: 0.0368286, 0.0089\n",
      "2017-11-10 17:08:28: Loss and accuracy at step 4540: 0.0368867, 0.0142\n",
      "2017-11-10 17:08:30: Loss and accuracy at step 4541: 0.0368567, 0.0123\n",
      "2017-11-10 17:08:32: Loss and accuracy at step 4542: 0.0367296, 0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:08:35: Loss and accuracy at step 4543: 0.0367794, 0.0142\n",
      "2017-11-10 17:08:37: Loss and accuracy at step 4544: 0.0368819, 0.0108\n",
      "2017-11-10 17:08:39: Loss and accuracy at step 4545: 0.0368612, 0.0113\n",
      "2017-11-10 17:08:41: Loss and accuracy at step 4546: 0.0368721, 0.0182\n",
      "2017-11-10 17:08:43: Loss and accuracy at step 4547: 0.036803, 0.0101\n",
      "2017-11-10 17:08:45: Loss and accuracy at step 4548: 0.0367392, 0.0142\n",
      "2017-11-10 17:08:47: Loss and accuracy at step 4549: 0.0368083, 0.0106\n",
      "2017-11-10 17:08:49: Loss and accuracy at step 4550: 0.0367675, 0.0154\n",
      "2017-11-10 17:08:51: Loss and accuracy at step 4551: 0.0367565, 0.0084\n",
      "2017-11-10 17:08:53: Loss and accuracy at step 4552: 0.0367763, 0.0127\n",
      "2017-11-10 17:08:55: Loss and accuracy at step 4553: 0.0368409, 0.0135\n",
      "2017-11-10 17:08:57: Loss and accuracy at step 4554: 0.0368523, 0.0096\n",
      "2017-11-10 17:09:00: Loss and accuracy at step 4555: 0.0367418, 0.0147\n",
      "2017-11-10 17:09:02: Loss and accuracy at step 4556: 0.0367936, 0.0099\n",
      "2017-11-10 17:09:04: Loss and accuracy at step 4557: 0.0368414, 0.016\n",
      "2017-11-10 17:09:06: Loss and accuracy at step 4558: 0.0367205, 0.0102\n",
      "2017-11-10 17:09:08: Loss and accuracy at step 4559: 0.0367851, 0.022\n",
      "2017-11-10 17:09:10: Loss and accuracy at step 4560: 0.0368235, 0.0107\n",
      "2017-11-10 17:09:12: Loss and accuracy at step 4561: 0.0367356, 0.0069\n",
      "2017-11-10 17:09:14: Loss and accuracy at step 4562: 0.0367696, 0.0352\n",
      "2017-11-10 17:09:16: Loss and accuracy at step 4563: 0.0368247, 0.0089\n",
      "2017-11-10 17:09:18: Loss and accuracy at step 4564: 0.036819, 0.0137\n",
      "2017-11-10 17:09:20: Loss and accuracy at step 4565: 0.0366897, 0.0104\n",
      "2017-11-10 17:09:23: Loss and accuracy at step 4566: 0.0367621, 0.011\n",
      "2017-11-10 17:09:25: Loss and accuracy at step 4567: 0.0367102, 0.0182\n",
      "2017-11-10 17:09:27: Loss and accuracy at step 4568: 0.0367867, 0.0079\n",
      "2017-11-10 17:09:29: Loss and accuracy at step 4569: 0.0368041, 0.0165\n",
      "2017-11-10 17:09:31: Loss and accuracy at step 4570: 0.0367548, 0.0105\n",
      "2017-11-10 17:09:33: Loss and accuracy at step 4571: 0.0368906, 0.0122\n",
      "2017-11-10 17:09:35: Loss and accuracy at step 4572: 0.036692, 0.0121\n",
      "2017-11-10 17:09:37: Loss and accuracy at step 4573: 0.0368844, 0.009\n",
      "2017-11-10 17:09:39: Loss and accuracy at step 4574: 0.0367508, 0.0192\n",
      "2017-11-10 17:09:41: Loss and accuracy at step 4575: 0.036829, 0.0071\n",
      "2017-11-10 17:09:43: Loss and accuracy at step 4576: 0.0367843, 0.0132\n",
      "2017-11-10 17:09:46: Loss and accuracy at step 4577: 0.0368717, 0.0118\n",
      "2017-11-10 17:09:48: Loss and accuracy at step 4578: 0.0368256, 0.0083\n",
      "2017-11-10 17:09:50: Loss and accuracy at step 4579: 0.036901, 0.0169\n",
      "2017-11-10 17:09:52: Loss and accuracy at step 4580: 0.0369105, 0.0085\n",
      "2017-11-10 17:09:54: Loss and accuracy at step 4581: 0.0368657, 0.0144\n",
      "2017-11-10 17:09:56: Loss and accuracy at step 4582: 0.0367098, 0.0108\n",
      "2017-11-10 17:09:58: Loss and accuracy at step 4583: 0.0368377, 0.0101\n",
      "2017-11-10 17:10:00: Loss and accuracy at step 4584: 0.0368463, 0.0158\n",
      "2017-11-10 17:10:02: Loss and accuracy at step 4585: 0.0368607, 0.0071\n",
      "2017-11-10 17:10:04: Loss and accuracy at step 4586: 0.0367683, 0.0265\n",
      "2017-11-10 17:10:06: Loss and accuracy at step 4587: 0.0368098, 0.0055\n",
      "2017-11-10 17:10:08: Loss and accuracy at step 4588: 0.0367738, 0.0142\n",
      "2017-11-10 17:10:11: Loss and accuracy at step 4589: 0.0368582, 0.0237\n",
      "2017-11-10 17:10:13: Loss and accuracy at step 4590: 0.0368617, 0.0034\n",
      "2017-11-10 17:10:15: Loss and accuracy at step 4591: 0.036817, 0.0204\n",
      "2017-11-10 17:10:17: Loss and accuracy at step 4592: 0.036839, 0.0128\n",
      "2017-11-10 17:10:19: Loss and accuracy at step 4593: 0.0368254, 0.0125\n",
      "2017-11-10 17:10:21: Loss and accuracy at step 4594: 0.0367769, 0.0122\n",
      "2017-11-10 17:10:23: Loss and accuracy at step 4595: 0.0368482, 0.0086\n",
      "2017-11-10 17:10:25: Loss and accuracy at step 4596: 0.0367887, 0.0155\n",
      "2017-11-10 17:10:27: Loss and accuracy at step 4597: 0.0368526, 0.007\n",
      "2017-11-10 17:10:30: Loss and accuracy at step 4598: 0.0368365, 0.014\n",
      "2017-11-10 17:10:32: Loss and accuracy at step 4599: 0.0367488, 0.0104\n",
      "2017-11-10 17:10:34: Loss and accuracy at step 4600: 0.0368252, 0.0128\n",
      "2017-11-10 17:10:36: Loss and accuracy at step 4601: 0.0367438, 0.0105\n",
      "2017-11-10 17:10:38: Loss and accuracy at step 4602: 0.0367674, 0.0122\n",
      "2017-11-10 17:10:40: Loss and accuracy at step 4603: 0.0367572, 0.0125\n",
      "2017-11-10 17:10:42: Loss and accuracy at step 4604: 0.0367529, 0.0072\n",
      "2017-11-10 17:10:44: Loss and accuracy at step 4605: 0.0368053, 0.0136\n",
      "2017-11-10 17:10:46: Loss and accuracy at step 4606: 0.0368042, 0.0123\n",
      "2017-11-10 17:10:48: Loss and accuracy at step 4607: 0.0369282, 0.0073\n",
      "2017-11-10 17:10:50: Loss and accuracy at step 4608: 0.0368663, 0.0338\n",
      "2017-11-10 17:10:52: Loss and accuracy at step 4609: 0.0369299, 0.0091\n",
      "2017-11-10 17:10:55: Loss and accuracy at step 4610: 0.0369183, 0.0109\n",
      "2017-11-10 17:10:57: Loss and accuracy at step 4611: 0.0368088, 0.0218\n",
      "2017-11-10 17:10:59: Loss and accuracy at step 4612: 0.0368865, 0.0107\n",
      "2017-11-10 17:11:01: Loss and accuracy at step 4613: 0.0368342, 0.0144\n",
      "2017-11-10 17:11:03: Loss and accuracy at step 4614: 0.0367918, 0.0114\n",
      "2017-11-10 17:11:05: Loss and accuracy at step 4615: 0.0368551, 0.0132\n",
      "2017-11-10 17:11:07: Loss and accuracy at step 4616: 0.0368737, 0.0178\n",
      "2017-11-10 17:11:09: Loss and accuracy at step 4617: 0.0369049, 0.0054\n",
      "2017-11-10 17:11:11: Loss and accuracy at step 4618: 0.0368708, 0.0206\n",
      "2017-11-10 17:11:13: Loss and accuracy at step 4619: 0.0368229, 0.01\n",
      "2017-11-10 17:11:15: Loss and accuracy at step 4620: 0.0368257, 0.0074\n",
      "2017-11-10 17:11:17: Loss and accuracy at step 4621: 0.0367957, 0.0151\n",
      "2017-11-10 17:11:19: Loss and accuracy at step 4622: 0.036799, 0.0178\n",
      "2017-11-10 17:11:22: Loss and accuracy at step 4623: 0.0368597, 0.0164\n",
      "2017-11-10 17:11:24: Loss and accuracy at step 4624: 0.0368369, 0.0069\n",
      "2017-11-10 17:11:26: Loss and accuracy at step 4625: 0.0368579, 0.0245\n",
      "2017-11-10 17:11:28: Loss and accuracy at step 4626: 0.0368701, 0.0127\n",
      "2017-11-10 17:11:30: Loss and accuracy at step 4627: 0.0368336, 0.0073\n",
      "2017-11-10 17:11:32: Loss and accuracy at step 4628: 0.0367448, 0.0482\n",
      "2017-11-10 17:11:34: Loss and accuracy at step 4629: 0.0367798, 0.0107\n",
      "2017-11-10 17:11:36: Loss and accuracy at step 4630: 0.0368287, 0.0105\n",
      "2017-11-10 17:11:38: Loss and accuracy at step 4631: 0.0369023, 0.1082\n",
      "2017-11-10 17:11:40: Loss and accuracy at step 4632: 0.0368619, 0.0075\n",
      "2017-11-10 17:11:42: Loss and accuracy at step 4633: 0.0368661, 0.0155\n",
      "2017-11-10 17:11:45: Loss and accuracy at step 4634: 0.0368161, 0.1325\n",
      "2017-11-10 17:11:47: Loss and accuracy at step 4635: 0.0367778, 0.008\n",
      "2017-11-10 17:11:49: Loss and accuracy at step 4636: 0.0369018, 0.0231\n",
      "2017-11-10 17:11:51: Loss and accuracy at step 4637: 0.0369516, 0.05\n",
      "2017-11-10 17:11:53: Loss and accuracy at step 4638: 0.0368627, 0.0101\n",
      "2017-11-10 17:11:55: Loss and accuracy at step 4639: 0.0367453, 0.0301\n",
      "2017-11-10 17:11:57: Loss and accuracy at step 4640: 0.0368498, 0.0058\n",
      "2017-11-10 17:11:59: Loss and accuracy at step 4641: 0.036975, 0.0279\n",
      "2017-11-10 17:12:01: Loss and accuracy at step 4642: 0.0368122, 0.0256\n",
      "2017-11-10 17:12:03: Loss and accuracy at step 4643: 0.0368902, 0.0111\n",
      "2017-11-10 17:12:05: Loss and accuracy at step 4644: 0.0369112, 0.0444\n",
      "2017-11-10 17:12:08: Loss and accuracy at step 4645: 0.0369004, 0.0064\n",
      "2017-11-10 17:12:10: Loss and accuracy at step 4646: 0.0368796, 0.0289\n",
      "2017-11-10 17:12:12: Loss and accuracy at step 4647: 0.0368976, 0.0318\n",
      "2017-11-10 17:12:14: Loss and accuracy at step 4648: 0.036909, 0.0059\n",
      "2017-11-10 17:12:16: Loss and accuracy at step 4649: 0.0369567, 0.0257\n",
      "2017-11-10 17:12:18: Loss and accuracy at step 4650: 0.0369061, 0.0137\n",
      "2017-11-10 17:12:20: Loss and accuracy at step 4651: 0.0368552, 0.0179\n",
      "2017-11-10 17:12:22: Loss and accuracy at step 4652: 0.0368383, 0.0234\n",
      "2017-11-10 17:12:24: Loss and accuracy at step 4653: 0.0369172, 0.0073\n",
      "2017-11-10 17:12:27: Loss and accuracy at step 4654: 0.0368079, 0.0162\n",
      "2017-11-10 17:12:29: Loss and accuracy at step 4655: 0.0368057, 0.0394\n",
      "2017-11-10 17:12:31: Loss and accuracy at step 4656: 0.0370257, 0.0049\n",
      "2017-11-10 17:12:33: Loss and accuracy at step 4657: 0.0368333, 0.0166\n",
      "2017-11-10 17:12:35: Loss and accuracy at step 4658: 0.0368221, 0.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:12:37: Loss and accuracy at step 4659: 0.0368925, 0.0058\n",
      "2017-11-10 17:12:39: Loss and accuracy at step 4660: 0.0368534, 0.0158\n",
      "2017-11-10 17:12:41: Loss and accuracy at step 4661: 0.0369015, 0.0231\n",
      "2017-11-10 17:12:43: Loss and accuracy at step 4662: 0.0368806, 0.0078\n",
      "2017-11-10 17:12:45: Loss and accuracy at step 4663: 0.036932, 0.0221\n",
      "2017-11-10 17:12:47: Loss and accuracy at step 4664: 0.0368414, 0.0105\n",
      "2017-11-10 17:12:49: Loss and accuracy at step 4665: 0.036792, 0.0147\n",
      "2017-11-10 17:12:52: Loss and accuracy at step 4666: 0.0369197, 0.0261\n",
      "2017-11-10 17:12:54: Loss and accuracy at step 4667: 0.0368728, 0.0074\n",
      "2017-11-10 17:12:56: Loss and accuracy at step 4668: 0.0368085, 0.014\n",
      "2017-11-10 17:12:58: Loss and accuracy at step 4669: 0.0368836, 0.0285\n",
      "2017-11-10 17:13:00: Loss and accuracy at step 4670: 0.0368854, 0.007\n",
      "2017-11-10 17:13:02: Loss and accuracy at step 4671: 0.0368652, 0.0151\n",
      "2017-11-10 17:13:04: Loss and accuracy at step 4672: 0.0368814, 0.0227\n",
      "2017-11-10 17:13:06: Loss and accuracy at step 4673: 0.03689, 0.0079\n",
      "2017-11-10 17:13:08: Loss and accuracy at step 4674: 0.0367903, 0.0193\n",
      "2017-11-10 17:13:10: Loss and accuracy at step 4675: 0.0368844, 0.0197\n",
      "2017-11-10 17:13:12: Loss and accuracy at step 4676: 0.0368457, 0.0076\n",
      "2017-11-10 17:13:15: Loss and accuracy at step 4677: 0.0368158, 0.0295\n",
      "2017-11-10 17:13:17: Loss and accuracy at step 4678: 0.0368428, 0.022\n",
      "2017-11-10 17:13:19: Loss and accuracy at step 4679: 0.0369235, 0.0071\n",
      "2017-11-10 17:13:21: Loss and accuracy at step 4680: 0.0368485, 0.0344\n",
      "2017-11-10 17:13:23: Loss and accuracy at step 4681: 0.0369821, 0.02\n",
      "2017-11-10 17:13:25: Loss and accuracy at step 4682: 0.0368365, 0.0062\n",
      "2017-11-10 17:13:27: Loss and accuracy at step 4683: 0.0368052, 0.0297\n",
      "2017-11-10 17:13:29: Loss and accuracy at step 4684: 0.0368079, 0.016\n",
      "2017-11-10 17:13:31: Loss and accuracy at step 4685: 0.0368523, 0.0055\n",
      "2017-11-10 17:13:33: Loss and accuracy at step 4686: 0.0368511, 0.02\n",
      "2017-11-10 17:13:35: Loss and accuracy at step 4687: 0.0367672, 0.015\n",
      "2017-11-10 17:13:38: Loss and accuracy at step 4688: 0.0367995, 0.0081\n",
      "2017-11-10 17:13:40: Loss and accuracy at step 4689: 0.0368461, 0.0136\n",
      "2017-11-10 17:13:42: Loss and accuracy at step 4690: 0.0368508, 0.0196\n",
      "2017-11-10 17:13:44: Loss and accuracy at step 4691: 0.0368412, 0.0087\n",
      "2017-11-10 17:13:46: Loss and accuracy at step 4692: 0.0367985, 0.0106\n",
      "2017-11-10 17:13:48: Loss and accuracy at step 4693: 0.0369476, 0.0159\n",
      "2017-11-10 17:13:50: Loss and accuracy at step 4694: 0.0368966, 0.0098\n",
      "2017-11-10 17:13:52: Loss and accuracy at step 4695: 0.0367814, 0.0095\n",
      "2017-11-10 17:13:54: Loss and accuracy at step 4696: 0.036839, 0.0112\n",
      "2017-11-10 17:13:56: Loss and accuracy at step 4697: 0.0368583, 0.0152\n",
      "2017-11-10 17:13:58: Loss and accuracy at step 4698: 0.0369001, 0.0104\n",
      "2017-11-10 17:14:00: Loss and accuracy at step 4699: 0.036835, 0.0119\n",
      "2017-11-10 17:14:03: Loss and accuracy at step 4700: 0.0367666, 0.0141\n",
      "2017-11-10 17:14:05: Loss and accuracy at step 4701: 0.0369468, 0.0089\n",
      "2017-11-10 17:14:07: Loss and accuracy at step 4702: 0.0367753, 0.0133\n",
      "2017-11-10 17:14:09: Loss and accuracy at step 4703: 0.0367817, 0.013\n",
      "2017-11-10 17:14:11: Loss and accuracy at step 4704: 0.0368253, 0.0109\n",
      "2017-11-10 17:14:13: Loss and accuracy at step 4705: 0.0368273, 0.0095\n",
      "2017-11-10 17:14:15: Loss and accuracy at step 4706: 0.036737, 0.0162\n",
      "2017-11-10 17:14:17: Loss and accuracy at step 4707: 0.0368671, 0.0145\n",
      "2017-11-10 17:14:19: Loss and accuracy at step 4708: 0.0368145, 0.0074\n",
      "2017-11-10 17:14:22: Loss and accuracy at step 4709: 0.0368671, 0.0121\n",
      "2017-11-10 17:14:24: Loss and accuracy at step 4710: 0.0368725, 0.0187\n",
      "2017-11-10 17:14:26: Loss and accuracy at step 4711: 0.0367774, 0.0104\n",
      "2017-11-10 17:14:28: Loss and accuracy at step 4712: 0.036895, 0.0089\n",
      "2017-11-10 17:14:30: Loss and accuracy at step 4713: 0.036851, 0.014\n",
      "2017-11-10 17:14:32: Loss and accuracy at step 4714: 0.0368314, 0.0134\n",
      "2017-11-10 17:14:34: Loss and accuracy at step 4715: 0.0367929, 0.0123\n",
      "2017-11-10 17:14:36: Loss and accuracy at step 4716: 0.0367971, 0.0106\n",
      "2017-11-10 17:14:38: Loss and accuracy at step 4717: 0.0369041, 0.0071\n",
      "2017-11-10 17:14:40: Loss and accuracy at step 4718: 0.0368086, 0.0167\n",
      "2017-11-10 17:14:42: Loss and accuracy at step 4719: 0.036838, 0.0189\n",
      "2017-11-10 17:14:44: Loss and accuracy at step 4720: 0.0368018, 0.0062\n",
      "2017-11-10 17:14:46: Loss and accuracy at step 4721: 0.0368014, 0.0113\n",
      "2017-11-10 17:14:49: Loss and accuracy at step 4722: 0.0368407, 0.0218\n",
      "2017-11-10 17:14:51: Loss and accuracy at step 4723: 0.0367384, 0.0105\n",
      "2017-11-10 17:14:53: Loss and accuracy at step 4724: 0.0368204, 0.0074\n",
      "2017-11-10 17:14:55: Loss and accuracy at step 4725: 0.0367654, 0.0145\n",
      "2017-11-10 17:14:57: Loss and accuracy at step 4726: 0.0368062, 0.0111\n",
      "2017-11-10 17:14:59: Loss and accuracy at step 4727: 0.036725, 0.0112\n",
      "2017-11-10 17:15:01: Loss and accuracy at step 4728: 0.036867, 0.0112\n",
      "2017-11-10 17:15:03: Loss and accuracy at step 4729: 0.0367658, 0.0095\n",
      "2017-11-10 17:15:05: Loss and accuracy at step 4730: 0.0367766, 0.0108\n",
      "2017-11-10 17:15:07: Loss and accuracy at step 4731: 0.0367533, 0.0153\n",
      "2017-11-10 17:15:09: Loss and accuracy at step 4732: 0.0368307, 0.0121\n",
      "2017-11-10 17:15:11: Loss and accuracy at step 4733: 0.036867, 0.0092\n",
      "2017-11-10 17:15:14: Loss and accuracy at step 4734: 0.0366981, 0.013\n",
      "2017-11-10 17:15:16: Loss and accuracy at step 4735: 0.036766, 0.0124\n",
      "2017-11-10 17:15:18: Loss and accuracy at step 4736: 0.036715, 0.0107\n",
      "2017-11-10 17:15:20: Loss and accuracy at step 4737: 0.0367657, 0.0133\n",
      "2017-11-10 17:15:22: Loss and accuracy at step 4738: 0.0367485, 0.0122\n",
      "2017-11-10 17:15:24: Loss and accuracy at step 4739: 0.0367654, 0.0132\n",
      "2017-11-10 17:15:26: Loss and accuracy at step 4740: 0.0367504, 0.0158\n",
      "2017-11-10 17:15:28: Loss and accuracy at step 4741: 0.0366842, 0.0103\n",
      "2017-11-10 17:15:30: Loss and accuracy at step 4742: 0.0367653, 0.0128\n",
      "2017-11-10 17:15:32: Loss and accuracy at step 4743: 0.0367499, 0.0129\n",
      "2017-11-10 17:15:35: Loss and accuracy at step 4744: 0.0367689, 0.011\n",
      "2017-11-10 17:15:37: Loss and accuracy at step 4745: 0.0367945, 0.0133\n",
      "2017-11-10 17:15:39: Loss and accuracy at step 4746: 0.0368281, 0.0094\n",
      "2017-11-10 17:15:41: Loss and accuracy at step 4747: 0.0367877, 0.0144\n",
      "2017-11-10 17:15:43: Loss and accuracy at step 4748: 0.0368997, 0.0096\n",
      "2017-11-10 17:15:45: Loss and accuracy at step 4749: 0.0368323, 0.0094\n",
      "2017-11-10 17:15:47: Loss and accuracy at step 4750: 0.0368081, 0.0172\n",
      "2017-11-10 17:15:49: Loss and accuracy at step 4751: 0.0368307, 0.0101\n",
      "2017-11-10 17:15:51: Loss and accuracy at step 4752: 0.03678, 0.0108\n",
      "2017-11-10 17:15:53: Loss and accuracy at step 4753: 0.03688, 0.014\n",
      "2017-11-10 17:15:55: Loss and accuracy at step 4754: 0.0367884, 0.0125\n",
      "2017-11-10 17:15:57: Loss and accuracy at step 4755: 0.0367522, 0.0117\n",
      "2017-11-10 17:16:00: Loss and accuracy at step 4756: 0.0367522, 0.0157\n",
      "2017-11-10 17:16:02: Loss and accuracy at step 4757: 0.0368064, 0.0079\n",
      "2017-11-10 17:16:04: Loss and accuracy at step 4758: 0.0367895, 0.013\n",
      "2017-11-10 17:16:06: Loss and accuracy at step 4759: 0.0368504, 0.0189\n",
      "2017-11-10 17:16:08: Loss and accuracy at step 4760: 0.0368716, 0.0099\n",
      "2017-11-10 17:16:10: Loss and accuracy at step 4761: 0.0368382, 0.0118\n",
      "2017-11-10 17:16:12: Loss and accuracy at step 4762: 0.0367702, 0.0151\n",
      "2017-11-10 17:16:14: Loss and accuracy at step 4763: 0.0368097, 0.0148\n",
      "2017-11-10 17:16:16: Loss and accuracy at step 4764: 0.0368551, 0.012\n",
      "2017-11-10 17:16:18: Loss and accuracy at step 4765: 0.0367703, 0.0129\n",
      "2017-11-10 17:16:21: Loss and accuracy at step 4766: 0.0367663, 0.0146\n",
      "2017-11-10 17:16:23: Loss and accuracy at step 4767: 0.0367752, 0.0141\n",
      "2017-11-10 17:16:25: Loss and accuracy at step 4768: 0.0368708, 0.0115\n",
      "2017-11-10 17:16:27: Loss and accuracy at step 4769: 0.0368157, 0.0126\n",
      "2017-11-10 17:16:29: Loss and accuracy at step 4770: 0.0367557, 0.0114\n",
      "2017-11-10 17:16:31: Loss and accuracy at step 4771: 0.0368799, 0.0104\n",
      "2017-11-10 17:16:33: Loss and accuracy at step 4772: 0.0368787, 0.0158\n",
      "2017-11-10 17:16:35: Loss and accuracy at step 4773: 0.0367184, 0.0136\n",
      "2017-11-10 17:16:37: Loss and accuracy at step 4774: 0.0367717, 0.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:16:39: Loss and accuracy at step 4775: 0.0367718, 0.0106\n",
      "2017-11-10 17:16:41: Loss and accuracy at step 4776: 0.0367839, 0.0148\n",
      "2017-11-10 17:16:43: Loss and accuracy at step 4777: 0.0368789, 0.0111\n",
      "2017-11-10 17:16:46: Loss and accuracy at step 4778: 0.0368031, 0.0139\n",
      "2017-11-10 17:16:48: Loss and accuracy at step 4779: 0.03677, 0.0108\n",
      "2017-11-10 17:16:50: Loss and accuracy at step 4780: 0.0367348, 0.0078\n",
      "2017-11-10 17:16:52: Loss and accuracy at step 4781: 0.0368295, 0.0185\n",
      "2017-11-10 17:16:54: Loss and accuracy at step 4782: 0.0368052, 0.01\n",
      "2017-11-10 17:16:56: Loss and accuracy at step 4783: 0.0368524, 0.0096\n",
      "2017-11-10 17:16:58: Loss and accuracy at step 4784: 0.0367466, 0.0208\n",
      "2017-11-10 17:17:00: Loss and accuracy at step 4785: 0.0368144, 0.0125\n",
      "2017-11-10 17:17:02: Loss and accuracy at step 4786: 0.0367734, 0.0096\n",
      "2017-11-10 17:17:04: Loss and accuracy at step 4787: 0.0367067, 0.0172\n",
      "2017-11-10 17:17:06: Loss and accuracy at step 4788: 0.0368556, 0.0128\n",
      "2017-11-10 17:17:08: Loss and accuracy at step 4789: 0.0367578, 0.011\n",
      "2017-11-10 17:17:11: Loss and accuracy at step 4790: 0.0367722, 0.0146\n",
      "2017-11-10 17:17:13: Loss and accuracy at step 4791: 0.0367685, 0.0134\n",
      "2017-11-10 17:17:15: Loss and accuracy at step 4792: 0.0367482, 0.009\n",
      "2017-11-10 17:17:17: Loss and accuracy at step 4793: 0.0368252, 0.015\n",
      "2017-11-10 17:17:19: Loss and accuracy at step 4794: 0.0368095, 0.012\n",
      "2017-11-10 17:17:21: Loss and accuracy at step 4795: 0.0367382, 0.0099\n",
      "2017-11-10 17:17:23: Loss and accuracy at step 4796: 0.0367862, 0.012\n",
      "2017-11-10 17:17:25: Loss and accuracy at step 4797: 0.0367486, 0.0108\n",
      "2017-11-10 17:17:27: Loss and accuracy at step 4798: 0.0368124, 0.0109\n",
      "2017-11-10 17:17:29: Loss and accuracy at step 4799: 0.0367604, 0.0121\n",
      "2017-11-10 17:17:31: Loss and accuracy at step 4800: 0.0367784, 0.0134\n",
      "2017-11-10 17:17:34: Loss and accuracy at step 4801: 0.0368304, 0.0109\n",
      "2017-11-10 17:17:36: Loss and accuracy at step 4802: 0.0368271, 0.0107\n",
      "2017-11-10 17:17:38: Loss and accuracy at step 4803: 0.036739, 0.016\n",
      "2017-11-10 17:17:40: Loss and accuracy at step 4804: 0.0368304, 0.0152\n",
      "2017-11-10 17:17:42: Loss and accuracy at step 4805: 0.0368797, 0.0105\n",
      "2017-11-10 17:17:44: Loss and accuracy at step 4806: 0.0368407, 0.0122\n",
      "2017-11-10 17:17:46: Loss and accuracy at step 4807: 0.0368148, 0.0121\n",
      "2017-11-10 17:17:48: Loss and accuracy at step 4808: 0.0368822, 0.0122\n",
      "2017-11-10 17:17:50: Loss and accuracy at step 4809: 0.0368552, 0.0107\n",
      "2017-11-10 17:17:52: Loss and accuracy at step 4810: 0.0368287, 0.0112\n",
      "2017-11-10 17:17:54: Loss and accuracy at step 4811: 0.0366845, 0.0178\n",
      "2017-11-10 17:17:56: Loss and accuracy at step 4812: 0.0367906, 0.0095\n",
      "2017-11-10 17:17:59: Loss and accuracy at step 4813: 0.0366666, 0.0086\n",
      "2017-11-10 17:18:01: Loss and accuracy at step 4814: 0.0368194, 0.0223\n",
      "2017-11-10 17:18:03: Loss and accuracy at step 4815: 0.0367859, 0.0107\n",
      "2017-11-10 17:18:05: Loss and accuracy at step 4816: 0.036819, 0.0117\n",
      "2017-11-10 17:18:07: Loss and accuracy at step 4817: 0.0368093, 0.0131\n",
      "2017-11-10 17:18:09: Loss and accuracy at step 4818: 0.0367911, 0.0115\n",
      "2017-11-10 17:18:11: Loss and accuracy at step 4819: 0.0367757, 0.0131\n",
      "2017-11-10 17:18:13: Loss and accuracy at step 4820: 0.0367951, 0.0103\n",
      "2017-11-10 17:18:15: Loss and accuracy at step 4821: 0.0367404, 0.0152\n",
      "2017-11-10 17:18:17: Loss and accuracy at step 4822: 0.0367588, 0.0103\n",
      "2017-11-10 17:18:20: Loss and accuracy at step 4823: 0.03676, 0.0113\n",
      "2017-11-10 17:18:22: Loss and accuracy at step 4824: 0.0367417, 0.0121\n",
      "2017-11-10 17:18:24: Loss and accuracy at step 4825: 0.0367666, 0.0147\n",
      "2017-11-10 17:18:26: Loss and accuracy at step 4826: 0.0367715, 0.0116\n",
      "2017-11-10 17:18:28: Loss and accuracy at step 4827: 0.0368169, 0.013\n",
      "2017-11-10 17:18:30: Loss and accuracy at step 4828: 0.0368065, 0.0118\n",
      "2017-11-10 17:18:32: Loss and accuracy at step 4829: 0.0368404, 0.0147\n",
      "2017-11-10 17:18:34: Loss and accuracy at step 4830: 0.0367317, 0.0155\n",
      "2017-11-10 17:18:36: Loss and accuracy at step 4831: 0.0368438, 0.0094\n",
      "2017-11-10 17:18:38: Loss and accuracy at step 4832: 0.0367829, 0.0132\n",
      "2017-11-10 17:18:40: Loss and accuracy at step 4833: 0.0367891, 0.0164\n",
      "2017-11-10 17:18:42: Loss and accuracy at step 4834: 0.0368317, 0.013\n",
      "2017-11-10 17:18:45: Loss and accuracy at step 4835: 0.0368321, 0.0116\n",
      "2017-11-10 17:18:47: Loss and accuracy at step 4836: 0.0367931, 0.0106\n",
      "2017-11-10 17:18:49: Loss and accuracy at step 4837: 0.0367466, 0.0125\n",
      "2017-11-10 17:18:51: Loss and accuracy at step 4838: 0.036733, 0.0107\n",
      "2017-11-10 17:18:53: Loss and accuracy at step 4839: 0.0368544, 0.011\n",
      "2017-11-10 17:18:55: Loss and accuracy at step 4840: 0.0368154, 0.0137\n",
      "2017-11-10 17:18:57: Loss and accuracy at step 4841: 0.0367271, 0.011\n",
      "2017-11-10 17:18:59: Loss and accuracy at step 4842: 0.0367255, 0.0122\n",
      "2017-11-10 17:19:01: Loss and accuracy at step 4843: 0.0367463, 0.0122\n",
      "2017-11-10 17:19:03: Loss and accuracy at step 4844: 0.0367158, 0.0111\n",
      "2017-11-10 17:19:05: Loss and accuracy at step 4845: 0.0367817, 0.0111\n",
      "2017-11-10 17:19:08: Loss and accuracy at step 4846: 0.0368023, 0.0135\n",
      "2017-11-10 17:19:10: Loss and accuracy at step 4847: 0.0368312, 0.011\n",
      "2017-11-10 17:19:12: Loss and accuracy at step 4848: 0.0367722, 0.0128\n",
      "2017-11-10 17:19:14: Loss and accuracy at step 4849: 0.0367249, 0.0125\n",
      "2017-11-10 17:19:16: Loss and accuracy at step 4850: 0.0367993, 0.0091\n",
      "2017-11-10 17:19:18: Loss and accuracy at step 4851: 0.0368521, 0.0107\n",
      "2017-11-10 17:19:20: Loss and accuracy at step 4852: 0.0368408, 0.0118\n",
      "2017-11-10 17:19:22: Loss and accuracy at step 4853: 0.0367287, 0.0131\n",
      "2017-11-10 17:19:24: Loss and accuracy at step 4854: 0.0367594, 0.0095\n",
      "2017-11-10 17:19:26: Loss and accuracy at step 4855: 0.0368699, 0.0125\n",
      "2017-11-10 17:19:28: Loss and accuracy at step 4856: 0.0367575, 0.0094\n",
      "2017-11-10 17:19:30: Loss and accuracy at step 4857: 0.0368022, 0.0159\n",
      "2017-11-10 17:19:33: Loss and accuracy at step 4858: 0.0367688, 0.0114\n",
      "2017-11-10 17:19:35: Loss and accuracy at step 4859: 0.0368869, 0.0108\n",
      "2017-11-10 17:19:37: Loss and accuracy at step 4860: 0.0368292, 0.0161\n",
      "2017-11-10 17:19:39: Loss and accuracy at step 4861: 0.036761, 0.0095\n",
      "2017-11-10 17:19:41: Loss and accuracy at step 4862: 0.0368075, 0.0103\n",
      "2017-11-10 17:19:43: Loss and accuracy at step 4863: 0.0367299, 0.0167\n",
      "2017-11-10 17:19:45: Loss and accuracy at step 4864: 0.0368596, 0.0065\n",
      "2017-11-10 17:19:47: Loss and accuracy at step 4865: 0.0367748, 0.0143\n",
      "2017-11-10 17:19:49: Loss and accuracy at step 4866: 0.0367157, 0.0131\n",
      "2017-11-10 17:19:51: Loss and accuracy at step 4867: 0.0368052, 0.0091\n",
      "2017-11-10 17:19:53: Loss and accuracy at step 4868: 0.0368524, 0.0157\n",
      "2017-11-10 17:19:55: Loss and accuracy at step 4869: 0.0368368, 0.008\n",
      "2017-11-10 17:19:58: Loss and accuracy at step 4870: 0.0366857, 0.0103\n",
      "2017-11-10 17:20:00: Loss and accuracy at step 4871: 0.0367199, 0.0188\n",
      "2017-11-10 17:20:02: Loss and accuracy at step 4872: 0.0367934, 0.0092\n",
      "2017-11-10 17:20:04: Loss and accuracy at step 4873: 0.036839, 0.0141\n",
      "2017-11-10 17:20:06: Loss and accuracy at step 4874: 0.0367342, 0.0208\n",
      "2017-11-10 17:20:08: Loss and accuracy at step 4875: 0.0367571, 0.0102\n",
      "2017-11-10 17:20:10: Loss and accuracy at step 4876: 0.036797, 0.0127\n",
      "2017-11-10 17:20:12: Loss and accuracy at step 4877: 0.0368311, 0.0147\n",
      "2017-11-10 17:20:14: Loss and accuracy at step 4878: 0.036789, 0.0109\n",
      "2017-11-10 17:20:17: Loss and accuracy at step 4879: 0.0368292, 0.0139\n",
      "2017-11-10 17:20:19: Loss and accuracy at step 4880: 0.0368262, 0.0136\n",
      "2017-11-10 17:20:21: Loss and accuracy at step 4881: 0.0367907, 0.0122\n",
      "2017-11-10 17:20:23: Loss and accuracy at step 4882: 0.0367509, 0.0103\n",
      "2017-11-10 17:20:25: Loss and accuracy at step 4883: 0.0368227, 0.0124\n",
      "2017-11-10 17:20:27: Loss and accuracy at step 4884: 0.036769, 0.011\n",
      "2017-11-10 17:20:29: Loss and accuracy at step 4885: 0.0367941, 0.0142\n",
      "2017-11-10 17:20:31: Loss and accuracy at step 4886: 0.0367696, 0.0127\n",
      "2017-11-10 17:20:33: Loss and accuracy at step 4887: 0.0367698, 0.0135\n",
      "2017-11-10 17:20:35: Loss and accuracy at step 4888: 0.036741, 0.0144\n",
      "2017-11-10 17:20:37: Loss and accuracy at step 4889: 0.0368116, 0.0107\n",
      "2017-11-10 17:20:39: Loss and accuracy at step 4890: 0.0367973, 0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:20:42: Loss and accuracy at step 4891: 0.0367705, 0.0229\n",
      "2017-11-10 17:20:44: Loss and accuracy at step 4892: 0.0368729, 0.0074\n",
      "2017-11-10 17:20:46: Loss and accuracy at step 4893: 0.0368083, 0.0094\n",
      "2017-11-10 17:20:48: Loss and accuracy at step 4894: 0.0367878, 0.0179\n",
      "2017-11-10 17:20:50: Loss and accuracy at step 4895: 0.0368478, 0.0086\n",
      "2017-11-10 17:20:52: Loss and accuracy at step 4896: 0.0368841, 0.0087\n",
      "2017-11-10 17:20:54: Loss and accuracy at step 4897: 0.0368509, 0.0179\n",
      "2017-11-10 17:20:56: Loss and accuracy at step 4898: 0.0368274, 0.0123\n",
      "2017-11-10 17:20:58: Loss and accuracy at step 4899: 0.036753, 0.0103\n",
      "2017-11-10 17:21:00: Loss and accuracy at step 4900: 0.0368202, 0.0187\n",
      "2017-11-10 17:21:02: Loss and accuracy at step 4901: 0.0368834, 0.0121\n",
      "2017-11-10 17:21:04: Loss and accuracy at step 4902: 0.036842, 0.0096\n",
      "2017-11-10 17:21:07: Loss and accuracy at step 4903: 0.0367679, 0.0208\n",
      "2017-11-10 17:21:09: Loss and accuracy at step 4904: 0.0368055, 0.0098\n",
      "2017-11-10 17:21:11: Loss and accuracy at step 4905: 0.0366624, 0.0117\n",
      "2017-11-10 17:21:13: Loss and accuracy at step 4906: 0.0368284, 0.0129\n",
      "2017-11-10 17:21:15: Loss and accuracy at step 4907: 0.0367911, 0.0088\n",
      "2017-11-10 17:21:17: Loss and accuracy at step 4908: 0.036822, 0.0176\n",
      "2017-11-10 17:21:19: Loss and accuracy at step 4909: 0.0368209, 0.0186\n",
      "2017-11-10 17:21:21: Loss and accuracy at step 4910: 0.0367778, 0.0087\n",
      "2017-11-10 17:21:23: Loss and accuracy at step 4911: 0.0367622, 0.0125\n",
      "2017-11-10 17:21:25: Loss and accuracy at step 4912: 0.0367626, 0.0197\n",
      "2017-11-10 17:21:27: Loss and accuracy at step 4913: 0.0368744, 0.0083\n",
      "2017-11-10 17:21:29: Loss and accuracy at step 4914: 0.0367923, 0.0131\n",
      "2017-11-10 17:21:32: Loss and accuracy at step 4915: 0.0367288, 0.0141\n",
      "2017-11-10 17:21:34: Loss and accuracy at step 4916: 0.0367608, 0.0105\n",
      "2017-11-10 17:21:36: Loss and accuracy at step 4917: 0.0367705, 0.0113\n",
      "2017-11-10 17:21:38: Loss and accuracy at step 4918: 0.0367676, 0.0121\n",
      "2017-11-10 17:21:40: Loss and accuracy at step 4919: 0.0367965, 0.0121\n",
      "2017-11-10 17:21:42: Loss and accuracy at step 4920: 0.0367773, 0.014\n",
      "2017-11-10 17:21:44: Loss and accuracy at step 4921: 0.0367069, 0.0129\n",
      "2017-11-10 17:21:46: Loss and accuracy at step 4922: 0.0367673, 0.0108\n",
      "2017-11-10 17:21:48: Loss and accuracy at step 4923: 0.036806, 0.0113\n",
      "2017-11-10 17:21:50: Loss and accuracy at step 4924: 0.0368338, 0.0172\n",
      "2017-11-10 17:21:52: Loss and accuracy at step 4925: 0.036795, 0.0068\n",
      "2017-11-10 17:21:55: Loss and accuracy at step 4926: 0.0367398, 0.0135\n",
      "2017-11-10 17:21:57: Loss and accuracy at step 4927: 0.0368068, 0.0169\n",
      "2017-11-10 17:21:59: Loss and accuracy at step 4928: 0.0368646, 0.0079\n",
      "2017-11-10 17:22:01: Loss and accuracy at step 4929: 0.0367983, 0.0134\n",
      "2017-11-10 17:22:03: Loss and accuracy at step 4930: 0.0367969, 0.0133\n",
      "2017-11-10 17:22:05: Loss and accuracy at step 4931: 0.0367819, 0.0098\n",
      "2017-11-10 17:22:07: Loss and accuracy at step 4932: 0.0367026, 0.0115\n",
      "2017-11-10 17:22:09: Loss and accuracy at step 4933: 0.0368283, 0.0078\n",
      "2017-11-10 17:22:11: Loss and accuracy at step 4934: 0.036788, 0.0161\n",
      "2017-11-10 17:22:14: Loss and accuracy at step 4935: 0.0368362, 0.0122\n",
      "2017-11-10 17:22:16: Loss and accuracy at step 4936: 0.03687, 0.0108\n",
      "2017-11-10 17:22:18: Loss and accuracy at step 4937: 0.0368375, 0.0113\n",
      "2017-11-10 17:22:20: Loss and accuracy at step 4938: 0.0367663, 0.0128\n",
      "2017-11-10 17:22:22: Loss and accuracy at step 4939: 0.0368445, 0.0125\n",
      "2017-11-10 17:22:24: Loss and accuracy at step 4940: 0.0367846, 0.0135\n",
      "2017-11-10 17:22:26: Loss and accuracy at step 4941: 0.0367915, 0.01\n",
      "2017-11-10 17:22:28: Loss and accuracy at step 4942: 0.0367605, 0.0091\n",
      "2017-11-10 17:22:30: Loss and accuracy at step 4943: 0.0367812, 0.0139\n",
      "2017-11-10 17:22:32: Loss and accuracy at step 4944: 0.036759, 0.0096\n",
      "2017-11-10 17:22:34: Loss and accuracy at step 4945: 0.0367959, 0.0119\n",
      "2017-11-10 17:22:36: Loss and accuracy at step 4946: 0.0367315, 0.012\n",
      "2017-11-10 17:22:39: Loss and accuracy at step 4947: 0.0367481, 0.013\n",
      "2017-11-10 17:22:41: Loss and accuracy at step 4948: 0.0368445, 0.0126\n",
      "2017-11-10 17:22:43: Loss and accuracy at step 4949: 0.0368386, 0.012\n",
      "2017-11-10 17:22:45: Loss and accuracy at step 4950: 0.0367021, 0.0148\n",
      "2017-11-10 17:22:47: Loss and accuracy at step 4951: 0.0367846, 0.0106\n",
      "2017-11-10 17:22:49: Loss and accuracy at step 4952: 0.0367471, 0.0129\n",
      "2017-11-10 17:22:51: Loss and accuracy at step 4953: 0.0368602, 0.0155\n",
      "2017-11-10 17:22:53: Loss and accuracy at step 4954: 0.036762, 0.0096\n",
      "2017-11-10 17:22:55: Loss and accuracy at step 4955: 0.0367264, 0.0082\n",
      "2017-11-10 17:22:57: Loss and accuracy at step 4956: 0.0367181, 0.0178\n",
      "2017-11-10 17:22:59: Loss and accuracy at step 4957: 0.0367635, 0.0077\n",
      "2017-11-10 17:23:01: Loss and accuracy at step 4958: 0.0368891, 0.0129\n",
      "2017-11-10 17:23:04: Loss and accuracy at step 4959: 0.0368097, 0.0109\n",
      "2017-11-10 17:23:06: Loss and accuracy at step 4960: 0.0367296, 0.0104\n",
      "2017-11-10 17:23:08: Loss and accuracy at step 4961: 0.0367365, 0.0199\n",
      "2017-11-10 17:23:10: Loss and accuracy at step 4962: 0.0368098, 0.0066\n",
      "2017-11-10 17:23:12: Loss and accuracy at step 4963: 0.0368202, 0.011\n",
      "2017-11-10 17:23:14: Loss and accuracy at step 4964: 0.0367682, 0.0163\n",
      "2017-11-10 17:23:16: Loss and accuracy at step 4965: 0.0368273, 0.0086\n",
      "2017-11-10 17:23:18: Loss and accuracy at step 4966: 0.0368846, 0.0136\n",
      "2017-11-10 17:23:20: Loss and accuracy at step 4967: 0.0369045, 0.0134\n",
      "2017-11-10 17:23:22: Loss and accuracy at step 4968: 0.0368205, 0.0088\n",
      "2017-11-10 17:23:24: Loss and accuracy at step 4969: 0.0367862, 0.0148\n",
      "2017-11-10 17:23:27: Loss and accuracy at step 4970: 0.0367946, 0.0097\n",
      "2017-11-10 17:23:29: Loss and accuracy at step 4971: 0.0368343, 0.0124\n",
      "2017-11-10 17:23:31: Loss and accuracy at step 4972: 0.0368165, 0.0102\n",
      "2017-11-10 17:23:33: Loss and accuracy at step 4973: 0.0369023, 0.0173\n",
      "2017-11-10 17:23:35: Loss and accuracy at step 4974: 0.0367469, 0.0122\n",
      "2017-11-10 17:23:37: Loss and accuracy at step 4975: 0.0367493, 0.0095\n",
      "2017-11-10 17:23:39: Loss and accuracy at step 4976: 0.0367675, 0.0153\n",
      "2017-11-10 17:23:41: Loss and accuracy at step 4977: 0.0368236, 0.0113\n",
      "2017-11-10 17:23:43: Loss and accuracy at step 4978: 0.0368134, 0.0095\n",
      "2017-11-10 17:23:45: Loss and accuracy at step 4979: 0.0368371, 0.0159\n",
      "2017-11-10 17:23:47: Loss and accuracy at step 4980: 0.0368288, 0.0121\n",
      "2017-11-10 17:23:49: Loss and accuracy at step 4981: 0.0367365, 0.0163\n",
      "2017-11-10 17:23:52: Loss and accuracy at step 4982: 0.0368425, 0.0094\n",
      "2017-11-10 17:23:54: Loss and accuracy at step 4983: 0.0367987, 0.0119\n",
      "2017-11-10 17:23:56: Loss and accuracy at step 4984: 0.0368084, 0.0185\n",
      "2017-11-10 17:23:58: Loss and accuracy at step 4985: 0.036739, 0.0107\n",
      "2017-11-10 17:24:00: Loss and accuracy at step 4986: 0.0368355, 0.0128\n",
      "2017-11-10 17:24:02: Loss and accuracy at step 4987: 0.0367605, 0.0119\n",
      "2017-11-10 17:24:04: Loss and accuracy at step 4988: 0.0368457, 0.0153\n",
      "2017-11-10 17:24:06: Loss and accuracy at step 4989: 0.0367938, 0.0114\n",
      "2017-11-10 17:24:08: Loss and accuracy at step 4990: 0.0367854, 0.0123\n",
      "2017-11-10 17:24:10: Loss and accuracy at step 4991: 0.0367674, 0.0142\n",
      "2017-11-10 17:24:13: Loss and accuracy at step 4992: 0.0367882, 0.0118\n",
      "2017-11-10 17:24:15: Loss and accuracy at step 4993: 0.0368118, 0.0088\n",
      "2017-11-10 17:24:17: Loss and accuracy at step 4994: 0.0367931, 0.0161\n",
      "2017-11-10 17:24:19: Loss and accuracy at step 4995: 0.0368134, 0.0116\n",
      "2017-11-10 17:24:21: Loss and accuracy at step 4996: 0.0367654, 0.0131\n",
      "2017-11-10 17:24:23: Loss and accuracy at step 4997: 0.0367884, 0.0126\n",
      "2017-11-10 17:24:25: Loss and accuracy at step 4998: 0.0367854, 0.0097\n",
      "2017-11-10 17:24:27: Loss and accuracy at step 4999: 0.0368682, 0.0133\n",
      "2017-11-10 17:24:29: Loss and accuracy at step 5000: 0.0368057, 0.0101\n",
      "2017-11-10 17:24:31: Loss and accuracy at step 5001: 0.0367191, 0.0123\n",
      "2017-11-10 17:24:33: Loss and accuracy at step 5002: 0.0367751, 0.0141\n",
      "2017-11-10 17:24:35: Loss and accuracy at step 5003: 0.0368792, 0.011\n",
      "2017-11-10 17:24:38: Loss and accuracy at step 5004: 0.0367553, 0.0154\n",
      "2017-11-10 17:24:40: Loss and accuracy at step 5005: 0.0367949, 0.0131\n",
      "2017-11-10 17:24:42: Loss and accuracy at step 5006: 0.0368753, 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:24:44: Loss and accuracy at step 5007: 0.0368246, 0.0165\n",
      "2017-11-10 17:24:46: Loss and accuracy at step 5008: 0.0367543, 0.0117\n",
      "2017-11-10 17:24:48: Loss and accuracy at step 5009: 0.0367742, 0.0118\n",
      "2017-11-10 17:24:50: Loss and accuracy at step 5010: 0.0368088, 0.0182\n",
      "2017-11-10 17:24:52: Loss and accuracy at step 5011: 0.0368093, 0.0099\n",
      "2017-11-10 17:24:54: Loss and accuracy at step 5012: 0.0366891, 0.0141\n",
      "2017-11-10 17:24:56: Loss and accuracy at step 5013: 0.0367868, 0.009\n",
      "2017-11-10 17:24:58: Loss and accuracy at step 5014: 0.0367598, 0.0145\n",
      "2017-11-10 17:25:01: Loss and accuracy at step 5015: 0.0367913, 0.0119\n",
      "2017-11-10 17:25:03: Loss and accuracy at step 5016: 0.0367928, 0.0093\n",
      "2017-11-10 17:25:05: Loss and accuracy at step 5017: 0.0367568, 0.0132\n",
      "2017-11-10 17:25:07: Loss and accuracy at step 5018: 0.0367953, 0.014\n",
      "2017-11-10 17:25:09: Loss and accuracy at step 5019: 0.0368164, 0.0092\n",
      "2017-11-10 17:25:11: Loss and accuracy at step 5020: 0.0367412, 0.0102\n",
      "2017-11-10 17:25:13: Loss and accuracy at step 5021: 0.0368143, 0.0135\n",
      "2017-11-10 17:25:15: Loss and accuracy at step 5022: 0.0367563, 0.0094\n",
      "2017-11-10 17:25:17: Loss and accuracy at step 5023: 0.0367116, 0.015\n",
      "2017-11-10 17:25:19: Loss and accuracy at step 5024: 0.036806, 0.0112\n",
      "2017-11-10 17:25:21: Loss and accuracy at step 5025: 0.0368326, 0.0108\n",
      "2017-11-10 17:25:23: Loss and accuracy at step 5026: 0.0367556, 0.0114\n",
      "2017-11-10 17:25:26: Loss and accuracy at step 5027: 0.0367219, 0.0089\n",
      "2017-11-10 17:25:28: Loss and accuracy at step 5028: 0.036842, 0.0178\n",
      "2017-11-10 17:25:30: Loss and accuracy at step 5029: 0.0367677, 0.0098\n",
      "2017-11-10 17:25:32: Loss and accuracy at step 5030: 0.0367624, 0.0097\n",
      "2017-11-10 17:25:34: Loss and accuracy at step 5031: 0.036783, 0.0149\n",
      "2017-11-10 17:25:36: Loss and accuracy at step 5032: 0.0367813, 0.0102\n",
      "2017-11-10 17:25:38: Loss and accuracy at step 5033: 0.0367702, 0.0123\n",
      "2017-11-10 17:25:40: Loss and accuracy at step 5034: 0.0368492, 0.0112\n",
      "2017-11-10 17:25:42: Loss and accuracy at step 5035: 0.0368388, 0.0136\n",
      "2017-11-10 17:25:44: Loss and accuracy at step 5036: 0.0368534, 0.013\n",
      "2017-11-10 17:25:46: Loss and accuracy at step 5037: 0.0367792, 0.0094\n",
      "2017-11-10 17:25:49: Loss and accuracy at step 5038: 0.036787, 0.0123\n",
      "2017-11-10 17:25:51: Loss and accuracy at step 5039: 0.0367703, 0.0117\n",
      "2017-11-10 17:25:53: Loss and accuracy at step 5040: 0.0368116, 0.0103\n",
      "2017-11-10 17:25:55: Loss and accuracy at step 5041: 0.0369029, 0.0109\n",
      "2017-11-10 17:25:57: Loss and accuracy at step 5042: 0.0367537, 0.0079\n",
      "2017-11-10 17:25:59: Loss and accuracy at step 5043: 0.0367448, 0.0161\n",
      "2017-11-10 17:26:01: Loss and accuracy at step 5044: 0.0368156, 0.0148\n",
      "2017-11-10 17:26:03: Loss and accuracy at step 5045: 0.0369103, 0.0084\n",
      "2017-11-10 17:26:05: Loss and accuracy at step 5046: 0.036849, 0.0123\n",
      "2017-11-10 17:26:07: Loss and accuracy at step 5047: 0.036779, 0.0145\n",
      "2017-11-10 17:26:09: Loss and accuracy at step 5048: 0.0367934, 0.0102\n",
      "2017-11-10 17:26:11: Loss and accuracy at step 5049: 0.0368856, 0.014\n",
      "2017-11-10 17:26:14: Loss and accuracy at step 5050: 0.0368813, 0.0113\n",
      "2017-11-10 17:26:16: Loss and accuracy at step 5051: 0.0368002, 0.0093\n",
      "2017-11-10 17:26:18: Loss and accuracy at step 5052: 0.0368178, 0.0146\n",
      "2017-11-10 17:26:20: Loss and accuracy at step 5053: 0.0368066, 0.0105\n",
      "2017-11-10 17:26:22: Loss and accuracy at step 5054: 0.0368146, 0.0115\n",
      "2017-11-10 17:26:24: Loss and accuracy at step 5055: 0.0368338, 0.0142\n",
      "2017-11-10 17:26:26: Loss and accuracy at step 5056: 0.0367596, 0.0101\n",
      "2017-11-10 17:26:28: Loss and accuracy at step 5057: 0.0367939, 0.0121\n",
      "2017-11-10 17:26:30: Loss and accuracy at step 5058: 0.0368785, 0.0133\n",
      "2017-11-10 17:26:32: Loss and accuracy at step 5059: 0.0368465, 0.01\n",
      "2017-11-10 17:26:34: Loss and accuracy at step 5060: 0.0368003, 0.0104\n",
      "2017-11-10 17:26:37: Loss and accuracy at step 5061: 0.0367723, 0.0148\n",
      "2017-11-10 17:26:39: Loss and accuracy at step 5062: 0.036815, 0.012\n",
      "2017-11-10 17:26:41: Loss and accuracy at step 5063: 0.0366817, 0.0072\n",
      "2017-11-10 17:26:43: Loss and accuracy at step 5064: 0.0367431, 0.0184\n",
      "2017-11-10 17:26:45: Loss and accuracy at step 5065: 0.0367788, 0.0115\n",
      "2017-11-10 17:26:47: Loss and accuracy at step 5066: 0.036776, 0.0096\n",
      "2017-11-10 17:26:49: Loss and accuracy at step 5067: 0.0368215, 0.013\n",
      "2017-11-10 17:26:51: Loss and accuracy at step 5068: 0.0367497, 0.0144\n",
      "2017-11-10 17:26:53: Loss and accuracy at step 5069: 0.0368111, 0.0091\n",
      "2017-11-10 17:26:55: Loss and accuracy at step 5070: 0.0367493, 0.0088\n",
      "2017-11-10 17:26:57: Loss and accuracy at step 5071: 0.036865, 0.0229\n",
      "2017-11-10 17:26:59: Loss and accuracy at step 5072: 0.0368047, 0.0125\n",
      "2017-11-10 17:27:02: Loss and accuracy at step 5073: 0.0368034, 0.0097\n",
      "2017-11-10 17:27:04: Loss and accuracy at step 5074: 0.0367387, 0.0199\n",
      "2017-11-10 17:27:06: Loss and accuracy at step 5075: 0.0368016, 0.011\n",
      "2017-11-10 17:27:08: Loss and accuracy at step 5076: 0.0367716, 0.0104\n",
      "2017-11-10 17:27:10: Loss and accuracy at step 5077: 0.036813, 0.0131\n",
      "2017-11-10 17:27:12: Loss and accuracy at step 5078: 0.0368495, 0.0101\n",
      "2017-11-10 17:27:14: Loss and accuracy at step 5079: 0.0367274, 0.0199\n",
      "2017-11-10 17:27:16: Loss and accuracy at step 5080: 0.0367787, 0.0066\n",
      "2017-11-10 17:27:18: Loss and accuracy at step 5081: 0.0368876, 0.0159\n",
      "2017-11-10 17:27:20: Loss and accuracy at step 5082: 0.0368352, 0.017\n",
      "2017-11-10 17:27:22: Loss and accuracy at step 5083: 0.0368063, 0.0078\n",
      "2017-11-10 17:27:24: Loss and accuracy at step 5084: 0.036802, 0.0232\n",
      "2017-11-10 17:27:27: Loss and accuracy at step 5085: 0.0367807, 0.009\n",
      "2017-11-10 17:27:29: Loss and accuracy at step 5086: 0.0367068, 0.0182\n",
      "2017-11-10 17:27:31: Loss and accuracy at step 5087: 0.0368114, 0.014\n",
      "2017-11-10 17:27:33: Loss and accuracy at step 5088: 0.036779, 0.0073\n",
      "2017-11-10 17:27:35: Loss and accuracy at step 5089: 0.0368104, 0.0163\n",
      "2017-11-10 17:27:37: Loss and accuracy at step 5090: 0.0368209, 0.0089\n",
      "2017-11-10 17:27:39: Loss and accuracy at step 5091: 0.0368285, 0.013\n",
      "2017-11-10 17:27:41: Loss and accuracy at step 5092: 0.0368124, 0.0122\n",
      "2017-11-10 17:27:43: Loss and accuracy at step 5093: 0.0368081, 0.0136\n",
      "2017-11-10 17:27:45: Loss and accuracy at step 5094: 0.0368421, 0.0129\n",
      "2017-11-10 17:27:47: Loss and accuracy at step 5095: 0.0368588, 0.006\n",
      "2017-11-10 17:27:49: Loss and accuracy at step 5096: 0.0367527, 0.0142\n",
      "2017-11-10 17:27:52: Loss and accuracy at step 5097: 0.0366756, 0.0132\n",
      "2017-11-10 17:27:54: Loss and accuracy at step 5098: 0.0368279, 0.0151\n",
      "2017-11-10 17:27:56: Loss and accuracy at step 5099: 0.0367306, 0.0139\n",
      "2017-11-10 17:27:58: Loss and accuracy at step 5100: 0.036759, 0.0096\n",
      "2017-11-10 17:28:00: Loss and accuracy at step 5101: 0.0367743, 0.0207\n",
      "2017-11-10 17:28:02: Loss and accuracy at step 5102: 0.0367892, 0.0082\n",
      "2017-11-10 17:28:04: Loss and accuracy at step 5103: 0.0367685, 0.0142\n",
      "2017-11-10 17:28:06: Loss and accuracy at step 5104: 0.0367896, 0.0101\n",
      "2017-11-10 17:28:08: Loss and accuracy at step 5105: 0.0367316, 0.0112\n",
      "2017-11-10 17:28:10: Loss and accuracy at step 5106: 0.0367485, 0.0135\n",
      "2017-11-10 17:28:13: Loss and accuracy at step 5107: 0.0368051, 0.0084\n",
      "2017-11-10 17:28:15: Loss and accuracy at step 5108: 0.0368814, 0.0161\n",
      "2017-11-10 17:28:17: Loss and accuracy at step 5109: 0.0367597, 0.0132\n",
      "2017-11-10 17:28:19: Loss and accuracy at step 5110: 0.0368228, 0.0111\n",
      "2017-11-10 17:28:21: Loss and accuracy at step 5111: 0.0368445, 0.0143\n",
      "2017-11-10 17:28:23: Loss and accuracy at step 5112: 0.0367825, 0.0104\n",
      "2017-11-10 17:28:25: Loss and accuracy at step 5113: 0.0367591, 0.013\n",
      "2017-11-10 17:28:27: Loss and accuracy at step 5114: 0.0367859, 0.0121\n",
      "2017-11-10 17:28:29: Loss and accuracy at step 5115: 0.0368337, 0.0132\n",
      "2017-11-10 17:28:31: Loss and accuracy at step 5116: 0.036811, 0.0114\n",
      "2017-11-10 17:28:33: Loss and accuracy at step 5117: 0.0367858, 0.0127\n",
      "2017-11-10 17:28:36: Loss and accuracy at step 5118: 0.0368746, 0.0125\n",
      "2017-11-10 17:28:38: Loss and accuracy at step 5119: 0.0368022, 0.0126\n",
      "2017-11-10 17:28:40: Loss and accuracy at step 5120: 0.0367158, 0.0129\n",
      "2017-11-10 17:28:42: Loss and accuracy at step 5121: 0.036799, 0.0087\n",
      "2017-11-10 17:28:44: Loss and accuracy at step 5122: 0.036816, 0.0146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:28:46: Loss and accuracy at step 5123: 0.0367927, 0.0125\n",
      "2017-11-10 17:28:48: Loss and accuracy at step 5124: 0.0367285, 0.0087\n",
      "2017-11-10 17:28:50: Loss and accuracy at step 5125: 0.036783, 0.0197\n",
      "2017-11-10 17:28:52: Loss and accuracy at step 5126: 0.0367255, 0.0097\n",
      "2017-11-10 17:28:54: Loss and accuracy at step 5127: 0.0367795, 0.0131\n",
      "2017-11-10 17:28:56: Loss and accuracy at step 5128: 0.0368012, 0.0151\n",
      "2017-11-10 17:28:58: Loss and accuracy at step 5129: 0.0367698, 0.006\n",
      "2017-11-10 17:29:01: Loss and accuracy at step 5130: 0.0367175, 0.0243\n",
      "2017-11-10 17:29:03: Loss and accuracy at step 5131: 0.0367441, 0.0082\n",
      "2017-11-10 17:29:05: Loss and accuracy at step 5132: 0.0368048, 0.0126\n",
      "2017-11-10 17:29:07: Loss and accuracy at step 5133: 0.036732, 0.013\n",
      "2017-11-10 17:29:09: Loss and accuracy at step 5134: 0.0367742, 0.0113\n",
      "2017-11-10 17:29:11: Loss and accuracy at step 5135: 0.0368128, 0.0165\n",
      "2017-11-10 17:29:13: Loss and accuracy at step 5136: 0.0368397, 0.0098\n",
      "2017-11-10 17:29:15: Loss and accuracy at step 5137: 0.0368215, 0.0122\n",
      "2017-11-10 17:29:17: Loss and accuracy at step 5138: 0.0368017, 0.0184\n",
      "2017-11-10 17:29:19: Loss and accuracy at step 5139: 0.0367252, 0.01\n",
      "2017-11-10 17:29:21: Loss and accuracy at step 5140: 0.0367911, 0.015\n",
      "2017-11-10 17:29:24: Loss and accuracy at step 5141: 0.0368028, 0.0136\n",
      "2017-11-10 17:29:26: Loss and accuracy at step 5142: 0.0368243, 0.011\n",
      "2017-11-10 17:29:28: Loss and accuracy at step 5143: 0.0368354, 0.0173\n",
      "2017-11-10 17:29:30: Loss and accuracy at step 5144: 0.0368814, 0.0077\n",
      "2017-11-10 17:29:32: Loss and accuracy at step 5145: 0.0368153, 0.0107\n",
      "2017-11-10 17:29:34: Loss and accuracy at step 5146: 0.0367594, 0.0212\n",
      "2017-11-10 17:29:36: Loss and accuracy at step 5147: 0.0368541, 0.0085\n",
      "2017-11-10 17:29:38: Loss and accuracy at step 5148: 0.0368588, 0.0135\n",
      "2017-11-10 17:29:40: Loss and accuracy at step 5149: 0.036844, 0.0132\n",
      "2017-11-10 17:29:42: Loss and accuracy at step 5150: 0.0368303, 0.0113\n",
      "2017-11-10 17:29:44: Loss and accuracy at step 5151: 0.0367604, 0.0113\n",
      "2017-11-10 17:29:46: Loss and accuracy at step 5152: 0.0368172, 0.009\n",
      "2017-11-10 17:29:48: Loss and accuracy at step 5153: 0.0368991, 0.018\n",
      "2017-11-10 17:29:51: Loss and accuracy at step 5154: 0.0368386, 0.0065\n",
      "2017-11-10 17:29:53: Loss and accuracy at step 5155: 0.0368178, 0.0157\n",
      "2017-11-10 17:29:55: Loss and accuracy at step 5156: 0.0368283, 0.0125\n",
      "2017-11-10 17:29:57: Loss and accuracy at step 5157: 0.036771, 0.0095\n",
      "2017-11-10 17:29:59: Loss and accuracy at step 5158: 0.0367804, 0.017\n",
      "2017-11-10 17:30:01: Loss and accuracy at step 5159: 0.0367323, 0.0088\n",
      "2017-11-10 17:30:03: Loss and accuracy at step 5160: 0.0367999, 0.0127\n",
      "2017-11-10 17:30:05: Loss and accuracy at step 5161: 0.0367793, 0.0141\n",
      "2017-11-10 17:30:07: Loss and accuracy at step 5162: 0.0368163, 0.0079\n",
      "2017-11-10 17:30:09: Loss and accuracy at step 5163: 0.0368245, 0.0143\n",
      "2017-11-10 17:30:11: Loss and accuracy at step 5164: 0.0368292, 0.0127\n",
      "2017-11-10 17:30:14: Loss and accuracy at step 5165: 0.0368364, 0.0104\n",
      "2017-11-10 17:30:16: Loss and accuracy at step 5166: 0.0368055, 0.0135\n",
      "2017-11-10 17:30:18: Loss and accuracy at step 5167: 0.0368219, 0.009\n",
      "2017-11-10 17:30:20: Loss and accuracy at step 5168: 0.0366696, 0.0119\n",
      "2017-11-10 17:30:22: Loss and accuracy at step 5169: 0.0367982, 0.0104\n",
      "2017-11-10 17:30:24: Loss and accuracy at step 5170: 0.0367888, 0.0099\n",
      "2017-11-10 17:30:26: Loss and accuracy at step 5171: 0.0367018, 0.0127\n",
      "2017-11-10 17:30:28: Loss and accuracy at step 5172: 0.0368367, 0.0152\n",
      "2017-11-10 17:30:30: Loss and accuracy at step 5173: 0.0367709, 0.009\n",
      "2017-11-10 17:30:32: Loss and accuracy at step 5174: 0.0367967, 0.0126\n",
      "2017-11-10 17:30:34: Loss and accuracy at step 5175: 0.0368145, 0.0153\n",
      "2017-11-10 17:30:37: Loss and accuracy at step 5176: 0.0368192, 0.0109\n",
      "2017-11-10 17:30:39: Loss and accuracy at step 5177: 0.0369057, 0.0185\n",
      "2017-11-10 17:30:41: Loss and accuracy at step 5178: 0.0367745, 0.0088\n",
      "2017-11-10 17:30:43: Loss and accuracy at step 5179: 0.0366901, 0.0119\n",
      "2017-11-10 17:30:45: Loss and accuracy at step 5180: 0.0367805, 0.0157\n",
      "2017-11-10 17:30:47: Loss and accuracy at step 5181: 0.0367151, 0.0115\n",
      "2017-11-10 17:30:49: Loss and accuracy at step 5182: 0.0367711, 0.013\n",
      "2017-11-10 17:30:51: Loss and accuracy at step 5183: 0.0367443, 0.0131\n",
      "2017-11-10 17:30:53: Loss and accuracy at step 5184: 0.0368434, 0.0147\n",
      "2017-11-10 17:30:55: Loss and accuracy at step 5185: 0.0368191, 0.0101\n",
      "2017-11-10 17:30:57: Loss and accuracy at step 5186: 0.0368095, 0.0116\n",
      "2017-11-10 17:31:00: Loss and accuracy at step 5187: 0.0367962, 0.0136\n",
      "2017-11-10 17:31:02: Loss and accuracy at step 5188: 0.0368175, 0.0099\n",
      "2017-11-10 17:31:04: Loss and accuracy at step 5189: 0.0367856, 0.0177\n",
      "2017-11-10 17:31:06: Loss and accuracy at step 5190: 0.0368506, 0.0072\n",
      "2017-11-10 17:31:08: Loss and accuracy at step 5191: 0.0367883, 0.0139\n",
      "2017-11-10 17:31:10: Loss and accuracy at step 5192: 0.036834, 0.016\n",
      "2017-11-10 17:31:12: Loss and accuracy at step 5193: 0.0367752, 0.0101\n",
      "2017-11-10 17:31:14: Loss and accuracy at step 5194: 0.0367777, 0.0199\n",
      "2017-11-10 17:31:16: Loss and accuracy at step 5195: 0.0368318, 0.0113\n",
      "2017-11-10 17:31:18: Loss and accuracy at step 5196: 0.03686, 0.0125\n",
      "2017-11-10 17:31:20: Loss and accuracy at step 5197: 0.0368068, 0.0154\n",
      "2017-11-10 17:31:22: Loss and accuracy at step 5198: 0.0367678, 0.0088\n",
      "2017-11-10 17:31:25: Loss and accuracy at step 5199: 0.0368617, 0.0147\n",
      "2017-11-10 17:31:27: Loss and accuracy at step 5200: 0.0368039, 0.0151\n",
      "2017-11-10 17:31:29: Loss and accuracy at step 5201: 0.0368222, 0.0074\n",
      "2017-11-10 17:31:31: Loss and accuracy at step 5202: 0.0368243, 0.0122\n",
      "2017-11-10 17:31:33: Loss and accuracy at step 5203: 0.0367268, 0.0173\n",
      "2017-11-10 17:31:35: Loss and accuracy at step 5204: 0.0367701, 0.0064\n",
      "2017-11-10 17:31:37: Loss and accuracy at step 5205: 0.0367803, 0.0161\n",
      "2017-11-10 17:31:39: Loss and accuracy at step 5206: 0.0368551, 0.013\n",
      "2017-11-10 17:31:41: Loss and accuracy at step 5207: 0.0368207, 0.0106\n",
      "2017-11-10 17:31:43: Loss and accuracy at step 5208: 0.0367134, 0.0091\n",
      "2017-11-10 17:31:45: Loss and accuracy at step 5209: 0.0368236, 0.0147\n",
      "2017-11-10 17:31:47: Loss and accuracy at step 5210: 0.0367882, 0.0097\n",
      "2017-11-10 17:31:50: Loss and accuracy at step 5211: 0.0367994, 0.0109\n",
      "2017-11-10 17:31:52: Loss and accuracy at step 5212: 0.0367086, 0.0155\n",
      "2017-11-10 17:31:54: Loss and accuracy at step 5213: 0.0367626, 0.0108\n",
      "2017-11-10 17:31:56: Loss and accuracy at step 5214: 0.0367791, 0.0117\n",
      "2017-11-10 17:31:58: Loss and accuracy at step 5215: 0.0367015, 0.0113\n",
      "2017-11-10 17:32:00: Loss and accuracy at step 5216: 0.0367905, 0.0112\n",
      "2017-11-10 17:32:02: Loss and accuracy at step 5217: 0.0367856, 0.0158\n",
      "2017-11-10 17:32:04: Loss and accuracy at step 5218: 0.0367886, 0.0092\n",
      "2017-11-10 17:32:06: Loss and accuracy at step 5219: 0.0368005, 0.0133\n",
      "2017-11-10 17:32:08: Loss and accuracy at step 5220: 0.0367534, 0.0114\n",
      "2017-11-10 17:32:11: Loss and accuracy at step 5221: 0.0367815, 0.0092\n",
      "2017-11-10 17:32:13: Loss and accuracy at step 5222: 0.0367621, 0.0153\n",
      "2017-11-10 17:32:15: Loss and accuracy at step 5223: 0.0368424, 0.0098\n",
      "2017-11-10 17:32:17: Loss and accuracy at step 5224: 0.036734, 0.0117\n",
      "2017-11-10 17:32:19: Loss and accuracy at step 5225: 0.0368327, 0.0138\n",
      "2017-11-10 17:32:21: Loss and accuracy at step 5226: 0.0368326, 0.0121\n",
      "2017-11-10 17:32:23: Loss and accuracy at step 5227: 0.0368288, 0.0141\n",
      "2017-11-10 17:32:25: Loss and accuracy at step 5228: 0.036766, 0.011\n",
      "2017-11-10 17:32:27: Loss and accuracy at step 5229: 0.0368589, 0.0154\n",
      "2017-11-10 17:32:29: Loss and accuracy at step 5230: 0.0368864, 0.0103\n",
      "2017-11-10 17:32:31: Loss and accuracy at step 5231: 0.0368547, 0.013\n",
      "2017-11-10 17:32:34: Loss and accuracy at step 5232: 0.0367631, 0.0117\n",
      "2017-11-10 17:32:36: Loss and accuracy at step 5233: 0.0368695, 0.013\n",
      "2017-11-10 17:32:38: Loss and accuracy at step 5234: 0.0367345, 0.0157\n",
      "2017-11-10 17:32:40: Loss and accuracy at step 5235: 0.0367162, 0.011\n",
      "2017-11-10 17:32:42: Loss and accuracy at step 5236: 0.0368036, 0.0194\n",
      "2017-11-10 17:32:44: Loss and accuracy at step 5237: 0.0367393, 0.0098\n",
      "2017-11-10 17:32:46: Loss and accuracy at step 5238: 0.036753, 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:32:48: Loss and accuracy at step 5239: 0.0367671, 0.0116\n",
      "2017-11-10 17:32:50: Loss and accuracy at step 5240: 0.0367728, 0.0103\n",
      "2017-11-10 17:32:52: Loss and accuracy at step 5241: 0.0368454, 0.0149\n",
      "2017-11-10 17:32:54: Loss and accuracy at step 5242: 0.0368102, 0.0094\n",
      "2017-11-10 17:32:56: Loss and accuracy at step 5243: 0.0367663, 0.0136\n",
      "2017-11-10 17:32:59: Loss and accuracy at step 5244: 0.0367936, 0.0117\n",
      "2017-11-10 17:33:01: Loss and accuracy at step 5245: 0.0367426, 0.0111\n",
      "2017-11-10 17:33:03: Loss and accuracy at step 5246: 0.0368795, 0.0129\n",
      "2017-11-10 17:33:05: Loss and accuracy at step 5247: 0.036816, 0.0112\n",
      "2017-11-10 17:33:07: Loss and accuracy at step 5248: 0.0367585, 0.0137\n",
      "2017-11-10 17:33:09: Loss and accuracy at step 5249: 0.0367925, 0.0111\n",
      "2017-11-10 17:33:11: Loss and accuracy at step 5250: 0.0368001, 0.0162\n",
      "2017-11-10 17:33:13: Loss and accuracy at step 5251: 0.0367624, 0.0116\n",
      "2017-11-10 17:33:15: Loss and accuracy at step 5252: 0.0368554, 0.0131\n",
      "2017-11-10 17:33:17: Loss and accuracy at step 5253: 0.0368636, 0.0123\n",
      "2017-11-10 17:33:19: Loss and accuracy at step 5254: 0.0368601, 0.0093\n",
      "2017-11-10 17:33:21: Loss and accuracy at step 5255: 0.03687, 0.0122\n",
      "2017-11-10 17:33:24: Loss and accuracy at step 5256: 0.0368536, 0.0091\n",
      "2017-11-10 17:33:26: Loss and accuracy at step 5257: 0.0368252, 0.0127\n",
      "2017-11-10 17:33:28: Loss and accuracy at step 5258: 0.0368676, 0.0098\n",
      "2017-11-10 17:33:30: Loss and accuracy at step 5259: 0.03679, 0.0118\n",
      "2017-11-10 17:33:32: Loss and accuracy at step 5260: 0.036807, 0.0129\n",
      "2017-11-10 17:33:34: Loss and accuracy at step 5261: 0.0368165, 0.0134\n",
      "2017-11-10 17:33:36: Loss and accuracy at step 5262: 0.0368649, 0.0132\n",
      "2017-11-10 17:33:38: Loss and accuracy at step 5263: 0.0368604, 0.0121\n",
      "2017-11-10 17:33:40: Loss and accuracy at step 5264: 0.0368538, 0.0087\n",
      "2017-11-10 17:33:42: Loss and accuracy at step 5265: 0.0368307, 0.0203\n",
      "2017-11-10 17:33:44: Loss and accuracy at step 5266: 0.036789, 0.0098\n",
      "2017-11-10 17:33:46: Loss and accuracy at step 5267: 0.0367861, 0.0096\n",
      "2017-11-10 17:33:49: Loss and accuracy at step 5268: 0.036735, 0.014\n",
      "2017-11-10 17:33:51: Loss and accuracy at step 5269: 0.0367756, 0.013\n",
      "2017-11-10 17:33:53: Loss and accuracy at step 5270: 0.0367192, 0.0104\n",
      "2017-11-10 17:33:55: Loss and accuracy at step 5271: 0.0368481, 0.0169\n",
      "2017-11-10 17:33:57: Loss and accuracy at step 5272: 0.0368013, 0.0123\n",
      "2017-11-10 17:33:59: Loss and accuracy at step 5273: 0.0367715, 0.0101\n",
      "2017-11-10 17:34:01: Loss and accuracy at step 5274: 0.0368116, 0.0153\n",
      "2017-11-10 17:34:03: Loss and accuracy at step 5275: 0.0367678, 0.0114\n",
      "2017-11-10 17:34:05: Loss and accuracy at step 5276: 0.0367333, 0.0102\n",
      "2017-11-10 17:34:07: Loss and accuracy at step 5277: 0.0368357, 0.0186\n",
      "2017-11-10 17:34:09: Loss and accuracy at step 5278: 0.0367533, 0.0116\n",
      "2017-11-10 17:34:12: Loss and accuracy at step 5279: 0.0367229, 0.0131\n",
      "2017-11-10 17:34:14: Loss and accuracy at step 5280: 0.036814, 0.0128\n",
      "2017-11-10 17:34:16: Loss and accuracy at step 5281: 0.0367287, 0.014\n",
      "2017-11-10 17:34:18: Loss and accuracy at step 5282: 0.036672, 0.0137\n",
      "2017-11-10 17:34:20: Loss and accuracy at step 5283: 0.0366815, 0.0113\n",
      "2017-11-10 17:34:22: Loss and accuracy at step 5284: 0.0368704, 0.0154\n",
      "2017-11-10 17:34:24: Loss and accuracy at step 5285: 0.0367764, 0.0127\n",
      "2017-11-10 17:34:26: Loss and accuracy at step 5286: 0.036797, 0.0091\n",
      "2017-11-10 17:34:28: Loss and accuracy at step 5287: 0.0368505, 0.0314\n",
      "2017-11-10 17:34:30: Loss and accuracy at step 5288: 0.0368399, 0.0097\n",
      "2017-11-10 17:34:33: Loss and accuracy at step 5289: 0.0368316, 0.0144\n",
      "2017-11-10 17:34:35: Loss and accuracy at step 5290: 0.0367466, 0.0225\n",
      "2017-11-10 17:34:37: Loss and accuracy at step 5291: 0.0368117, 0.0083\n",
      "2017-11-10 17:34:39: Loss and accuracy at step 5292: 0.03683, 0.0217\n",
      "2017-11-10 17:34:41: Loss and accuracy at step 5293: 0.036792, 0.0113\n",
      "2017-11-10 17:34:43: Loss and accuracy at step 5294: 0.0368225, 0.0164\n",
      "2017-11-10 17:34:45: Loss and accuracy at step 5295: 0.036828, 0.0107\n",
      "2017-11-10 17:34:47: Loss and accuracy at step 5296: 0.0368233, 0.0106\n",
      "2017-11-10 17:34:49: Loss and accuracy at step 5297: 0.0367835, 0.0205\n",
      "2017-11-10 17:34:51: Loss and accuracy at step 5298: 0.0367533, 0.0054\n",
      "2017-11-10 17:34:53: Loss and accuracy at step 5299: 0.0368323, 0.0149\n",
      "2017-11-10 17:34:55: Loss and accuracy at step 5300: 0.0368384, 0.0167\n",
      "2017-11-10 17:34:58: Loss and accuracy at step 5301: 0.0368587, 0.0069\n",
      "2017-11-10 17:35:00: Loss and accuracy at step 5302: 0.0369182, 0.0224\n",
      "2017-11-10 17:35:02: Loss and accuracy at step 5303: 0.0368275, 0.0085\n",
      "2017-11-10 17:35:04: Loss and accuracy at step 5304: 0.0368368, 0.0127\n",
      "2017-11-10 17:35:06: Loss and accuracy at step 5305: 0.0368495, 0.0178\n",
      "2017-11-10 17:35:08: Loss and accuracy at step 5306: 0.0367897, 0.0061\n",
      "2017-11-10 17:35:10: Loss and accuracy at step 5307: 0.0369141, 0.0234\n",
      "2017-11-10 17:35:12: Loss and accuracy at step 5308: 0.0368777, 0.0066\n",
      "2017-11-10 17:35:14: Loss and accuracy at step 5309: 0.0368234, 0.0087\n",
      "2017-11-10 17:35:16: Loss and accuracy at step 5310: 0.0369418, 0.0404\n",
      "2017-11-10 17:35:18: Loss and accuracy at step 5311: 0.0369095, 0.0032\n",
      "2017-11-10 17:35:20: Loss and accuracy at step 5312: 0.0370434, 0.0159\n",
      "2017-11-10 17:35:23: Loss and accuracy at step 5313: 0.0369442, 0.0319\n",
      "2017-11-10 17:35:25: Loss and accuracy at step 5314: 0.0370235, 0.0057\n",
      "2017-11-10 17:35:27: Loss and accuracy at step 5315: 0.0371555, 0.0259\n",
      "2017-11-10 17:35:29: Loss and accuracy at step 5316: 0.0369984, 0.009\n",
      "2017-11-10 17:35:31: Loss and accuracy at step 5317: 0.0370383, 0.0134\n",
      "2017-11-10 17:35:33: Loss and accuracy at step 5318: 0.0371676, 0.0536\n",
      "2017-11-10 17:35:35: Loss and accuracy at step 5319: 0.0372079, 0.0041\n",
      "2017-11-10 17:35:37: Loss and accuracy at step 5320: 0.0372269, 0.0154\n",
      "2017-11-10 17:35:39: Loss and accuracy at step 5321: 0.0372059, 0.0406\n",
      "2017-11-10 17:35:41: Loss and accuracy at step 5322: 0.0373951, 0.0039\n",
      "2017-11-10 17:35:43: Loss and accuracy at step 5323: 0.0371794, 0.0234\n",
      "2017-11-10 17:35:46: Loss and accuracy at step 5324: 0.0376014, 0.0969\n",
      "2017-11-10 17:35:48: Loss and accuracy at step 5325: 0.0373318, 0.0036\n",
      "2017-11-10 17:35:50: Loss and accuracy at step 5326: 0.0376841, 0.0304\n",
      "2017-11-10 17:35:52: Loss and accuracy at step 5327: 0.0377551, 0.1149\n",
      "2017-11-10 17:35:54: Loss and accuracy at step 5328: 0.0377352, 0.0052\n",
      "2017-11-10 17:35:56: Loss and accuracy at step 5329: 0.0377339, 0.0181\n",
      "2017-11-10 17:35:58: Loss and accuracy at step 5330: 0.0374949, 0.0233\n",
      "2017-11-10 17:36:00: Loss and accuracy at step 5331: 0.0374601, 0.0072\n",
      "2017-11-10 17:36:02: Loss and accuracy at step 5332: 0.0373906, 0.0137\n",
      "2017-11-10 17:36:04: Loss and accuracy at step 5333: 0.0372977, 0.0207\n",
      "2017-11-10 17:36:06: Loss and accuracy at step 5334: 0.0372498, 0.0134\n",
      "2017-11-10 17:36:08: Loss and accuracy at step 5335: 0.0374026, 0.0089\n",
      "2017-11-10 17:36:11: Loss and accuracy at step 5336: 0.0371668, 0.0178\n",
      "2017-11-10 17:36:13: Loss and accuracy at step 5337: 0.0370865, 0.0334\n",
      "2017-11-10 17:36:15: Loss and accuracy at step 5338: 0.037319, 0.0072\n",
      "2017-11-10 17:36:17: Loss and accuracy at step 5339: 0.037068, 0.0187\n",
      "2017-11-10 17:36:19: Loss and accuracy at step 5340: 0.0370835, 0.0113\n",
      "2017-11-10 17:36:21: Loss and accuracy at step 5341: 0.0371344, 0.022\n",
      "2017-11-10 17:36:23: Loss and accuracy at step 5342: 0.0371183, 0.0183\n",
      "2017-11-10 17:36:25: Loss and accuracy at step 5343: 0.037026, 0.0073\n",
      "2017-11-10 17:36:27: Loss and accuracy at step 5344: 0.0371104, 0.012\n",
      "2017-11-10 17:36:29: Loss and accuracy at step 5345: 0.036999, 0.0207\n",
      "2017-11-10 17:36:32: Loss and accuracy at step 5346: 0.0369486, 0.0103\n",
      "2017-11-10 17:36:34: Loss and accuracy at step 5347: 0.0369901, 0.0042\n",
      "2017-11-10 17:36:36: Loss and accuracy at step 5348: 0.0369661, 0.0204\n",
      "2017-11-10 17:36:38: Loss and accuracy at step 5349: 0.0369688, 0.0246\n",
      "2017-11-10 17:36:40: Loss and accuracy at step 5350: 0.0369463, 0.0101\n",
      "2017-11-10 17:36:42: Loss and accuracy at step 5351: 0.0369062, 0.0056\n",
      "2017-11-10 17:36:44: Loss and accuracy at step 5352: 0.036848, 0.033\n",
      "2017-11-10 17:36:46: Loss and accuracy at step 5353: 0.0369012, 0.0154\n",
      "2017-11-10 17:36:48: Loss and accuracy at step 5354: 0.0368672, 0.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:36:50: Loss and accuracy at step 5355: 0.0367722, 0.0114\n",
      "2017-11-10 17:36:52: Loss and accuracy at step 5356: 0.0367866, 0.0198\n",
      "2017-11-10 17:36:55: Loss and accuracy at step 5357: 0.0367629, 0.0098\n",
      "2017-11-10 17:36:57: Loss and accuracy at step 5358: 0.0367927, 0.0093\n",
      "2017-11-10 17:36:59: Loss and accuracy at step 5359: 0.0368694, 0.0336\n",
      "2017-11-10 17:37:01: Loss and accuracy at step 5360: 0.0368488, 0.0157\n",
      "2017-11-10 17:37:03: Loss and accuracy at step 5361: 0.0368863, 0.0092\n",
      "2017-11-10 17:37:05: Loss and accuracy at step 5362: 0.036779, 0.0095\n",
      "2017-11-10 17:37:07: Loss and accuracy at step 5363: 0.0368355, 0.0344\n",
      "2017-11-10 17:37:09: Loss and accuracy at step 5364: 0.036907, 0.0095\n",
      "2017-11-10 17:37:11: Loss and accuracy at step 5365: 0.0368237, 0.0062\n",
      "2017-11-10 17:37:13: Loss and accuracy at step 5366: 0.0368067, 0.016\n",
      "2017-11-10 17:37:15: Loss and accuracy at step 5367: 0.0367849, 0.0259\n",
      "2017-11-10 17:37:17: Loss and accuracy at step 5368: 0.0369056, 0.0083\n",
      "2017-11-10 17:37:19: Loss and accuracy at step 5369: 0.0368571, 0.0081\n",
      "2017-11-10 17:37:22: Loss and accuracy at step 5370: 0.0369324, 0.018\n",
      "2017-11-10 17:37:24: Loss and accuracy at step 5371: 0.0367853, 0.0155\n",
      "2017-11-10 17:37:26: Loss and accuracy at step 5372: 0.0368213, 0.0083\n",
      "2017-11-10 17:37:28: Loss and accuracy at step 5373: 0.0367577, 0.0108\n",
      "2017-11-10 17:37:30: Loss and accuracy at step 5374: 0.0367858, 0.0186\n",
      "2017-11-10 17:37:32: Loss and accuracy at step 5375: 0.0368189, 0.0125\n",
      "2017-11-10 17:37:34: Loss and accuracy at step 5376: 0.0369048, 0.01\n",
      "2017-11-10 17:37:36: Loss and accuracy at step 5377: 0.0367738, 0.0117\n",
      "2017-11-10 17:37:38: Loss and accuracy at step 5378: 0.036743, 0.0148\n",
      "2017-11-10 17:37:40: Loss and accuracy at step 5379: 0.0368829, 0.0116\n",
      "2017-11-10 17:37:42: Loss and accuracy at step 5380: 0.0368342, 0.0119\n",
      "2017-11-10 17:37:45: Loss and accuracy at step 5381: 0.0368011, 0.0139\n",
      "2017-11-10 17:37:47: Loss and accuracy at step 5382: 0.0368599, 0.0115\n",
      "2017-11-10 17:37:49: Loss and accuracy at step 5383: 0.0368404, 0.0091\n",
      "2017-11-10 17:37:51: Loss and accuracy at step 5384: 0.036762, 0.0131\n",
      "2017-11-10 17:37:53: Loss and accuracy at step 5385: 0.0368154, 0.0157\n",
      "2017-11-10 17:37:55: Loss and accuracy at step 5386: 0.036829, 0.0168\n",
      "2017-11-10 17:37:57: Loss and accuracy at step 5387: 0.0368459, 0.0106\n",
      "2017-11-10 17:37:59: Loss and accuracy at step 5388: 0.0368313, 0.0103\n",
      "2017-11-10 17:38:01: Loss and accuracy at step 5389: 0.0367808, 0.0132\n",
      "2017-11-10 17:38:03: Loss and accuracy at step 5390: 0.0368757, 0.0171\n",
      "2017-11-10 17:38:05: Loss and accuracy at step 5391: 0.0367609, 0.0098\n",
      "2017-11-10 17:38:07: Loss and accuracy at step 5392: 0.0368114, 0.0083\n",
      "2017-11-10 17:38:09: Loss and accuracy at step 5393: 0.0367762, 0.0158\n",
      "2017-11-10 17:38:12: Loss and accuracy at step 5394: 0.0368404, 0.0138\n",
      "2017-11-10 17:38:14: Loss and accuracy at step 5395: 0.0368024, 0.011\n",
      "2017-11-10 17:38:16: Loss and accuracy at step 5396: 0.036852, 0.0103\n",
      "2017-11-10 17:38:18: Loss and accuracy at step 5397: 0.0367951, 0.0137\n",
      "2017-11-10 17:38:20: Loss and accuracy at step 5398: 0.036745, 0.0171\n",
      "2017-11-10 17:38:22: Loss and accuracy at step 5399: 0.0367496, 0.0127\n",
      "2017-11-10 17:38:24: Loss and accuracy at step 5400: 0.0368161, 0.0106\n",
      "2017-11-10 17:38:26: Loss and accuracy at step 5401: 0.0367094, 0.0125\n",
      "2017-11-10 17:38:28: Loss and accuracy at step 5402: 0.0366488, 0.0132\n",
      "2017-11-10 17:38:30: Loss and accuracy at step 5403: 0.0367995, 0.0108\n",
      "2017-11-10 17:38:33: Loss and accuracy at step 5404: 0.0367168, 0.0114\n",
      "2017-11-10 17:38:35: Loss and accuracy at step 5405: 0.0367884, 0.0097\n",
      "2017-11-10 17:38:37: Loss and accuracy at step 5406: 0.0366769, 0.0118\n",
      "2017-11-10 17:38:39: Loss and accuracy at step 5407: 0.0367438, 0.0159\n",
      "2017-11-10 17:38:41: Loss and accuracy at step 5408: 0.036816, 0.0101\n",
      "2017-11-10 17:38:43: Loss and accuracy at step 5409: 0.0367779, 0.0099\n",
      "2017-11-10 17:38:45: Loss and accuracy at step 5410: 0.0367512, 0.0129\n",
      "2017-11-10 17:38:47: Loss and accuracy at step 5411: 0.0367329, 0.0171\n",
      "2017-11-10 17:38:49: Loss and accuracy at step 5412: 0.0367912, 0.0184\n",
      "2017-11-10 17:38:51: Loss and accuracy at step 5413: 0.0368421, 0.0095\n",
      "2017-11-10 17:38:54: Loss and accuracy at step 5414: 0.0367902, 0.0104\n",
      "2017-11-10 17:38:56: Loss and accuracy at step 5415: 0.0368102, 0.015\n",
      "2017-11-10 17:38:58: Loss and accuracy at step 5416: 0.0368077, 0.0159\n",
      "2017-11-10 17:39:00: Loss and accuracy at step 5417: 0.0367758, 0.0099\n",
      "2017-11-10 17:39:02: Loss and accuracy at step 5418: 0.0367756, 0.0109\n",
      "2017-11-10 17:39:04: Loss and accuracy at step 5419: 0.036798, 0.0175\n",
      "2017-11-10 17:39:06: Loss and accuracy at step 5420: 0.0367791, 0.0144\n",
      "2017-11-10 17:39:08: Loss and accuracy at step 5421: 0.0368572, 0.0089\n",
      "2017-11-10 17:39:10: Loss and accuracy at step 5422: 0.0367783, 0.0117\n",
      "2017-11-10 17:39:12: Loss and accuracy at step 5423: 0.0368956, 0.0168\n",
      "2017-11-10 17:39:14: Loss and accuracy at step 5424: 0.0367148, 0.0134\n",
      "2017-11-10 17:39:16: Loss and accuracy at step 5425: 0.0367786, 0.011\n",
      "2017-11-10 17:39:19: Loss and accuracy at step 5426: 0.0367536, 0.0103\n",
      "2017-11-10 17:39:21: Loss and accuracy at step 5427: 0.0368165, 0.015\n",
      "2017-11-10 17:39:23: Loss and accuracy at step 5428: 0.0367677, 0.0136\n",
      "2017-11-10 17:39:25: Loss and accuracy at step 5429: 0.0368398, 0.0118\n",
      "2017-11-10 17:39:27: Loss and accuracy at step 5430: 0.036737, 0.0132\n",
      "2017-11-10 17:39:29: Loss and accuracy at step 5431: 0.036771, 0.0117\n",
      "2017-11-10 17:39:31: Loss and accuracy at step 5432: 0.0368466, 0.0121\n",
      "2017-11-10 17:39:33: Loss and accuracy at step 5433: 0.0368061, 0.0107\n",
      "2017-11-10 17:39:35: Loss and accuracy at step 5434: 0.0368341, 0.0115\n",
      "2017-11-10 17:39:37: Loss and accuracy at step 5435: 0.036729, 0.0118\n",
      "2017-11-10 17:39:39: Loss and accuracy at step 5436: 0.0367163, 0.012\n",
      "2017-11-10 17:39:41: Loss and accuracy at step 5437: 0.0367686, 0.01\n",
      "2017-11-10 17:39:44: Loss and accuracy at step 5438: 0.036802, 0.0111\n",
      "2017-11-10 17:39:46: Loss and accuracy at step 5439: 0.0367339, 0.0105\n",
      "2017-11-10 17:39:48: Loss and accuracy at step 5440: 0.0367543, 0.0154\n",
      "2017-11-10 17:39:50: Loss and accuracy at step 5441: 0.0368054, 0.0115\n",
      "2017-11-10 17:39:52: Loss and accuracy at step 5442: 0.0367699, 0.0103\n",
      "2017-11-10 17:39:54: Loss and accuracy at step 5443: 0.0368116, 0.011\n",
      "2017-11-10 17:39:56: Loss and accuracy at step 5444: 0.0367351, 0.0118\n",
      "2017-11-10 17:39:58: Loss and accuracy at step 5445: 0.036717, 0.0108\n",
      "2017-11-10 17:40:00: Loss and accuracy at step 5446: 0.0367558, 0.0097\n",
      "2017-11-10 17:40:02: Loss and accuracy at step 5447: 0.0367967, 0.0126\n",
      "2017-11-10 17:40:04: Loss and accuracy at step 5448: 0.0367007, 0.0128\n",
      "2017-11-10 17:40:07: Loss and accuracy at step 5449: 0.0367167, 0.0114\n",
      "2017-11-10 17:40:09: Loss and accuracy at step 5450: 0.0367421, 0.0102\n",
      "2017-11-10 17:40:11: Loss and accuracy at step 5451: 0.0367599, 0.0149\n",
      "2017-11-10 17:40:13: Loss and accuracy at step 5452: 0.0368062, 0.0141\n",
      "2017-11-10 17:40:15: Loss and accuracy at step 5453: 0.0368051, 0.0085\n",
      "2017-11-10 17:40:17: Loss and accuracy at step 5454: 0.0367886, 0.0108\n",
      "2017-11-10 17:40:19: Loss and accuracy at step 5455: 0.036796, 0.0167\n",
      "2017-11-10 17:40:21: Loss and accuracy at step 5456: 0.0367287, 0.0112\n",
      "2017-11-10 17:40:23: Loss and accuracy at step 5457: 0.0368183, 0.0114\n",
      "2017-11-10 17:40:25: Loss and accuracy at step 5458: 0.0368459, 0.0132\n",
      "2017-11-10 17:40:28: Loss and accuracy at step 5459: 0.0368536, 0.0113\n",
      "2017-11-10 17:40:30: Loss and accuracy at step 5460: 0.036747, 0.0147\n",
      "2017-11-10 17:40:32: Loss and accuracy at step 5461: 0.0367989, 0.0124\n",
      "2017-11-10 17:40:34: Loss and accuracy at step 5462: 0.036813, 0.0107\n",
      "2017-11-10 17:40:36: Loss and accuracy at step 5463: 0.0367987, 0.0146\n",
      "2017-11-10 17:40:38: Loss and accuracy at step 5464: 0.0367547, 0.0145\n",
      "2017-11-10 17:40:40: Loss and accuracy at step 5465: 0.0367887, 0.0108\n",
      "2017-11-10 17:40:42: Loss and accuracy at step 5466: 0.0367989, 0.0109\n",
      "2017-11-10 17:40:44: Loss and accuracy at step 5467: 0.0368326, 0.0133\n",
      "2017-11-10 17:40:46: Loss and accuracy at step 5468: 0.0367213, 0.0099\n",
      "2017-11-10 17:40:48: Loss and accuracy at step 5469: 0.0367563, 0.0119\n",
      "2017-11-10 17:40:51: Loss and accuracy at step 5470: 0.03673, 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:40:53: Loss and accuracy at step 5471: 0.036793, 0.0152\n",
      "2017-11-10 17:40:55: Loss and accuracy at step 5472: 0.0366827, 0.0121\n",
      "2017-11-10 17:40:57: Loss and accuracy at step 5473: 0.0367296, 0.0128\n",
      "2017-11-10 17:40:59: Loss and accuracy at step 5474: 0.0368456, 0.0102\n",
      "2017-11-10 17:41:01: Loss and accuracy at step 5475: 0.0367922, 0.0109\n",
      "2017-11-10 17:41:03: Loss and accuracy at step 5476: 0.0367919, 0.0106\n",
      "2017-11-10 17:41:05: Loss and accuracy at step 5477: 0.0367413, 0.0135\n",
      "2017-11-10 17:41:07: Loss and accuracy at step 5478: 0.0367409, 0.013\n",
      "2017-11-10 17:41:09: Loss and accuracy at step 5479: 0.0367128, 0.0103\n",
      "2017-11-10 17:41:11: Loss and accuracy at step 5480: 0.0367433, 0.0102\n",
      "2017-11-10 17:41:13: Loss and accuracy at step 5481: 0.0367426, 0.0108\n",
      "2017-11-10 17:41:16: Loss and accuracy at step 5482: 0.0367942, 0.019\n",
      "2017-11-10 17:41:18: Loss and accuracy at step 5483: 0.0367859, 0.0101\n",
      "2017-11-10 17:41:20: Loss and accuracy at step 5484: 0.0366923, 0.01\n",
      "2017-11-10 17:41:22: Loss and accuracy at step 5485: 0.0367561, 0.015\n",
      "2017-11-10 17:41:24: Loss and accuracy at step 5486: 0.0368486, 0.0116\n",
      "2017-11-10 17:41:26: Loss and accuracy at step 5487: 0.0368227, 0.01\n",
      "2017-11-10 17:41:28: Loss and accuracy at step 5488: 0.0366609, 0.011\n",
      "2017-11-10 17:41:30: Loss and accuracy at step 5489: 0.0367288, 0.0119\n",
      "2017-11-10 17:41:32: Loss and accuracy at step 5490: 0.0367434, 0.0129\n",
      "2017-11-10 17:41:34: Loss and accuracy at step 5491: 0.0367586, 0.0096\n",
      "2017-11-10 17:41:36: Loss and accuracy at step 5492: 0.036806, 0.0104\n",
      "2017-11-10 17:41:38: Loss and accuracy at step 5493: 0.0367925, 0.0149\n",
      "2017-11-10 17:41:41: Loss and accuracy at step 5494: 0.0367262, 0.0111\n",
      "2017-11-10 17:41:43: Loss and accuracy at step 5495: 0.0367157, 0.0108\n",
      "2017-11-10 17:41:45: Loss and accuracy at step 5496: 0.0367727, 0.0125\n",
      "2017-11-10 17:41:47: Loss and accuracy at step 5497: 0.0367277, 0.0137\n",
      "2017-11-10 17:41:49: Loss and accuracy at step 5498: 0.0367837, 0.0124\n",
      "2017-11-10 17:41:51: Loss and accuracy at step 5499: 0.0368043, 0.0161\n",
      "2017-11-10 17:41:53: Loss and accuracy at step 5500: 0.0368195, 0.0088\n",
      "2017-11-10 17:41:55: Loss and accuracy at step 5501: 0.0367577, 0.0097\n",
      "2017-11-10 17:41:57: Loss and accuracy at step 5502: 0.036724, 0.0107\n",
      "2017-11-10 17:41:59: Loss and accuracy at step 5503: 0.036802, 0.0144\n",
      "2017-11-10 17:42:01: Loss and accuracy at step 5504: 0.0367455, 0.0117\n",
      "2017-11-10 17:42:04: Loss and accuracy at step 5505: 0.0368057, 0.0137\n",
      "2017-11-10 17:42:06: Loss and accuracy at step 5506: 0.0367312, 0.0142\n",
      "2017-11-10 17:42:08: Loss and accuracy at step 5507: 0.0367479, 0.0099\n",
      "2017-11-10 17:42:10: Loss and accuracy at step 5508: 0.0368376, 0.0112\n",
      "2017-11-10 17:42:12: Loss and accuracy at step 5509: 0.036783, 0.0115\n",
      "2017-11-10 17:42:14: Loss and accuracy at step 5510: 0.0367781, 0.0086\n",
      "2017-11-10 17:42:16: Loss and accuracy at step 5511: 0.0367707, 0.0122\n",
      "2017-11-10 17:42:18: Loss and accuracy at step 5512: 0.0367359, 0.0128\n",
      "2017-11-10 17:42:20: Loss and accuracy at step 5513: 0.0367564, 0.0092\n",
      "2017-11-10 17:42:23: Loss and accuracy at step 5514: 0.0367937, 0.0114\n",
      "2017-11-10 17:42:25: Loss and accuracy at step 5515: 0.0367568, 0.0122\n",
      "2017-11-10 17:42:27: Loss and accuracy at step 5516: 0.0367022, 0.0142\n",
      "2017-11-10 17:42:29: Loss and accuracy at step 5517: 0.0368078, 0.013\n",
      "2017-11-10 17:42:31: Loss and accuracy at step 5518: 0.0367453, 0.0123\n",
      "2017-11-10 17:42:33: Loss and accuracy at step 5519: 0.0367656, 0.0113\n",
      "2017-11-10 17:42:35: Loss and accuracy at step 5520: 0.0367594, 0.0144\n",
      "2017-11-10 17:42:37: Loss and accuracy at step 5521: 0.0367636, 0.0129\n",
      "2017-11-10 17:42:39: Loss and accuracy at step 5522: 0.0368013, 0.0112\n",
      "2017-11-10 17:42:41: Loss and accuracy at step 5523: 0.0367571, 0.0109\n",
      "2017-11-10 17:42:43: Loss and accuracy at step 5524: 0.0367819, 0.0129\n",
      "2017-11-10 17:42:45: Loss and accuracy at step 5525: 0.036751, 0.0123\n",
      "2017-11-10 17:42:48: Loss and accuracy at step 5526: 0.0368381, 0.0116\n",
      "2017-11-10 17:42:50: Loss and accuracy at step 5527: 0.0367305, 0.0125\n",
      "2017-11-10 17:42:52: Loss and accuracy at step 5528: 0.0367072, 0.0166\n",
      "2017-11-10 17:42:54: Loss and accuracy at step 5529: 0.0366953, 0.0121\n",
      "2017-11-10 17:42:56: Loss and accuracy at step 5530: 0.0368311, 0.0114\n",
      "2017-11-10 17:42:58: Loss and accuracy at step 5531: 0.0367789, 0.0114\n",
      "2017-11-10 17:43:00: Loss and accuracy at step 5532: 0.0367646, 0.0136\n",
      "2017-11-10 17:43:02: Loss and accuracy at step 5533: 0.0367864, 0.0111\n",
      "2017-11-10 17:43:04: Loss and accuracy at step 5534: 0.0367815, 0.0113\n",
      "2017-11-10 17:43:06: Loss and accuracy at step 5535: 0.036774, 0.012\n",
      "2017-11-10 17:43:08: Loss and accuracy at step 5536: 0.0367663, 0.0121\n",
      "2017-11-10 17:43:10: Loss and accuracy at step 5537: 0.0367518, 0.0124\n",
      "2017-11-10 17:43:13: Loss and accuracy at step 5538: 0.0368211, 0.0145\n",
      "2017-11-10 17:43:15: Loss and accuracy at step 5539: 0.0367981, 0.0112\n",
      "2017-11-10 17:43:17: Loss and accuracy at step 5540: 0.0368478, 0.012\n",
      "2017-11-10 17:43:19: Loss and accuracy at step 5541: 0.0368032, 0.0126\n",
      "2017-11-10 17:43:21: Loss and accuracy at step 5542: 0.0367967, 0.0106\n",
      "2017-11-10 17:43:23: Loss and accuracy at step 5543: 0.0368064, 0.0124\n",
      "2017-11-10 17:43:25: Loss and accuracy at step 5544: 0.0367965, 0.0114\n",
      "2017-11-10 17:43:27: Loss and accuracy at step 5545: 0.0367898, 0.0113\n",
      "2017-11-10 17:43:29: Loss and accuracy at step 5546: 0.0368019, 0.0144\n",
      "2017-11-10 17:43:31: Loss and accuracy at step 5547: 0.036765, 0.0098\n",
      "2017-11-10 17:43:33: Loss and accuracy at step 5548: 0.0367873, 0.0143\n",
      "2017-11-10 17:43:35: Loss and accuracy at step 5549: 0.0367392, 0.0134\n",
      "2017-11-10 17:43:38: Loss and accuracy at step 5550: 0.036752, 0.0108\n",
      "2017-11-10 17:43:40: Loss and accuracy at step 5551: 0.0366932, 0.0137\n",
      "2017-11-10 17:43:42: Loss and accuracy at step 5552: 0.0367396, 0.0109\n",
      "2017-11-10 17:43:44: Loss and accuracy at step 5553: 0.0367092, 0.0135\n",
      "2017-11-10 17:43:46: Loss and accuracy at step 5554: 0.0368395, 0.0147\n",
      "2017-11-10 17:43:48: Loss and accuracy at step 5555: 0.0367964, 0.0122\n",
      "2017-11-10 17:43:50: Loss and accuracy at step 5556: 0.0367317, 0.0101\n",
      "2017-11-10 17:43:52: Loss and accuracy at step 5557: 0.0368111, 0.0154\n",
      "2017-11-10 17:43:54: Loss and accuracy at step 5558: 0.0368478, 0.0115\n",
      "2017-11-10 17:43:56: Loss and accuracy at step 5559: 0.036734, 0.012\n",
      "2017-11-10 17:43:58: Loss and accuracy at step 5560: 0.0368474, 0.0157\n",
      "2017-11-10 17:44:01: Loss and accuracy at step 5561: 0.0367687, 0.0119\n",
      "2017-11-10 17:44:03: Loss and accuracy at step 5562: 0.0368101, 0.0119\n",
      "2017-11-10 17:44:05: Loss and accuracy at step 5563: 0.0369062, 0.012\n",
      "2017-11-10 17:44:07: Loss and accuracy at step 5564: 0.0367872, 0.0116\n",
      "2017-11-10 17:44:09: Loss and accuracy at step 5565: 0.0368334, 0.0131\n",
      "2017-11-10 17:44:11: Loss and accuracy at step 5566: 0.0366901, 0.0119\n",
      "2017-11-10 17:44:13: Loss and accuracy at step 5567: 0.0367817, 0.0092\n",
      "2017-11-10 17:44:15: Loss and accuracy at step 5568: 0.0367764, 0.011\n",
      "2017-11-10 17:44:17: Loss and accuracy at step 5569: 0.0367682, 0.0156\n",
      "2017-11-10 17:44:19: Loss and accuracy at step 5570: 0.036846, 0.0123\n",
      "2017-11-10 17:44:22: Loss and accuracy at step 5571: 0.0368736, 0.0094\n",
      "2017-11-10 17:44:24: Loss and accuracy at step 5572: 0.0367965, 0.0136\n",
      "2017-11-10 17:44:26: Loss and accuracy at step 5573: 0.0368567, 0.0131\n",
      "2017-11-10 17:44:28: Loss and accuracy at step 5574: 0.0367885, 0.0097\n",
      "2017-11-10 17:44:30: Loss and accuracy at step 5575: 0.0368134, 0.0095\n",
      "2017-11-10 17:44:32: Loss and accuracy at step 5576: 0.0367813, 0.0139\n",
      "2017-11-10 17:44:34: Loss and accuracy at step 5577: 0.0368271, 0.013\n",
      "2017-11-10 17:44:36: Loss and accuracy at step 5578: 0.0367473, 0.0103\n",
      "2017-11-10 17:44:38: Loss and accuracy at step 5579: 0.0368257, 0.0148\n",
      "2017-11-10 17:44:40: Loss and accuracy at step 5580: 0.0367312, 0.0113\n",
      "2017-11-10 17:44:42: Loss and accuracy at step 5581: 0.0367116, 0.0099\n",
      "2017-11-10 17:44:44: Loss and accuracy at step 5582: 0.0366844, 0.0142\n",
      "2017-11-10 17:44:47: Loss and accuracy at step 5583: 0.0367417, 0.0149\n",
      "2017-11-10 17:44:49: Loss and accuracy at step 5584: 0.0367936, 0.0092\n",
      "2017-11-10 17:44:51: Loss and accuracy at step 5585: 0.0367268, 0.0107\n",
      "2017-11-10 17:44:53: Loss and accuracy at step 5586: 0.0367835, 0.0147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:44:55: Loss and accuracy at step 5587: 0.0367362, 0.0112\n",
      "2017-11-10 17:44:57: Loss and accuracy at step 5588: 0.0367014, 0.0138\n",
      "2017-11-10 17:44:59: Loss and accuracy at step 5589: 0.0366635, 0.0152\n",
      "2017-11-10 17:45:01: Loss and accuracy at step 5590: 0.0368019, 0.0121\n",
      "2017-11-10 17:45:03: Loss and accuracy at step 5591: 0.0367336, 0.0093\n",
      "2017-11-10 17:45:05: Loss and accuracy at step 5592: 0.0367321, 0.0131\n",
      "2017-11-10 17:45:07: Loss and accuracy at step 5593: 0.0367788, 0.0109\n",
      "2017-11-10 17:45:10: Loss and accuracy at step 5594: 0.036818, 0.0136\n",
      "2017-11-10 17:45:12: Loss and accuracy at step 5595: 0.0368104, 0.0126\n",
      "2017-11-10 17:45:14: Loss and accuracy at step 5596: 0.0368079, 0.0126\n",
      "2017-11-10 17:45:16: Loss and accuracy at step 5597: 0.0368236, 0.0145\n",
      "2017-11-10 17:45:18: Loss and accuracy at step 5598: 0.0367736, 0.0117\n",
      "2017-11-10 17:45:20: Loss and accuracy at step 5599: 0.0367702, 0.0124\n",
      "2017-11-10 17:45:22: Loss and accuracy at step 5600: 0.036758, 0.0115\n",
      "2017-11-10 17:45:24: Loss and accuracy at step 5601: 0.0368234, 0.012\n",
      "2017-11-10 17:45:26: Loss and accuracy at step 5602: 0.0367068, 0.012\n",
      "2017-11-10 17:45:28: Loss and accuracy at step 5603: 0.0368096, 0.0116\n",
      "2017-11-10 17:45:30: Loss and accuracy at step 5604: 0.0367824, 0.01\n",
      "2017-11-10 17:45:33: Loss and accuracy at step 5605: 0.0367935, 0.0158\n",
      "2017-11-10 17:45:35: Loss and accuracy at step 5606: 0.0367752, 0.011\n",
      "2017-11-10 17:45:37: Loss and accuracy at step 5607: 0.0368262, 0.0102\n",
      "2017-11-10 17:45:39: Loss and accuracy at step 5608: 0.0367607, 0.0158\n",
      "2017-11-10 17:45:41: Loss and accuracy at step 5609: 0.0368148, 0.0126\n",
      "2017-11-10 17:45:43: Loss and accuracy at step 5610: 0.0367389, 0.014\n",
      "2017-11-10 17:45:45: Loss and accuracy at step 5611: 0.0367365, 0.0117\n",
      "2017-11-10 17:45:47: Loss and accuracy at step 5612: 0.0367373, 0.0112\n",
      "2017-11-10 17:45:49: Loss and accuracy at step 5613: 0.036803, 0.0131\n",
      "2017-11-10 17:45:51: Loss and accuracy at step 5614: 0.0367698, 0.0107\n",
      "2017-11-10 17:45:53: Loss and accuracy at step 5615: 0.0367762, 0.0155\n",
      "2017-11-10 17:45:55: Loss and accuracy at step 5616: 0.0368392, 0.0128\n",
      "2017-11-10 17:45:58: Loss and accuracy at step 5617: 0.0367138, 0.0111\n",
      "2017-11-10 17:46:00: Loss and accuracy at step 5618: 0.03674, 0.0147\n",
      "2017-11-10 17:46:02: Loss and accuracy at step 5619: 0.0367353, 0.0127\n",
      "2017-11-10 17:46:04: Loss and accuracy at step 5620: 0.0367686, 0.0137\n",
      "2017-11-10 17:46:06: Loss and accuracy at step 5621: 0.0366962, 0.0129\n",
      "2017-11-10 17:46:08: Loss and accuracy at step 5622: 0.0367793, 0.0109\n",
      "2017-11-10 17:46:10: Loss and accuracy at step 5623: 0.0367907, 0.0123\n",
      "2017-11-10 17:46:12: Loss and accuracy at step 5624: 0.0366926, 0.0101\n",
      "2017-11-10 17:46:14: Loss and accuracy at step 5625: 0.0368229, 0.0174\n",
      "2017-11-10 17:46:17: Loss and accuracy at step 5626: 0.0367892, 0.011\n",
      "2017-11-10 17:46:19: Loss and accuracy at step 5627: 0.0368473, 0.0118\n",
      "2017-11-10 17:46:21: Loss and accuracy at step 5628: 0.0367828, 0.0104\n",
      "2017-11-10 17:46:23: Loss and accuracy at step 5629: 0.036761, 0.0175\n",
      "2017-11-10 17:46:25: Loss and accuracy at step 5630: 0.0367904, 0.0118\n",
      "2017-11-10 17:46:27: Loss and accuracy at step 5631: 0.036853, 0.0114\n",
      "2017-11-10 17:46:29: Loss and accuracy at step 5632: 0.0368058, 0.0121\n",
      "2017-11-10 17:46:31: Loss and accuracy at step 5633: 0.03676, 0.013\n",
      "2017-11-10 17:46:33: Loss and accuracy at step 5634: 0.0368336, 0.012\n",
      "2017-11-10 17:46:35: Loss and accuracy at step 5635: 0.0367554, 0.0132\n",
      "2017-11-10 17:46:37: Loss and accuracy at step 5636: 0.0367198, 0.0161\n",
      "2017-11-10 17:46:39: Loss and accuracy at step 5637: 0.0367073, 0.0125\n",
      "2017-11-10 17:46:41: Loss and accuracy at step 5638: 0.0368767, 0.0103\n",
      "2017-11-10 17:46:44: Loss and accuracy at step 5639: 0.0367476, 0.013\n",
      "2017-11-10 17:46:46: Loss and accuracy at step 5640: 0.036751, 0.0149\n",
      "2017-11-10 17:46:48: Loss and accuracy at step 5641: 0.0368046, 0.0124\n",
      "2017-11-10 17:46:50: Loss and accuracy at step 5642: 0.036802, 0.0109\n",
      "2017-11-10 17:46:52: Loss and accuracy at step 5643: 0.0367786, 0.0124\n",
      "2017-11-10 17:46:54: Loss and accuracy at step 5644: 0.0367665, 0.0119\n",
      "2017-11-10 17:46:56: Loss and accuracy at step 5645: 0.0367464, 0.0116\n",
      "2017-11-10 17:46:58: Loss and accuracy at step 5646: 0.036711, 0.0135\n",
      "2017-11-10 17:47:00: Loss and accuracy at step 5647: 0.036771, 0.0123\n",
      "2017-11-10 17:47:02: Loss and accuracy at step 5648: 0.036769, 0.0121\n",
      "2017-11-10 17:47:04: Loss and accuracy at step 5649: 0.036685, 0.013\n",
      "2017-11-10 17:47:06: Loss and accuracy at step 5650: 0.036796, 0.0181\n",
      "2017-11-10 17:47:09: Loss and accuracy at step 5651: 0.0367867, 0.0103\n",
      "2017-11-10 17:47:11: Loss and accuracy at step 5652: 0.0367229, 0.0105\n",
      "2017-11-10 17:47:13: Loss and accuracy at step 5653: 0.0368055, 0.0151\n",
      "2017-11-10 17:47:15: Loss and accuracy at step 5654: 0.0368327, 0.0145\n",
      "2017-11-10 17:47:17: Loss and accuracy at step 5655: 0.0368476, 0.0103\n",
      "2017-11-10 17:47:19: Loss and accuracy at step 5656: 0.0368175, 0.0114\n",
      "2017-11-10 17:47:21: Loss and accuracy at step 5657: 0.03681, 0.0154\n",
      "2017-11-10 17:47:23: Loss and accuracy at step 5658: 0.0367949, 0.0148\n",
      "2017-11-10 17:47:25: Loss and accuracy at step 5659: 0.0367406, 0.0095\n",
      "2017-11-10 17:47:27: Loss and accuracy at step 5660: 0.0367744, 0.01\n",
      "2017-11-10 17:47:29: Loss and accuracy at step 5661: 0.0368157, 0.0116\n",
      "2017-11-10 17:47:32: Loss and accuracy at step 5662: 0.0367537, 0.0129\n",
      "2017-11-10 17:47:34: Loss and accuracy at step 5663: 0.0367577, 0.014\n",
      "2017-11-10 17:47:36: Loss and accuracy at step 5664: 0.0368416, 0.0108\n",
      "2017-11-10 17:47:38: Loss and accuracy at step 5665: 0.0367888, 0.0114\n",
      "2017-11-10 17:47:40: Loss and accuracy at step 5666: 0.0367337, 0.0144\n",
      "2017-11-10 17:47:42: Loss and accuracy at step 5667: 0.036803, 0.0107\n",
      "2017-11-10 17:47:44: Loss and accuracy at step 5668: 0.0367032, 0.0107\n",
      "2017-11-10 17:47:46: Loss and accuracy at step 5669: 0.0367868, 0.012\n",
      "2017-11-10 17:47:48: Loss and accuracy at step 5670: 0.0368079, 0.0112\n",
      "2017-11-10 17:47:50: Loss and accuracy at step 5671: 0.0367077, 0.0099\n",
      "2017-11-10 17:47:52: Loss and accuracy at step 5672: 0.0367138, 0.0101\n",
      "2017-11-10 17:47:55: Loss and accuracy at step 5673: 0.0367843, 0.014\n",
      "2017-11-10 17:47:57: Loss and accuracy at step 5674: 0.0369023, 0.0119\n",
      "2017-11-10 17:47:59: Loss and accuracy at step 5675: 0.0367564, 0.0096\n",
      "2017-11-10 17:48:01: Loss and accuracy at step 5676: 0.0367521, 0.013\n",
      "2017-11-10 17:48:03: Loss and accuracy at step 5677: 0.0367966, 0.0133\n",
      "2017-11-10 17:48:05: Loss and accuracy at step 5678: 0.0366743, 0.0121\n",
      "2017-11-10 17:48:07: Loss and accuracy at step 5679: 0.0367546, 0.0144\n",
      "2017-11-10 17:48:09: Loss and accuracy at step 5680: 0.0367534, 0.0102\n",
      "2017-11-10 17:48:11: Loss and accuracy at step 5681: 0.0367553, 0.0148\n",
      "2017-11-10 17:48:13: Loss and accuracy at step 5682: 0.0368131, 0.0153\n",
      "2017-11-10 17:48:16: Loss and accuracy at step 5683: 0.0367807, 0.0114\n",
      "2017-11-10 17:48:18: Loss and accuracy at step 5684: 0.0367415, 0.0111\n",
      "2017-11-10 17:48:20: Loss and accuracy at step 5685: 0.0367795, 0.0144\n",
      "2017-11-10 17:48:22: Loss and accuracy at step 5686: 0.0367724, 0.0179\n",
      "2017-11-10 17:48:24: Loss and accuracy at step 5687: 0.0368285, 0.0109\n",
      "2017-11-10 17:48:26: Loss and accuracy at step 5688: 0.0368252, 0.0118\n",
      "2017-11-10 17:48:28: Loss and accuracy at step 5689: 0.0367204, 0.0161\n",
      "2017-11-10 17:48:30: Loss and accuracy at step 5690: 0.0367006, 0.0103\n",
      "2017-11-10 17:48:32: Loss and accuracy at step 5691: 0.0367281, 0.0133\n",
      "2017-11-10 17:48:34: Loss and accuracy at step 5692: 0.0367594, 0.0126\n",
      "2017-11-10 17:48:36: Loss and accuracy at step 5693: 0.036826, 0.0122\n",
      "2017-11-10 17:48:38: Loss and accuracy at step 5694: 0.0368101, 0.0119\n",
      "2017-11-10 17:48:41: Loss and accuracy at step 5695: 0.0368448, 0.0122\n",
      "2017-11-10 17:48:43: Loss and accuracy at step 5696: 0.0368059, 0.0131\n",
      "2017-11-10 17:48:45: Loss and accuracy at step 5697: 0.0367803, 0.0116\n",
      "2017-11-10 17:48:47: Loss and accuracy at step 5698: 0.0367623, 0.0141\n",
      "2017-11-10 17:48:49: Loss and accuracy at step 5699: 0.0367686, 0.0137\n",
      "2017-11-10 17:48:51: Loss and accuracy at step 5700: 0.0367446, 0.0118\n",
      "2017-11-10 17:48:53: Loss and accuracy at step 5701: 0.0367073, 0.0112\n",
      "2017-11-10 17:48:55: Loss and accuracy at step 5702: 0.0367778, 0.0132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:48:57: Loss and accuracy at step 5703: 0.0367563, 0.0096\n",
      "2017-11-10 17:48:59: Loss and accuracy at step 5704: 0.0367184, 0.0126\n",
      "2017-11-10 17:49:01: Loss and accuracy at step 5705: 0.0367711, 0.0139\n",
      "2017-11-10 17:49:03: Loss and accuracy at step 5706: 0.036754, 0.0094\n",
      "2017-11-10 17:49:06: Loss and accuracy at step 5707: 0.0367679, 0.0136\n",
      "2017-11-10 17:49:08: Loss and accuracy at step 5708: 0.0368216, 0.013\n",
      "2017-11-10 17:49:10: Loss and accuracy at step 5709: 0.0367385, 0.0125\n",
      "2017-11-10 17:49:12: Loss and accuracy at step 5710: 0.0367191, 0.0153\n",
      "2017-11-10 17:49:14: Loss and accuracy at step 5711: 0.0367954, 0.012\n",
      "2017-11-10 17:49:16: Loss and accuracy at step 5712: 0.0367027, 0.011\n",
      "2017-11-10 17:49:18: Loss and accuracy at step 5713: 0.0368173, 0.012\n",
      "2017-11-10 17:49:20: Loss and accuracy at step 5714: 0.0367919, 0.0097\n",
      "2017-11-10 17:49:22: Loss and accuracy at step 5715: 0.0367675, 0.0128\n",
      "2017-11-10 17:49:24: Loss and accuracy at step 5716: 0.036772, 0.0151\n",
      "2017-11-10 17:49:26: Loss and accuracy at step 5717: 0.0368802, 0.0139\n",
      "2017-11-10 17:49:28: Loss and accuracy at step 5718: 0.0367643, 0.0093\n",
      "2017-11-10 17:49:31: Loss and accuracy at step 5719: 0.0367635, 0.0129\n",
      "2017-11-10 17:49:33: Loss and accuracy at step 5720: 0.0367835, 0.0121\n",
      "2017-11-10 17:49:35: Loss and accuracy at step 5721: 0.0368473, 0.0106\n",
      "2017-11-10 17:49:37: Loss and accuracy at step 5722: 0.0368612, 0.0124\n",
      "2017-11-10 17:49:39: Loss and accuracy at step 5723: 0.0368586, 0.0088\n",
      "2017-11-10 17:49:41: Loss and accuracy at step 5724: 0.0367778, 0.0138\n",
      "2017-11-10 17:49:43: Loss and accuracy at step 5725: 0.0367517, 0.0141\n",
      "2017-11-10 17:49:45: Loss and accuracy at step 5726: 0.0368442, 0.0094\n",
      "2017-11-10 17:49:47: Loss and accuracy at step 5727: 0.0368118, 0.0176\n",
      "2017-11-10 17:49:49: Loss and accuracy at step 5728: 0.0368533, 0.0117\n",
      "2017-11-10 17:49:51: Loss and accuracy at step 5729: 0.0366404, 0.0077\n",
      "2017-11-10 17:49:54: Loss and accuracy at step 5730: 0.0367766, 0.0169\n",
      "2017-11-10 17:49:56: Loss and accuracy at step 5731: 0.0367237, 0.0103\n",
      "2017-11-10 17:49:58: Loss and accuracy at step 5732: 0.0368012, 0.0114\n",
      "2017-11-10 17:50:00: Loss and accuracy at step 5733: 0.0368623, 0.012\n",
      "2017-11-10 17:50:02: Loss and accuracy at step 5734: 0.0368083, 0.0099\n",
      "2017-11-10 17:50:04: Loss and accuracy at step 5735: 0.036755, 0.0144\n",
      "2017-11-10 17:50:06: Loss and accuracy at step 5736: 0.0366982, 0.0161\n",
      "2017-11-10 17:50:08: Loss and accuracy at step 5737: 0.0367306, 0.0099\n",
      "2017-11-10 17:50:10: Loss and accuracy at step 5738: 0.0368305, 0.0154\n",
      "2017-11-10 17:50:12: Loss and accuracy at step 5739: 0.0367768, 0.0191\n",
      "2017-11-10 17:50:15: Loss and accuracy at step 5740: 0.0367344, 0.0093\n",
      "2017-11-10 17:50:17: Loss and accuracy at step 5741: 0.0368183, 0.0129\n",
      "2017-11-10 17:50:19: Loss and accuracy at step 5742: 0.0368695, 0.0136\n",
      "2017-11-10 17:50:21: Loss and accuracy at step 5743: 0.0369023, 0.0108\n",
      "2017-11-10 17:50:23: Loss and accuracy at step 5744: 0.0367211, 0.0129\n",
      "2017-11-10 17:50:25: Loss and accuracy at step 5745: 0.036818, 0.0113\n",
      "2017-11-10 17:50:27: Loss and accuracy at step 5746: 0.0367334, 0.0133\n",
      "2017-11-10 17:50:29: Loss and accuracy at step 5747: 0.0367522, 0.0152\n",
      "2017-11-10 17:50:31: Loss and accuracy at step 5748: 0.0367215, 0.0101\n",
      "2017-11-10 17:50:33: Loss and accuracy at step 5749: 0.036777, 0.0148\n",
      "2017-11-10 17:50:35: Loss and accuracy at step 5750: 0.0367317, 0.0142\n",
      "2017-11-10 17:50:37: Loss and accuracy at step 5751: 0.0367088, 0.0124\n",
      "2017-11-10 17:50:40: Loss and accuracy at step 5752: 0.0368568, 0.0108\n",
      "2017-11-10 17:50:42: Loss and accuracy at step 5753: 0.0368197, 0.012\n",
      "2017-11-10 17:50:44: Loss and accuracy at step 5754: 0.0367602, 0.0183\n",
      "2017-11-10 17:50:46: Loss and accuracy at step 5755: 0.0368157, 0.0086\n",
      "2017-11-10 17:50:48: Loss and accuracy at step 5756: 0.0368188, 0.0119\n",
      "2017-11-10 17:50:50: Loss and accuracy at step 5757: 0.0367622, 0.0145\n",
      "2017-11-10 17:50:52: Loss and accuracy at step 5758: 0.0368003, 0.0134\n",
      "2017-11-10 17:50:54: Loss and accuracy at step 5759: 0.0366915, 0.0112\n",
      "2017-11-10 17:50:56: Loss and accuracy at step 5760: 0.0368332, 0.0104\n",
      "2017-11-10 17:50:58: Loss and accuracy at step 5761: 0.0367908, 0.0111\n",
      "2017-11-10 17:51:00: Loss and accuracy at step 5762: 0.036785, 0.0125\n",
      "2017-11-10 17:51:02: Loss and accuracy at step 5763: 0.0367769, 0.0094\n",
      "2017-11-10 17:51:05: Loss and accuracy at step 5764: 0.0367355, 0.0112\n",
      "2017-11-10 17:51:07: Loss and accuracy at step 5765: 0.0367734, 0.0133\n",
      "2017-11-10 17:51:09: Loss and accuracy at step 5766: 0.0368459, 0.0134\n",
      "2017-11-10 17:51:11: Loss and accuracy at step 5767: 0.0368208, 0.0095\n",
      "2017-11-10 17:51:13: Loss and accuracy at step 5768: 0.0367491, 0.0182\n",
      "2017-11-10 17:51:15: Loss and accuracy at step 5769: 0.0367998, 0.0116\n",
      "2017-11-10 17:51:17: Loss and accuracy at step 5770: 0.0367738, 0.0132\n",
      "2017-11-10 17:51:19: Loss and accuracy at step 5771: 0.0368145, 0.0132\n",
      "2017-11-10 17:51:21: Loss and accuracy at step 5772: 0.0367582, 0.0136\n",
      "2017-11-10 17:51:23: Loss and accuracy at step 5773: 0.0368221, 0.0157\n",
      "2017-11-10 17:51:25: Loss and accuracy at step 5774: 0.0367891, 0.0155\n",
      "2017-11-10 17:51:27: Loss and accuracy at step 5775: 0.0367733, 0.011\n",
      "2017-11-10 17:51:30: Loss and accuracy at step 5776: 0.0367096, 0.0154\n",
      "2017-11-10 17:51:32: Loss and accuracy at step 5777: 0.0368461, 0.0125\n",
      "2017-11-10 17:51:34: Loss and accuracy at step 5778: 0.0367752, 0.0119\n",
      "2017-11-10 17:51:36: Loss and accuracy at step 5779: 0.0367332, 0.0141\n",
      "2017-11-10 17:51:38: Loss and accuracy at step 5780: 0.0367746, 0.0101\n",
      "2017-11-10 17:51:40: Loss and accuracy at step 5781: 0.0368685, 0.0098\n",
      "2017-11-10 17:51:42: Loss and accuracy at step 5782: 0.0368669, 0.016\n",
      "2017-11-10 17:51:44: Loss and accuracy at step 5783: 0.0369041, 0.0089\n",
      "2017-11-10 17:51:46: Loss and accuracy at step 5784: 0.0367977, 0.013\n",
      "2017-11-10 17:51:48: Loss and accuracy at step 5785: 0.0368268, 0.0193\n",
      "2017-11-10 17:51:50: Loss and accuracy at step 5786: 0.0367251, 0.0103\n",
      "2017-11-10 17:51:52: Loss and accuracy at step 5787: 0.0367887, 0.0107\n",
      "2017-11-10 17:51:55: Loss and accuracy at step 5788: 0.0367256, 0.014\n",
      "2017-11-10 17:51:57: Loss and accuracy at step 5789: 0.0367392, 0.0161\n",
      "2017-11-10 17:51:59: Loss and accuracy at step 5790: 0.0368103, 0.0076\n",
      "2017-11-10 17:52:01: Loss and accuracy at step 5791: 0.0368685, 0.0126\n",
      "2017-11-10 17:52:03: Loss and accuracy at step 5792: 0.0368511, 0.0145\n",
      "2017-11-10 17:52:05: Loss and accuracy at step 5793: 0.036794, 0.0155\n",
      "2017-11-10 17:52:07: Loss and accuracy at step 5794: 0.0368936, 0.0096\n",
      "2017-11-10 17:52:09: Loss and accuracy at step 5795: 0.0368487, 0.0118\n",
      "2017-11-10 17:52:11: Loss and accuracy at step 5796: 0.0367186, 0.0186\n",
      "2017-11-10 17:52:13: Loss and accuracy at step 5797: 0.0367625, 0.0094\n",
      "2017-11-10 17:52:16: Loss and accuracy at step 5798: 0.0367333, 0.0119\n",
      "2017-11-10 17:52:18: Loss and accuracy at step 5799: 0.0368152, 0.012\n",
      "2017-11-10 17:52:20: Loss and accuracy at step 5800: 0.0367925, 0.0121\n",
      "2017-11-10 17:52:22: Loss and accuracy at step 5801: 0.036778, 0.0113\n",
      "2017-11-10 17:52:24: Loss and accuracy at step 5802: 0.0367397, 0.0124\n",
      "2017-11-10 17:52:26: Loss and accuracy at step 5803: 0.0368453, 0.0125\n",
      "2017-11-10 17:52:28: Loss and accuracy at step 5804: 0.0368343, 0.0146\n",
      "2017-11-10 17:52:30: Loss and accuracy at step 5805: 0.0367993, 0.0106\n",
      "2017-11-10 17:52:32: Loss and accuracy at step 5806: 0.0367527, 0.0115\n",
      "2017-11-10 17:52:34: Loss and accuracy at step 5807: 0.036755, 0.0124\n",
      "2017-11-10 17:52:36: Loss and accuracy at step 5808: 0.0367651, 0.0124\n",
      "2017-11-10 17:52:38: Loss and accuracy at step 5809: 0.0367537, 0.0117\n",
      "2017-11-10 17:52:41: Loss and accuracy at step 5810: 0.0367266, 0.0113\n",
      "2017-11-10 17:52:43: Loss and accuracy at step 5811: 0.0368197, 0.0118\n",
      "2017-11-10 17:52:45: Loss and accuracy at step 5812: 0.0367072, 0.0113\n",
      "2017-11-10 17:52:47: Loss and accuracy at step 5813: 0.0368584, 0.0136\n",
      "2017-11-10 17:52:49: Loss and accuracy at step 5814: 0.0368267, 0.011\n",
      "2017-11-10 17:52:51: Loss and accuracy at step 5815: 0.0367875, 0.0124\n",
      "2017-11-10 17:52:53: Loss and accuracy at step 5816: 0.0367498, 0.0131\n",
      "2017-11-10 17:52:55: Loss and accuracy at step 5817: 0.0367617, 0.0116\n",
      "2017-11-10 17:52:57: Loss and accuracy at step 5818: 0.0367997, 0.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:52:59: Loss and accuracy at step 5819: 0.0367492, 0.0173\n",
      "2017-11-10 17:53:01: Loss and accuracy at step 5820: 0.0366778, 0.0131\n",
      "2017-11-10 17:53:04: Loss and accuracy at step 5821: 0.0367475, 0.0089\n",
      "2017-11-10 17:53:06: Loss and accuracy at step 5822: 0.0367959, 0.0181\n",
      "2017-11-10 17:53:08: Loss and accuracy at step 5823: 0.0368139, 0.0116\n",
      "2017-11-10 17:53:10: Loss and accuracy at step 5824: 0.036877, 0.0078\n",
      "2017-11-10 17:53:12: Loss and accuracy at step 5825: 0.0367755, 0.0184\n",
      "2017-11-10 17:53:14: Loss and accuracy at step 5826: 0.0367189, 0.01\n",
      "2017-11-10 17:53:16: Loss and accuracy at step 5827: 0.0367944, 0.0126\n",
      "2017-11-10 17:53:18: Loss and accuracy at step 5828: 0.0367704, 0.0156\n",
      "2017-11-10 17:53:20: Loss and accuracy at step 5829: 0.0367949, 0.0085\n",
      "2017-11-10 17:53:22: Loss and accuracy at step 5830: 0.0367265, 0.012\n",
      "2017-11-10 17:53:24: Loss and accuracy at step 5831: 0.0368189, 0.0168\n",
      "2017-11-10 17:53:26: Loss and accuracy at step 5832: 0.0368141, 0.0089\n",
      "2017-11-10 17:53:29: Loss and accuracy at step 5833: 0.0367546, 0.0124\n",
      "2017-11-10 17:53:31: Loss and accuracy at step 5834: 0.0368365, 0.0129\n",
      "2017-11-10 17:53:33: Loss and accuracy at step 5835: 0.036806, 0.0141\n",
      "2017-11-10 17:53:35: Loss and accuracy at step 5836: 0.036817, 0.0116\n",
      "2017-11-10 17:53:37: Loss and accuracy at step 5837: 0.0367621, 0.009\n",
      "2017-11-10 17:53:39: Loss and accuracy at step 5838: 0.0367972, 0.0141\n",
      "2017-11-10 17:53:41: Loss and accuracy at step 5839: 0.0368109, 0.0092\n",
      "2017-11-10 17:53:43: Loss and accuracy at step 5840: 0.0368212, 0.0136\n",
      "2017-11-10 17:53:45: Loss and accuracy at step 5841: 0.0368717, 0.0114\n",
      "2017-11-10 17:53:47: Loss and accuracy at step 5842: 0.0368092, 0.0119\n",
      "2017-11-10 17:53:49: Loss and accuracy at step 5843: 0.0368239, 0.0138\n",
      "2017-11-10 17:53:51: Loss and accuracy at step 5844: 0.0368033, 0.0139\n",
      "2017-11-10 17:53:54: Loss and accuracy at step 5845: 0.0368001, 0.0091\n",
      "2017-11-10 17:53:56: Loss and accuracy at step 5846: 0.0367646, 0.0146\n",
      "2017-11-10 17:53:58: Loss and accuracy at step 5847: 0.0367849, 0.0114\n",
      "2017-11-10 17:54:00: Loss and accuracy at step 5848: 0.0367863, 0.0092\n",
      "2017-11-10 17:54:02: Loss and accuracy at step 5849: 0.0367336, 0.0157\n",
      "2017-11-10 17:54:04: Loss and accuracy at step 5850: 0.0368258, 0.0143\n",
      "2017-11-10 17:54:06: Loss and accuracy at step 5851: 0.0368115, 0.0106\n",
      "2017-11-10 17:54:08: Loss and accuracy at step 5852: 0.0367724, 0.0128\n",
      "2017-11-10 17:54:10: Loss and accuracy at step 5853: 0.036811, 0.0192\n",
      "2017-11-10 17:54:12: Loss and accuracy at step 5854: 0.0367756, 0.0102\n",
      "2017-11-10 17:54:15: Loss and accuracy at step 5855: 0.0367922, 0.0089\n",
      "2017-11-10 17:54:17: Loss and accuracy at step 5856: 0.0367413, 0.022\n",
      "2017-11-10 17:54:19: Loss and accuracy at step 5857: 0.0367282, 0.0103\n",
      "2017-11-10 17:54:21: Loss and accuracy at step 5858: 0.0367218, 0.0152\n",
      "2017-11-10 17:54:23: Loss and accuracy at step 5859: 0.0367443, 0.0147\n",
      "2017-11-10 17:54:25: Loss and accuracy at step 5860: 0.0367622, 0.0083\n",
      "2017-11-10 17:54:27: Loss and accuracy at step 5861: 0.0368025, 0.0163\n",
      "2017-11-10 17:54:29: Loss and accuracy at step 5862: 0.0367714, 0.0094\n",
      "2017-11-10 17:54:31: Loss and accuracy at step 5863: 0.0367233, 0.0184\n",
      "2017-11-10 17:54:33: Loss and accuracy at step 5864: 0.0367216, 0.0112\n",
      "2017-11-10 17:54:35: Loss and accuracy at step 5865: 0.0366944, 0.0087\n",
      "2017-11-10 17:54:37: Loss and accuracy at step 5866: 0.0367118, 0.02\n",
      "2017-11-10 17:54:40: Loss and accuracy at step 5867: 0.036769, 0.0095\n",
      "2017-11-10 17:54:42: Loss and accuracy at step 5868: 0.0367842, 0.0084\n",
      "2017-11-10 17:54:44: Loss and accuracy at step 5869: 0.0367489, 0.0168\n",
      "2017-11-10 17:54:46: Loss and accuracy at step 5870: 0.0368284, 0.0106\n",
      "2017-11-10 17:54:48: Loss and accuracy at step 5871: 0.0367461, 0.0098\n",
      "2017-11-10 17:54:50: Loss and accuracy at step 5872: 0.0367048, 0.0112\n",
      "2017-11-10 17:54:52: Loss and accuracy at step 5873: 0.0368416, 0.0125\n",
      "2017-11-10 17:54:54: Loss and accuracy at step 5874: 0.0367246, 0.0145\n",
      "2017-11-10 17:54:56: Loss and accuracy at step 5875: 0.0367709, 0.0128\n",
      "2017-11-10 17:54:58: Loss and accuracy at step 5876: 0.036698, 0.0097\n",
      "2017-11-10 17:55:00: Loss and accuracy at step 5877: 0.0367181, 0.0158\n",
      "2017-11-10 17:55:02: Loss and accuracy at step 5878: 0.0367177, 0.0124\n",
      "2017-11-10 17:55:05: Loss and accuracy at step 5879: 0.0367997, 0.0119\n",
      "2017-11-10 17:55:07: Loss and accuracy at step 5880: 0.0366489, 0.0131\n",
      "2017-11-10 17:55:09: Loss and accuracy at step 5881: 0.0367293, 0.0143\n",
      "2017-11-10 17:55:11: Loss and accuracy at step 5882: 0.0367796, 0.0103\n",
      "2017-11-10 17:55:13: Loss and accuracy at step 5883: 0.0367535, 0.0084\n",
      "2017-11-10 17:55:15: Loss and accuracy at step 5884: 0.0367349, 0.0199\n",
      "2017-11-10 17:55:17: Loss and accuracy at step 5885: 0.0367146, 0.0106\n",
      "2017-11-10 17:55:19: Loss and accuracy at step 5886: 0.0366751, 0.0114\n",
      "2017-11-10 17:55:21: Loss and accuracy at step 5887: 0.0367463, 0.0201\n",
      "2017-11-10 17:55:23: Loss and accuracy at step 5888: 0.0366486, 0.0082\n",
      "2017-11-10 17:55:26: Loss and accuracy at step 5889: 0.0367745, 0.0122\n",
      "2017-11-10 17:55:28: Loss and accuracy at step 5890: 0.0367768, 0.0194\n",
      "2017-11-10 17:55:30: Loss and accuracy at step 5891: 0.0367917, 0.0095\n",
      "2017-11-10 17:55:32: Loss and accuracy at step 5892: 0.0368574, 0.0135\n",
      "2017-11-10 17:55:34: Loss and accuracy at step 5893: 0.036816, 0.013\n",
      "2017-11-10 17:55:36: Loss and accuracy at step 5894: 0.0367838, 0.0094\n",
      "2017-11-10 17:55:38: Loss and accuracy at step 5895: 0.0367928, 0.0166\n",
      "2017-11-10 17:55:40: Loss and accuracy at step 5896: 0.0368022, 0.0113\n",
      "2017-11-10 17:55:42: Loss and accuracy at step 5897: 0.0367992, 0.011\n",
      "2017-11-10 17:55:44: Loss and accuracy at step 5898: 0.0367407, 0.0214\n",
      "2017-11-10 17:55:46: Loss and accuracy at step 5899: 0.0367805, 0.0099\n",
      "2017-11-10 17:55:48: Loss and accuracy at step 5900: 0.0368518, 0.0128\n",
      "2017-11-10 17:55:51: Loss and accuracy at step 5901: 0.0368394, 0.0201\n",
      "2017-11-10 17:55:53: Loss and accuracy at step 5902: 0.0368648, 0.0071\n",
      "2017-11-10 17:55:55: Loss and accuracy at step 5903: 0.0368886, 0.0158\n",
      "2017-11-10 17:55:57: Loss and accuracy at step 5904: 0.0366937, 0.0093\n",
      "2017-11-10 17:55:59: Loss and accuracy at step 5905: 0.0367055, 0.0122\n",
      "2017-11-10 17:56:01: Loss and accuracy at step 5906: 0.0367231, 0.0135\n",
      "2017-11-10 17:56:03: Loss and accuracy at step 5907: 0.0367612, 0.0118\n",
      "2017-11-10 17:56:05: Loss and accuracy at step 5908: 0.0367545, 0.0164\n",
      "2017-11-10 17:56:07: Loss and accuracy at step 5909: 0.0367675, 0.0076\n",
      "2017-11-10 17:56:09: Loss and accuracy at step 5910: 0.0367685, 0.0174\n",
      "2017-11-10 17:56:11: Loss and accuracy at step 5911: 0.0368885, 0.0181\n",
      "2017-11-10 17:56:14: Loss and accuracy at step 5912: 0.0368917, 0.0062\n",
      "2017-11-10 17:56:16: Loss and accuracy at step 5913: 0.0367705, 0.0194\n",
      "2017-11-10 17:56:18: Loss and accuracy at step 5914: 0.0368266, 0.0115\n",
      "2017-11-10 17:56:20: Loss and accuracy at step 5915: 0.0367788, 0.0092\n",
      "2017-11-10 17:56:22: Loss and accuracy at step 5916: 0.0367677, 0.0205\n",
      "2017-11-10 17:56:24: Loss and accuracy at step 5917: 0.0367643, 0.0091\n",
      "2017-11-10 17:56:26: Loss and accuracy at step 5918: 0.0367816, 0.0136\n",
      "2017-11-10 17:56:28: Loss and accuracy at step 5919: 0.0367466, 0.0226\n",
      "2017-11-10 17:56:30: Loss and accuracy at step 5920: 0.0368036, 0.0108\n",
      "2017-11-10 17:56:32: Loss and accuracy at step 5921: 0.036793, 0.0143\n",
      "2017-11-10 17:56:34: Loss and accuracy at step 5922: 0.036816, 0.0255\n",
      "2017-11-10 17:56:37: Loss and accuracy at step 5923: 0.0368516, 0.0152\n",
      "2017-11-10 17:56:39: Loss and accuracy at step 5924: 0.0367416, 0.019\n",
      "2017-11-10 17:56:41: Loss and accuracy at step 5925: 0.0367255, 0.0185\n",
      "2017-11-10 17:56:43: Loss and accuracy at step 5926: 0.0367741, 0.0201\n",
      "2017-11-10 17:56:45: Loss and accuracy at step 5927: 0.0368614, 0.019\n",
      "2017-11-10 17:56:47: Loss and accuracy at step 5928: 0.0368379, 0.0116\n",
      "2017-11-10 17:56:49: Loss and accuracy at step 5929: 0.0368706, 0.0263\n",
      "2017-11-10 17:56:51: Loss and accuracy at step 5930: 0.0368279, 0.0128\n",
      "2017-11-10 17:56:53: Loss and accuracy at step 5931: 0.0368425, 0.0121\n",
      "2017-11-10 17:56:55: Loss and accuracy at step 5932: 0.0368101, 0.0281\n",
      "2017-11-10 17:56:57: Loss and accuracy at step 5933: 0.03691, 0.0132\n",
      "2017-11-10 17:56:59: Loss and accuracy at step 5934: 0.0368551, 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 17:57:02: Loss and accuracy at step 5935: 0.0367832, 0.0172\n",
      "2017-11-10 17:57:04: Loss and accuracy at step 5936: 0.0368011, 0.0164\n",
      "2017-11-10 17:57:06: Loss and accuracy at step 5937: 0.0367821, 0.0082\n",
      "2017-11-10 17:57:08: Loss and accuracy at step 5938: 0.0367659, 0.0171\n",
      "2017-11-10 17:57:10: Loss and accuracy at step 5939: 0.0367813, 0.0223\n",
      "2017-11-10 17:57:12: Loss and accuracy at step 5940: 0.0367695, 0.0094\n",
      "2017-11-10 17:57:14: Loss and accuracy at step 5941: 0.0367307, 0.0124\n",
      "2017-11-10 17:57:16: Loss and accuracy at step 5942: 0.0368502, 0.0184\n",
      "2017-11-10 17:57:18: Loss and accuracy at step 5943: 0.0368168, 0.0124\n",
      "2017-11-10 17:57:20: Loss and accuracy at step 5944: 0.0367392, 0.012\n",
      "2017-11-10 17:57:22: Loss and accuracy at step 5945: 0.0368494, 0.015\n",
      "2017-11-10 17:57:24: Loss and accuracy at step 5946: 0.0368092, 0.0156\n",
      "2017-11-10 17:57:27: Loss and accuracy at step 5947: 0.036868, 0.0128\n",
      "2017-11-10 17:57:29: Loss and accuracy at step 5948: 0.0367436, 0.0134\n",
      "2017-11-10 17:57:31: Loss and accuracy at step 5949: 0.0368488, 0.0204\n",
      "2017-11-10 17:57:33: Loss and accuracy at step 5950: 0.0367288, 0.0093\n",
      "2017-11-10 17:57:35: Loss and accuracy at step 5951: 0.0368353, 0.0156\n",
      "2017-11-10 17:57:37: Loss and accuracy at step 5952: 0.03678, 0.0309\n",
      "2017-11-10 17:57:39: Loss and accuracy at step 5953: 0.0368687, 0.0086\n",
      "2017-11-10 17:57:41: Loss and accuracy at step 5954: 0.0367817, 0.0172\n",
      "2017-11-10 17:57:43: Loss and accuracy at step 5955: 0.0367891, 0.0384\n",
      "2017-11-10 17:57:45: Loss and accuracy at step 5956: 0.036797, 0.009\n",
      "2017-11-10 17:57:47: Loss and accuracy at step 5957: 0.0367502, 0.0168\n",
      "2017-11-10 17:57:50: Loss and accuracy at step 5958: 0.0368124, 0.0525\n",
      "2017-11-10 17:57:52: Loss and accuracy at step 5959: 0.0368006, 0.0087\n",
      "2017-11-10 17:57:54: Loss and accuracy at step 5960: 0.0368823, 0.0165\n",
      "2017-11-10 17:57:56: Loss and accuracy at step 5961: 0.036839, 0.0481\n",
      "2017-11-10 17:57:58: Loss and accuracy at step 5962: 0.0367854, 0.0125\n",
      "2017-11-10 17:58:00: Loss and accuracy at step 5963: 0.0367228, 0.0123\n",
      "2017-11-10 17:58:02: Loss and accuracy at step 5964: 0.036656, 0.0673\n",
      "2017-11-10 17:58:04: Loss and accuracy at step 5965: 0.036846, 0.0122\n",
      "2017-11-10 17:58:06: Loss and accuracy at step 5966: 0.0367806, 0.0105\n",
      "2017-11-10 17:58:08: Loss and accuracy at step 5967: 0.0367647, 0.0374\n",
      "2017-11-10 17:58:10: Loss and accuracy at step 5968: 0.0368047, 0.0088\n",
      "2017-11-10 17:58:13: Loss and accuracy at step 5969: 0.036757, 0.0126\n",
      "2017-11-10 17:58:15: Loss and accuracy at step 5970: 0.0368177, 0.0263\n",
      "2017-11-10 17:58:17: Loss and accuracy at step 5971: 0.0368483, 0.0092\n",
      "2017-11-10 17:58:19: Loss and accuracy at step 5972: 0.0367166, 0.0157\n",
      "2017-11-10 17:58:21: Loss and accuracy at step 5973: 0.0367903, 0.0157\n",
      "2017-11-10 17:58:23: Loss and accuracy at step 5974: 0.0368073, 0.0082\n",
      "2017-11-10 17:58:25: Loss and accuracy at step 5975: 0.0367557, 0.0168\n",
      "2017-11-10 17:58:27: Loss and accuracy at step 5976: 0.0367779, 0.0147\n",
      "2017-11-10 17:58:29: Loss and accuracy at step 5977: 0.036848, 0.0116\n",
      "2017-11-10 17:58:31: Loss and accuracy at step 5978: 0.0367953, 0.0134\n",
      "2017-11-10 17:58:33: Loss and accuracy at step 5979: 0.0368808, 0.0154\n",
      "2017-11-10 17:58:36: Loss and accuracy at step 5980: 0.0367322, 0.0099\n",
      "2017-11-10 17:58:38: Loss and accuracy at step 5981: 0.0368185, 0.0109\n",
      "2017-11-10 17:58:40: Loss and accuracy at step 5982: 0.0367687, 0.0227\n",
      "2017-11-10 17:58:42: Loss and accuracy at step 5983: 0.0367632, 0.0094\n",
      "2017-11-10 17:58:44: Loss and accuracy at step 5984: 0.0366974, 0.0184\n",
      "2017-11-10 17:58:46: Loss and accuracy at step 5985: 0.0367487, 0.0154\n",
      "2017-11-10 17:58:48: Loss and accuracy at step 5986: 0.0367745, 0.0076\n",
      "2017-11-10 17:58:50: Loss and accuracy at step 5987: 0.0366559, 0.0144\n",
      "2017-11-10 17:58:52: Loss and accuracy at step 5988: 0.0367696, 0.0208\n",
      "2017-11-10 17:58:54: Loss and accuracy at step 5989: 0.0368113, 0.0072\n",
      "2017-11-10 17:58:56: Loss and accuracy at step 5990: 0.0368085, 0.0144\n",
      "2017-11-10 17:58:59: Loss and accuracy at step 5991: 0.0367721, 0.0239\n",
      "2017-11-10 17:59:01: Loss and accuracy at step 5992: 0.0367604, 0.0083\n",
      "2017-11-10 17:59:03: Loss and accuracy at step 5993: 0.0367705, 0.0156\n",
      "2017-11-10 17:59:05: Loss and accuracy at step 5994: 0.0367098, 0.02\n",
      "2017-11-10 17:59:07: Loss and accuracy at step 5995: 0.0368264, 0.0101\n",
      "2017-11-10 17:59:09: Loss and accuracy at step 5996: 0.036771, 0.0102\n",
      "2017-11-10 17:59:11: Loss and accuracy at step 5997: 0.0367627, 0.0206\n",
      "2017-11-10 17:59:13: Loss and accuracy at step 5998: 0.0367933, 0.0103\n",
      "2017-11-10 17:59:15: Loss and accuracy at step 5999: 0.0366656, 0.0094\n",
      "2017-11-10 17:59:17: Loss and accuracy at step 6000: 0.036794, 0.0134\n",
      "2017-11-10 17:59:19: Loss and accuracy at step 6001: 0.036758, 0.0133\n",
      "2017-11-10 17:59:21: Loss and accuracy at step 6002: 0.0367208, 0.011\n",
      "2017-11-10 17:59:24: Loss and accuracy at step 6003: 0.0367304, 0.0116\n",
      "2017-11-10 17:59:26: Loss and accuracy at step 6004: 0.0367705, 0.0139\n",
      "2017-11-10 17:59:28: Loss and accuracy at step 6005: 0.0367814, 0.0147\n",
      "2017-11-10 17:59:30: Loss and accuracy at step 6006: 0.036789, 0.0087\n",
      "2017-11-10 17:59:32: Loss and accuracy at step 6007: 0.0367964, 0.0129\n",
      "2017-11-10 17:59:34: Loss and accuracy at step 6008: 0.0367001, 0.0187\n",
      "2017-11-10 17:59:36: Loss and accuracy at step 6009: 0.0367166, 0.0075\n",
      "2017-11-10 17:59:38: Loss and accuracy at step 6010: 0.0368603, 0.0106\n",
      "2017-11-10 17:59:40: Loss and accuracy at step 6011: 0.0367865, 0.0211\n",
      "2017-11-10 17:59:42: Loss and accuracy at step 6012: 0.0367989, 0.0089\n",
      "2017-11-10 17:59:44: Loss and accuracy at step 6013: 0.0368427, 0.0106\n",
      "2017-11-10 17:59:47: Loss and accuracy at step 6014: 0.0366941, 0.015\n",
      "2017-11-10 17:59:49: Loss and accuracy at step 6015: 0.0368724, 0.0097\n",
      "2017-11-10 17:59:51: Loss and accuracy at step 6016: 0.0368283, 0.0102\n",
      "2017-11-10 17:59:53: Loss and accuracy at step 6017: 0.036815, 0.0148\n",
      "2017-11-10 17:59:55: Loss and accuracy at step 6018: 0.0368056, 0.0133\n",
      "2017-11-10 17:59:57: Loss and accuracy at step 6019: 0.0368374, 0.0111\n",
      "2017-11-10 17:59:59: Loss and accuracy at step 6020: 0.0368378, 0.0154\n",
      "2017-11-10 18:00:01: Loss and accuracy at step 6021: 0.0367996, 0.0097\n",
      "2017-11-10 18:00:03: Loss and accuracy at step 6022: 0.0368544, 0.0122\n",
      "2017-11-10 18:00:05: Loss and accuracy at step 6023: 0.0368118, 0.0201\n",
      "2017-11-10 18:00:07: Loss and accuracy at step 6024: 0.036878, 0.0079\n",
      "2017-11-10 18:00:09: Loss and accuracy at step 6025: 0.0367666, 0.0119\n",
      "2017-11-10 18:00:12: Loss and accuracy at step 6026: 0.0366764, 0.0199\n",
      "2017-11-10 18:00:14: Loss and accuracy at step 6027: 0.036729, 0.0108\n",
      "2017-11-10 18:00:16: Loss and accuracy at step 6028: 0.0367757, 0.0121\n",
      "2017-11-10 18:00:18: Loss and accuracy at step 6029: 0.0368568, 0.0146\n",
      "2017-11-10 18:00:20: Loss and accuracy at step 6030: 0.0367586, 0.0137\n",
      "2017-11-10 18:00:22: Loss and accuracy at step 6031: 0.0368174, 0.0089\n",
      "2017-11-10 18:00:24: Loss and accuracy at step 6032: 0.0367517, 0.0167\n",
      "2017-11-10 18:00:26: Loss and accuracy at step 6033: 0.0368284, 0.0125\n",
      "2017-11-10 18:00:28: Loss and accuracy at step 6034: 0.0367077, 0.012\n",
      "2017-11-10 18:00:30: Loss and accuracy at step 6035: 0.0367553, 0.0136\n",
      "2017-11-10 18:00:33: Loss and accuracy at step 6036: 0.0367488, 0.0119\n",
      "2017-11-10 18:00:35: Loss and accuracy at step 6037: 0.0366847, 0.011\n",
      "2017-11-10 18:00:37: Loss and accuracy at step 6038: 0.0367361, 0.0128\n",
      "2017-11-10 18:00:39: Loss and accuracy at step 6039: 0.0367261, 0.013\n",
      "2017-11-10 18:00:41: Loss and accuracy at step 6040: 0.0365295, 0.0129\n",
      "2017-11-10 18:00:43: Loss and accuracy at step 6041: 0.0367267, 0.0121\n",
      "2017-11-10 18:00:45: Loss and accuracy at step 6042: 0.0367528, 0.0112\n",
      "2017-11-10 18:00:47: Loss and accuracy at step 6043: 0.0367723, 0.0146\n",
      "2017-11-10 18:00:49: Loss and accuracy at step 6044: 0.0367112, 0.0156\n",
      "2017-11-10 18:00:51: Loss and accuracy at step 6045: 0.0367354, 0.0111\n",
      "2017-11-10 18:00:53: Loss and accuracy at step 6046: 0.0366835, 0.0131\n",
      "2017-11-10 18:00:56: Loss and accuracy at step 6047: 0.036727, 0.0114\n",
      "2017-11-10 18:00:58: Loss and accuracy at step 6048: 0.0366952, 0.0119\n",
      "2017-11-10 18:01:00: Loss and accuracy at step 6049: 0.0367041, 0.0146\n",
      "2017-11-10 18:01:02: Loss and accuracy at step 6050: 0.0367903, 0.0106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:01:04: Loss and accuracy at step 6051: 0.0368169, 0.0094\n",
      "2017-11-10 18:01:06: Loss and accuracy at step 6052: 0.0367767, 0.0122\n",
      "2017-11-10 18:01:08: Loss and accuracy at step 6053: 0.0367933, 0.0136\n",
      "2017-11-10 18:01:10: Loss and accuracy at step 6054: 0.0367454, 0.0107\n",
      "2017-11-10 18:01:12: Loss and accuracy at step 6055: 0.0367857, 0.0096\n",
      "2017-11-10 18:01:14: Loss and accuracy at step 6056: 0.0367819, 0.0141\n",
      "2017-11-10 18:01:16: Loss and accuracy at step 6057: 0.0367042, 0.0107\n",
      "2017-11-10 18:01:19: Loss and accuracy at step 6058: 0.03678, 0.0132\n",
      "2017-11-10 18:01:21: Loss and accuracy at step 6059: 0.0368676, 0.0148\n",
      "2017-11-10 18:01:23: Loss and accuracy at step 6060: 0.0367621, 0.0174\n",
      "2017-11-10 18:01:25: Loss and accuracy at step 6061: 0.0368158, 0.0138\n",
      "2017-11-10 18:01:27: Loss and accuracy at step 6062: 0.0367924, 0.0099\n",
      "2017-11-10 18:01:29: Loss and accuracy at step 6063: 0.0366955, 0.016\n",
      "2017-11-10 18:01:31: Loss and accuracy at step 6064: 0.0367629, 0.0156\n",
      "2017-11-10 18:01:33: Loss and accuracy at step 6065: 0.0367897, 0.0099\n",
      "2017-11-10 18:01:35: Loss and accuracy at step 6066: 0.0368472, 0.0147\n",
      "2017-11-10 18:01:37: Loss and accuracy at step 6067: 0.0368501, 0.0091\n",
      "2017-11-10 18:01:39: Loss and accuracy at step 6068: 0.0368656, 0.0147\n",
      "2017-11-10 18:01:41: Loss and accuracy at step 6069: 0.0368187, 0.0121\n",
      "2017-11-10 18:01:44: Loss and accuracy at step 6070: 0.0367597, 0.0101\n",
      "2017-11-10 18:01:46: Loss and accuracy at step 6071: 0.0367687, 0.0142\n",
      "2017-11-10 18:01:48: Loss and accuracy at step 6072: 0.0368216, 0.0153\n",
      "2017-11-10 18:01:50: Loss and accuracy at step 6073: 0.0367716, 0.0094\n",
      "2017-11-10 18:01:52: Loss and accuracy at step 6074: 0.0367932, 0.0135\n",
      "2017-11-10 18:01:54: Loss and accuracy at step 6075: 0.0367075, 0.0162\n",
      "2017-11-10 18:01:56: Loss and accuracy at step 6076: 0.0368106, 0.012\n",
      "2017-11-10 18:01:58: Loss and accuracy at step 6077: 0.0367677, 0.0116\n",
      "2017-11-10 18:02:00: Loss and accuracy at step 6078: 0.0368606, 0.011\n",
      "2017-11-10 18:02:02: Loss and accuracy at step 6079: 0.0368212, 0.0128\n",
      "2017-11-10 18:02:04: Loss and accuracy at step 6080: 0.0366974, 0.0099\n",
      "2017-11-10 18:02:06: Loss and accuracy at step 6081: 0.0368132, 0.013\n",
      "2017-11-10 18:02:08: Loss and accuracy at step 6082: 0.0367528, 0.0132\n",
      "2017-11-10 18:02:11: Loss and accuracy at step 6083: 0.036722, 0.0092\n",
      "2017-11-10 18:02:13: Loss and accuracy at step 6084: 0.0367908, 0.0142\n",
      "2017-11-10 18:02:15: Loss and accuracy at step 6085: 0.03674, 0.011\n",
      "2017-11-10 18:02:17: Loss and accuracy at step 6086: 0.0367552, 0.0117\n",
      "2017-11-10 18:02:19: Loss and accuracy at step 6087: 0.0367234, 0.0116\n",
      "2017-11-10 18:02:21: Loss and accuracy at step 6088: 0.0366975, 0.0106\n",
      "2017-11-10 18:02:23: Loss and accuracy at step 6089: 0.0367637, 0.0123\n",
      "2017-11-10 18:02:25: Loss and accuracy at step 6090: 0.0367073, 0.0121\n",
      "2017-11-10 18:02:27: Loss and accuracy at step 6091: 0.0367766, 0.0104\n",
      "2017-11-10 18:02:30: Loss and accuracy at step 6092: 0.0367408, 0.0184\n",
      "2017-11-10 18:02:32: Loss and accuracy at step 6093: 0.0368518, 0.009\n",
      "2017-11-10 18:02:34: Loss and accuracy at step 6094: 0.0367913, 0.0092\n",
      "2017-11-10 18:02:36: Loss and accuracy at step 6095: 0.0367727, 0.0143\n",
      "2017-11-10 18:02:38: Loss and accuracy at step 6096: 0.036742, 0.0098\n",
      "2017-11-10 18:02:40: Loss and accuracy at step 6097: 0.0367405, 0.0091\n",
      "2017-11-10 18:02:42: Loss and accuracy at step 6098: 0.0367348, 0.0168\n",
      "2017-11-10 18:02:44: Loss and accuracy at step 6099: 0.0367312, 0.0132\n",
      "2017-11-10 18:02:46: Loss and accuracy at step 6100: 0.0367064, 0.0092\n",
      "2017-11-10 18:02:48: Loss and accuracy at step 6101: 0.0366919, 0.0118\n",
      "2017-11-10 18:02:50: Loss and accuracy at step 6102: 0.0367949, 0.0138\n",
      "2017-11-10 18:02:52: Loss and accuracy at step 6103: 0.0368548, 0.0091\n",
      "2017-11-10 18:02:55: Loss and accuracy at step 6104: 0.0368073, 0.0129\n",
      "2017-11-10 18:02:57: Loss and accuracy at step 6105: 0.0368587, 0.0109\n",
      "2017-11-10 18:02:59: Loss and accuracy at step 6106: 0.0367769, 0.0095\n",
      "2017-11-10 18:03:01: Loss and accuracy at step 6107: 0.0368001, 0.0111\n",
      "2017-11-10 18:03:03: Loss and accuracy at step 6108: 0.0368033, 0.0131\n",
      "2017-11-10 18:03:05: Loss and accuracy at step 6109: 0.0367187, 0.0117\n",
      "2017-11-10 18:03:07: Loss and accuracy at step 6110: 0.0367625, 0.0122\n",
      "2017-11-10 18:03:09: Loss and accuracy at step 6111: 0.0367113, 0.0107\n",
      "2017-11-10 18:03:11: Loss and accuracy at step 6112: 0.0367211, 0.0114\n",
      "2017-11-10 18:03:13: Loss and accuracy at step 6113: 0.0366791, 0.0143\n",
      "2017-11-10 18:03:15: Loss and accuracy at step 6114: 0.0367527, 0.0128\n",
      "2017-11-10 18:03:18: Loss and accuracy at step 6115: 0.0367652, 0.01\n",
      "2017-11-10 18:03:20: Loss and accuracy at step 6116: 0.0367266, 0.0147\n",
      "2017-11-10 18:03:22: Loss and accuracy at step 6117: 0.0367229, 0.013\n",
      "2017-11-10 18:03:24: Loss and accuracy at step 6118: 0.0367378, 0.0106\n",
      "2017-11-10 18:03:26: Loss and accuracy at step 6119: 0.0367984, 0.0164\n",
      "2017-11-10 18:03:28: Loss and accuracy at step 6120: 0.0368001, 0.0112\n",
      "2017-11-10 18:03:30: Loss and accuracy at step 6121: 0.0367761, 0.0119\n",
      "2017-11-10 18:03:32: Loss and accuracy at step 6122: 0.036817, 0.0093\n",
      "2017-11-10 18:03:34: Loss and accuracy at step 6123: 0.0367978, 0.016\n",
      "2017-11-10 18:03:36: Loss and accuracy at step 6124: 0.0366784, 0.0189\n",
      "2017-11-10 18:03:38: Loss and accuracy at step 6125: 0.0367377, 0.007\n",
      "2017-11-10 18:03:40: Loss and accuracy at step 6126: 0.0367927, 0.0155\n",
      "2017-11-10 18:03:43: Loss and accuracy at step 6127: 0.0367111, 0.015\n",
      "2017-11-10 18:03:45: Loss and accuracy at step 6128: 0.0366342, 0.009\n",
      "2017-11-10 18:03:47: Loss and accuracy at step 6129: 0.0367643, 0.0179\n",
      "2017-11-10 18:03:49: Loss and accuracy at step 6130: 0.036881, 0.0133\n",
      "2017-11-10 18:03:51: Loss and accuracy at step 6131: 0.0368103, 0.0113\n",
      "2017-11-10 18:03:53: Loss and accuracy at step 6132: 0.0367303, 0.015\n",
      "2017-11-10 18:03:55: Loss and accuracy at step 6133: 0.0367622, 0.0134\n",
      "2017-11-10 18:03:57: Loss and accuracy at step 6134: 0.0367981, 0.0137\n",
      "2017-11-10 18:03:59: Loss and accuracy at step 6135: 0.0367721, 0.01\n",
      "2017-11-10 18:04:01: Loss and accuracy at step 6136: 0.0367269, 0.0149\n",
      "2017-11-10 18:04:03: Loss and accuracy at step 6137: 0.0367849, 0.0079\n",
      "2017-11-10 18:04:05: Loss and accuracy at step 6138: 0.0367838, 0.011\n",
      "2017-11-10 18:04:08: Loss and accuracy at step 6139: 0.0368713, 0.0148\n",
      "2017-11-10 18:04:10: Loss and accuracy at step 6140: 0.0368614, 0.0105\n",
      "2017-11-10 18:04:12: Loss and accuracy at step 6141: 0.0368127, 0.0133\n",
      "2017-11-10 18:04:14: Loss and accuracy at step 6142: 0.0367574, 0.014\n",
      "2017-11-10 18:04:16: Loss and accuracy at step 6143: 0.0368198, 0.0089\n",
      "2017-11-10 18:04:18: Loss and accuracy at step 6144: 0.0367913, 0.0153\n",
      "2017-11-10 18:04:20: Loss and accuracy at step 6145: 0.0367833, 0.0092\n",
      "2017-11-10 18:04:22: Loss and accuracy at step 6146: 0.0366716, 0.0141\n",
      "2017-11-10 18:04:24: Loss and accuracy at step 6147: 0.0368531, 0.0109\n",
      "2017-11-10 18:04:26: Loss and accuracy at step 6148: 0.0367835, 0.0101\n",
      "2017-11-10 18:04:29: Loss and accuracy at step 6149: 0.036816, 0.0141\n",
      "2017-11-10 18:04:31: Loss and accuracy at step 6150: 0.0367874, 0.0124\n",
      "2017-11-10 18:04:33: Loss and accuracy at step 6151: 0.0368095, 0.0107\n",
      "2017-11-10 18:04:35: Loss and accuracy at step 6152: 0.0367652, 0.0105\n",
      "2017-11-10 18:04:37: Loss and accuracy at step 6153: 0.036825, 0.017\n",
      "2017-11-10 18:04:39: Loss and accuracy at step 6154: 0.0366918, 0.0112\n",
      "2017-11-10 18:04:41: Loss and accuracy at step 6155: 0.0368248, 0.01\n",
      "2017-11-10 18:04:43: Loss and accuracy at step 6156: 0.0367636, 0.0163\n",
      "2017-11-10 18:04:45: Loss and accuracy at step 6157: 0.0367785, 0.0164\n",
      "2017-11-10 18:04:47: Loss and accuracy at step 6158: 0.0367337, 0.0098\n",
      "2017-11-10 18:04:49: Loss and accuracy at step 6159: 0.036781, 0.0135\n",
      "2017-11-10 18:04:51: Loss and accuracy at step 6160: 0.0367495, 0.02\n",
      "2017-11-10 18:04:54: Loss and accuracy at step 6161: 0.0367419, 0.0082\n",
      "2017-11-10 18:04:56: Loss and accuracy at step 6162: 0.0367887, 0.0148\n",
      "2017-11-10 18:04:58: Loss and accuracy at step 6163: 0.0367433, 0.0171\n",
      "2017-11-10 18:05:00: Loss and accuracy at step 6164: 0.0368488, 0.0097\n",
      "2017-11-10 18:05:02: Loss and accuracy at step 6165: 0.0368243, 0.0125\n",
      "2017-11-10 18:05:04: Loss and accuracy at step 6166: 0.0367937, 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:05:06: Loss and accuracy at step 6167: 0.0367369, 0.0124\n",
      "2017-11-10 18:05:08: Loss and accuracy at step 6168: 0.0366977, 0.0112\n",
      "2017-11-10 18:05:10: Loss and accuracy at step 6169: 0.0366463, 0.0133\n",
      "2017-11-10 18:05:12: Loss and accuracy at step 6170: 0.0367997, 0.0134\n",
      "2017-11-10 18:05:14: Loss and accuracy at step 6171: 0.0367937, 0.0129\n",
      "2017-11-10 18:05:16: Loss and accuracy at step 6172: 0.0367483, 0.0116\n",
      "2017-11-10 18:05:19: Loss and accuracy at step 6173: 0.0367843, 0.0145\n",
      "2017-11-10 18:05:21: Loss and accuracy at step 6174: 0.0366778, 0.0142\n",
      "2017-11-10 18:05:23: Loss and accuracy at step 6175: 0.0367105, 0.0124\n",
      "2017-11-10 18:05:25: Loss and accuracy at step 6176: 0.0367722, 0.01\n",
      "2017-11-10 18:05:27: Loss and accuracy at step 6177: 0.0367148, 0.013\n",
      "2017-11-10 18:05:29: Loss and accuracy at step 6178: 0.0367518, 0.0146\n",
      "2017-11-10 18:05:31: Loss and accuracy at step 6179: 0.0367208, 0.012\n",
      "2017-11-10 18:05:33: Loss and accuracy at step 6180: 0.036821, 0.0118\n",
      "2017-11-10 18:05:35: Loss and accuracy at step 6181: 0.0367448, 0.0136\n",
      "2017-11-10 18:05:37: Loss and accuracy at step 6182: 0.0367029, 0.0102\n",
      "2017-11-10 18:05:39: Loss and accuracy at step 6183: 0.0367006, 0.0152\n",
      "2017-11-10 18:05:42: Loss and accuracy at step 6184: 0.0367884, 0.0125\n",
      "2017-11-10 18:05:44: Loss and accuracy at step 6185: 0.0367474, 0.0112\n",
      "2017-11-10 18:05:46: Loss and accuracy at step 6186: 0.0366793, 0.0156\n",
      "2017-11-10 18:05:48: Loss and accuracy at step 6187: 0.0368091, 0.0117\n",
      "2017-11-10 18:05:50: Loss and accuracy at step 6188: 0.0367638, 0.0118\n",
      "2017-11-10 18:05:52: Loss and accuracy at step 6189: 0.0368219, 0.0106\n",
      "2017-11-10 18:05:54: Loss and accuracy at step 6190: 0.0368185, 0.0125\n",
      "2017-11-10 18:05:56: Loss and accuracy at step 6191: 0.0367579, 0.0136\n",
      "2017-11-10 18:05:58: Loss and accuracy at step 6192: 0.0367991, 0.0082\n",
      "2017-11-10 18:06:00: Loss and accuracy at step 6193: 0.0367159, 0.0157\n",
      "2017-11-10 18:06:02: Loss and accuracy at step 6194: 0.0367713, 0.0127\n",
      "2017-11-10 18:06:04: Loss and accuracy at step 6195: 0.0366846, 0.0122\n",
      "2017-11-10 18:06:07: Loss and accuracy at step 6196: 0.03671, 0.0189\n",
      "2017-11-10 18:06:09: Loss and accuracy at step 6197: 0.036675, 0.0118\n",
      "2017-11-10 18:06:11: Loss and accuracy at step 6198: 0.03677, 0.011\n",
      "2017-11-10 18:06:13: Loss and accuracy at step 6199: 0.0368309, 0.0144\n",
      "2017-11-10 18:06:15: Loss and accuracy at step 6200: 0.0367359, 0.0112\n",
      "2017-11-10 18:06:17: Loss and accuracy at step 6201: 0.0367362, 0.0133\n",
      "2017-11-10 18:06:19: Loss and accuracy at step 6202: 0.0368451, 0.0096\n",
      "2017-11-10 18:06:21: Loss and accuracy at step 6203: 0.0367285, 0.0139\n",
      "2017-11-10 18:06:23: Loss and accuracy at step 6204: 0.0368244, 0.0139\n",
      "2017-11-10 18:06:26: Loss and accuracy at step 6205: 0.0368011, 0.0126\n",
      "2017-11-10 18:06:28: Loss and accuracy at step 6206: 0.0368915, 0.012\n",
      "2017-11-10 18:06:30: Loss and accuracy at step 6207: 0.0368881, 0.0119\n",
      "2017-11-10 18:06:32: Loss and accuracy at step 6208: 0.0367629, 0.0142\n",
      "2017-11-10 18:06:34: Loss and accuracy at step 6209: 0.0367371, 0.0117\n",
      "2017-11-10 18:06:36: Loss and accuracy at step 6210: 0.0368366, 0.0101\n",
      "2017-11-10 18:06:38: Loss and accuracy at step 6211: 0.0367577, 0.0208\n",
      "2017-11-10 18:06:40: Loss and accuracy at step 6212: 0.0367295, 0.0063\n",
      "2017-11-10 18:06:42: Loss and accuracy at step 6213: 0.0367627, 0.0153\n",
      "2017-11-10 18:06:44: Loss and accuracy at step 6214: 0.036786, 0.0132\n",
      "2017-11-10 18:06:46: Loss and accuracy at step 6215: 0.0368126, 0.0136\n",
      "2017-11-10 18:06:48: Loss and accuracy at step 6216: 0.0367865, 0.0128\n",
      "2017-11-10 18:06:50: Loss and accuracy at step 6217: 0.036675, 0.0093\n",
      "2017-11-10 18:06:53: Loss and accuracy at step 6218: 0.0367921, 0.0198\n",
      "2017-11-10 18:06:55: Loss and accuracy at step 6219: 0.0367654, 0.0101\n",
      "2017-11-10 18:06:57: Loss and accuracy at step 6220: 0.0367341, 0.0117\n",
      "2017-11-10 18:06:59: Loss and accuracy at step 6221: 0.0367339, 0.0175\n",
      "2017-11-10 18:07:01: Loss and accuracy at step 6222: 0.0367469, 0.0085\n",
      "2017-11-10 18:07:03: Loss and accuracy at step 6223: 0.0368125, 0.0183\n",
      "2017-11-10 18:07:05: Loss and accuracy at step 6224: 0.0367965, 0.0065\n",
      "2017-11-10 18:07:07: Loss and accuracy at step 6225: 0.0367922, 0.0138\n",
      "2017-11-10 18:07:09: Loss and accuracy at step 6226: 0.0368391, 0.0172\n",
      "2017-11-10 18:07:11: Loss and accuracy at step 6227: 0.0367849, 0.0068\n",
      "2017-11-10 18:07:13: Loss and accuracy at step 6228: 0.0367593, 0.0186\n",
      "2017-11-10 18:07:16: Loss and accuracy at step 6229: 0.0368242, 0.0105\n",
      "2017-11-10 18:07:18: Loss and accuracy at step 6230: 0.036752, 0.0145\n",
      "2017-11-10 18:07:20: Loss and accuracy at step 6231: 0.0367261, 0.0153\n",
      "2017-11-10 18:07:22: Loss and accuracy at step 6232: 0.0367525, 0.008\n",
      "2017-11-10 18:07:24: Loss and accuracy at step 6233: 0.0368489, 0.0184\n",
      "2017-11-10 18:07:26: Loss and accuracy at step 6234: 0.0367209, 0.0125\n",
      "2017-11-10 18:07:28: Loss and accuracy at step 6235: 0.0367755, 0.0119\n",
      "2017-11-10 18:07:30: Loss and accuracy at step 6236: 0.0367723, 0.0136\n",
      "2017-11-10 18:07:32: Loss and accuracy at step 6237: 0.0368265, 0.0105\n",
      "2017-11-10 18:07:34: Loss and accuracy at step 6238: 0.036795, 0.0212\n",
      "2017-11-10 18:07:36: Loss and accuracy at step 6239: 0.0367561, 0.0086\n",
      "2017-11-10 18:07:38: Loss and accuracy at step 6240: 0.0368566, 0.0093\n",
      "2017-11-10 18:07:41: Loss and accuracy at step 6241: 0.0367502, 0.0253\n",
      "2017-11-10 18:07:43: Loss and accuracy at step 6242: 0.0367784, 0.0061\n",
      "2017-11-10 18:07:45: Loss and accuracy at step 6243: 0.0367484, 0.0125\n",
      "2017-11-10 18:07:47: Loss and accuracy at step 6244: 0.0367975, 0.0138\n",
      "2017-11-10 18:07:49: Loss and accuracy at step 6245: 0.0368199, 0.0119\n",
      "2017-11-10 18:07:51: Loss and accuracy at step 6246: 0.0368035, 0.0126\n",
      "2017-11-10 18:07:53: Loss and accuracy at step 6247: 0.0367244, 0.0082\n",
      "2017-11-10 18:07:55: Loss and accuracy at step 6248: 0.0367947, 0.0229\n",
      "2017-11-10 18:07:57: Loss and accuracy at step 6249: 0.0367288, 0.0127\n",
      "2017-11-10 18:07:59: Loss and accuracy at step 6250: 0.0367493, 0.0072\n",
      "2017-11-10 18:08:01: Loss and accuracy at step 6251: 0.0367711, 0.0165\n",
      "2017-11-10 18:08:03: Loss and accuracy at step 6252: 0.0367508, 0.0115\n",
      "2017-11-10 18:08:06: Loss and accuracy at step 6253: 0.036882, 0.0092\n",
      "2017-11-10 18:08:08: Loss and accuracy at step 6254: 0.0368282, 0.0175\n",
      "2017-11-10 18:08:10: Loss and accuracy at step 6255: 0.036875, 0.0076\n",
      "2017-11-10 18:08:12: Loss and accuracy at step 6256: 0.0367908, 0.0207\n",
      "2017-11-10 18:08:14: Loss and accuracy at step 6257: 0.0367553, 0.01\n",
      "2017-11-10 18:08:16: Loss and accuracy at step 6258: 0.0367338, 0.0095\n",
      "2017-11-10 18:08:18: Loss and accuracy at step 6259: 0.036773, 0.0344\n",
      "2017-11-10 18:08:20: Loss and accuracy at step 6260: 0.0368502, 0.007\n",
      "2017-11-10 18:08:22: Loss and accuracy at step 6261: 0.0367818, 0.0125\n",
      "2017-11-10 18:08:25: Loss and accuracy at step 6262: 0.0368022, 0.0354\n",
      "2017-11-10 18:08:27: Loss and accuracy at step 6263: 0.0367813, 0.0065\n",
      "2017-11-10 18:08:29: Loss and accuracy at step 6264: 0.0367542, 0.0138\n",
      "2017-11-10 18:08:31: Loss and accuracy at step 6265: 0.0368177, 0.0199\n",
      "2017-11-10 18:08:33: Loss and accuracy at step 6266: 0.0367471, 0.0096\n",
      "2017-11-10 18:08:35: Loss and accuracy at step 6267: 0.0367191, 0.0184\n",
      "2017-11-10 18:08:38: Loss and accuracy at step 6268: 0.0368302, 0.0125\n",
      "2017-11-10 18:08:40: Loss and accuracy at step 6269: 0.0368403, 0.0144\n",
      "2017-11-10 18:08:42: Loss and accuracy at step 6270: 0.0368103, 0.0182\n",
      "2017-11-10 18:08:44: Loss and accuracy at step 6271: 0.0367046, 0.0095\n",
      "2017-11-10 18:08:46: Loss and accuracy at step 6272: 0.036762, 0.0193\n",
      "2017-11-10 18:08:48: Loss and accuracy at step 6273: 0.0367464, 0.0122\n",
      "2017-11-10 18:08:51: Loss and accuracy at step 6274: 0.0367903, 0.0125\n",
      "2017-11-10 18:08:53: Loss and accuracy at step 6275: 0.0366678, 0.0187\n",
      "2017-11-10 18:08:55: Loss and accuracy at step 6276: 0.0368031, 0.0105\n",
      "2017-11-10 18:08:57: Loss and accuracy at step 6277: 0.0367917, 0.016\n",
      "2017-11-10 18:08:59: Loss and accuracy at step 6278: 0.0367009, 0.0112\n",
      "2017-11-10 18:09:01: Loss and accuracy at step 6279: 0.0367588, 0.0131\n",
      "2017-11-10 18:09:03: Loss and accuracy at step 6280: 0.0368108, 0.0149\n",
      "2017-11-10 18:09:05: Loss and accuracy at step 6281: 0.036847, 0.0081\n",
      "2017-11-10 18:09:07: Loss and accuracy at step 6282: 0.0367901, 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:09:10: Loss and accuracy at step 6283: 0.0367461, 0.0162\n",
      "2017-11-10 18:09:12: Loss and accuracy at step 6284: 0.0367209, 0.0085\n",
      "2017-11-10 18:09:14: Loss and accuracy at step 6285: 0.0367322, 0.0146\n",
      "2017-11-10 18:09:16: Loss and accuracy at step 6286: 0.0367618, 0.013\n",
      "2017-11-10 18:09:18: Loss and accuracy at step 6287: 0.0367769, 0.0139\n",
      "2017-11-10 18:09:20: Loss and accuracy at step 6288: 0.0368193, 0.0137\n",
      "2017-11-10 18:09:22: Loss and accuracy at step 6289: 0.0367664, 0.0127\n",
      "2017-11-10 18:09:25: Loss and accuracy at step 6290: 0.036823, 0.0129\n",
      "2017-11-10 18:09:27: Loss and accuracy at step 6291: 0.0367497, 0.0136\n",
      "2017-11-10 18:09:29: Loss and accuracy at step 6292: 0.0366968, 0.0115\n",
      "2017-11-10 18:09:31: Loss and accuracy at step 6293: 0.0367314, 0.0126\n",
      "2017-11-10 18:09:33: Loss and accuracy at step 6294: 0.0367448, 0.0128\n",
      "2017-11-10 18:09:35: Loss and accuracy at step 6295: 0.0368266, 0.0136\n",
      "2017-11-10 18:09:37: Loss and accuracy at step 6296: 0.036777, 0.0093\n",
      "2017-11-10 18:09:39: Loss and accuracy at step 6297: 0.0368146, 0.0125\n",
      "2017-11-10 18:09:41: Loss and accuracy at step 6298: 0.0366678, 0.0156\n",
      "2017-11-10 18:09:43: Loss and accuracy at step 6299: 0.0367649, 0.0094\n",
      "2017-11-10 18:09:46: Loss and accuracy at step 6300: 0.0367649, 0.0153\n",
      "2017-11-10 18:09:48: Loss and accuracy at step 6301: 0.0367813, 0.0106\n",
      "2017-11-10 18:09:50: Loss and accuracy at step 6302: 0.0368087, 0.0161\n",
      "2017-11-10 18:09:52: Loss and accuracy at step 6303: 0.0367707, 0.0145\n",
      "2017-11-10 18:09:54: Loss and accuracy at step 6304: 0.0367419, 0.0089\n",
      "2017-11-10 18:09:56: Loss and accuracy at step 6305: 0.0367406, 0.0151\n",
      "2017-11-10 18:09:58: Loss and accuracy at step 6306: 0.0368438, 0.0137\n",
      "2017-11-10 18:10:00: Loss and accuracy at step 6307: 0.0367673, 0.0093\n",
      "2017-11-10 18:10:02: Loss and accuracy at step 6308: 0.0367255, 0.0154\n",
      "2017-11-10 18:10:05: Loss and accuracy at step 6309: 0.0367049, 0.0125\n",
      "2017-11-10 18:10:07: Loss and accuracy at step 6310: 0.0367191, 0.0138\n",
      "2017-11-10 18:10:09: Loss and accuracy at step 6311: 0.0367236, 0.0151\n",
      "2017-11-10 18:10:11: Loss and accuracy at step 6312: 0.0368256, 0.0093\n",
      "2017-11-10 18:10:13: Loss and accuracy at step 6313: 0.0366728, 0.0202\n",
      "2017-11-10 18:10:15: Loss and accuracy at step 6314: 0.0368351, 0.0126\n",
      "2017-11-10 18:10:17: Loss and accuracy at step 6315: 0.0366789, 0.0089\n",
      "2017-11-10 18:10:20: Loss and accuracy at step 6316: 0.0366872, 0.0143\n",
      "2017-11-10 18:10:22: Loss and accuracy at step 6317: 0.0367986, 0.01\n",
      "2017-11-10 18:10:24: Loss and accuracy at step 6318: 0.0367935, 0.0128\n",
      "2017-11-10 18:10:26: Loss and accuracy at step 6319: 0.0367821, 0.0111\n",
      "2017-11-10 18:10:28: Loss and accuracy at step 6320: 0.0367382, 0.0091\n",
      "2017-11-10 18:10:30: Loss and accuracy at step 6321: 0.0368885, 0.0246\n",
      "2017-11-10 18:10:32: Loss and accuracy at step 6322: 0.0368374, 0.0097\n",
      "2017-11-10 18:10:34: Loss and accuracy at step 6323: 0.0367497, 0.0133\n",
      "2017-11-10 18:10:36: Loss and accuracy at step 6324: 0.0369026, 0.0127\n",
      "2017-11-10 18:10:38: Loss and accuracy at step 6325: 0.0368067, 0.0113\n",
      "2017-11-10 18:10:40: Loss and accuracy at step 6326: 0.0367118, 0.0164\n",
      "2017-11-10 18:10:42: Loss and accuracy at step 6327: 0.0367417, 0.012\n",
      "2017-11-10 18:10:44: Loss and accuracy at step 6328: 0.0368715, 0.0111\n",
      "2017-11-10 18:10:47: Loss and accuracy at step 6329: 0.0367903, 0.0169\n",
      "2017-11-10 18:10:49: Loss and accuracy at step 6330: 0.0366962, 0.0109\n",
      "2017-11-10 18:10:51: Loss and accuracy at step 6331: 0.0368025, 0.0134\n",
      "2017-11-10 18:10:53: Loss and accuracy at step 6332: 0.0367866, 0.0127\n",
      "2017-11-10 18:10:55: Loss and accuracy at step 6333: 0.0367981, 0.0092\n",
      "2017-11-10 18:10:57: Loss and accuracy at step 6334: 0.0367524, 0.0152\n",
      "2017-11-10 18:10:59: Loss and accuracy at step 6335: 0.0367655, 0.0103\n",
      "2017-11-10 18:11:01: Loss and accuracy at step 6336: 0.0368233, 0.0133\n",
      "2017-11-10 18:11:03: Loss and accuracy at step 6337: 0.036791, 0.0126\n",
      "2017-11-10 18:11:05: Loss and accuracy at step 6338: 0.0368276, 0.014\n",
      "2017-11-10 18:11:07: Loss and accuracy at step 6339: 0.0368658, 0.0144\n",
      "2017-11-10 18:11:09: Loss and accuracy at step 6340: 0.0368243, 0.0093\n",
      "2017-11-10 18:11:12: Loss and accuracy at step 6341: 0.0368377, 0.0191\n",
      "2017-11-10 18:11:14: Loss and accuracy at step 6342: 0.0368084, 0.0143\n",
      "2017-11-10 18:11:16: Loss and accuracy at step 6343: 0.0368183, 0.009\n",
      "2017-11-10 18:11:18: Loss and accuracy at step 6344: 0.0367983, 0.0198\n",
      "2017-11-10 18:11:20: Loss and accuracy at step 6345: 0.0367183, 0.0098\n",
      "2017-11-10 18:11:22: Loss and accuracy at step 6346: 0.0368368, 0.014\n",
      "2017-11-10 18:11:24: Loss and accuracy at step 6347: 0.0368384, 0.0126\n",
      "2017-11-10 18:11:26: Loss and accuracy at step 6348: 0.0367613, 0.011\n",
      "2017-11-10 18:11:28: Loss and accuracy at step 6349: 0.036833, 0.0103\n",
      "2017-11-10 18:11:30: Loss and accuracy at step 6350: 0.0367553, 0.0176\n",
      "2017-11-10 18:11:32: Loss and accuracy at step 6351: 0.0367773, 0.0147\n",
      "2017-11-10 18:11:34: Loss and accuracy at step 6352: 0.0366867, 0.0083\n",
      "2017-11-10 18:11:37: Loss and accuracy at step 6353: 0.0367525, 0.0162\n",
      "2017-11-10 18:11:39: Loss and accuracy at step 6354: 0.0368379, 0.0162\n",
      "2017-11-10 18:11:41: Loss and accuracy at step 6355: 0.036784, 0.0096\n",
      "2017-11-10 18:11:43: Loss and accuracy at step 6356: 0.0368008, 0.0153\n",
      "2017-11-10 18:11:45: Loss and accuracy at step 6357: 0.0367856, 0.0161\n",
      "2017-11-10 18:11:47: Loss and accuracy at step 6358: 0.0368103, 0.01\n",
      "2017-11-10 18:11:49: Loss and accuracy at step 6359: 0.0367621, 0.0135\n",
      "2017-11-10 18:11:51: Loss and accuracy at step 6360: 0.036752, 0.0103\n",
      "2017-11-10 18:11:53: Loss and accuracy at step 6361: 0.0368283, 0.0156\n",
      "2017-11-10 18:11:55: Loss and accuracy at step 6362: 0.0367575, 0.014\n",
      "2017-11-10 18:11:57: Loss and accuracy at step 6363: 0.0368062, 0.0095\n",
      "2017-11-10 18:11:59: Loss and accuracy at step 6364: 0.0366963, 0.0151\n",
      "2017-11-10 18:12:02: Loss and accuracy at step 6365: 0.0366883, 0.0196\n",
      "2017-11-10 18:12:04: Loss and accuracy at step 6366: 0.0367838, 0.0099\n",
      "2017-11-10 18:12:06: Loss and accuracy at step 6367: 0.03678, 0.0095\n",
      "2017-11-10 18:12:08: Loss and accuracy at step 6368: 0.0368039, 0.0154\n",
      "2017-11-10 18:12:10: Loss and accuracy at step 6369: 0.0368113, 0.0183\n",
      "2017-11-10 18:12:12: Loss and accuracy at step 6370: 0.0367249, 0.0067\n",
      "2017-11-10 18:12:14: Loss and accuracy at step 6371: 0.0367912, 0.0226\n",
      "2017-11-10 18:12:16: Loss and accuracy at step 6372: 0.0368425, 0.0099\n",
      "2017-11-10 18:12:18: Loss and accuracy at step 6373: 0.0367909, 0.0088\n",
      "2017-11-10 18:12:21: Loss and accuracy at step 6374: 0.0367923, 0.0232\n",
      "2017-11-10 18:12:23: Loss and accuracy at step 6375: 0.0367746, 0.008\n",
      "2017-11-10 18:12:25: Loss and accuracy at step 6376: 0.036796, 0.0138\n",
      "2017-11-10 18:12:27: Loss and accuracy at step 6377: 0.0367975, 0.0115\n",
      "2017-11-10 18:12:29: Loss and accuracy at step 6378: 0.0368242, 0.0096\n",
      "2017-11-10 18:12:31: Loss and accuracy at step 6379: 0.0366976, 0.0185\n",
      "2017-11-10 18:12:33: Loss and accuracy at step 6380: 0.0369112, 0.0105\n",
      "2017-11-10 18:12:35: Loss and accuracy at step 6381: 0.0368676, 0.0099\n",
      "2017-11-10 18:12:38: Loss and accuracy at step 6382: 0.0368407, 0.016\n",
      "2017-11-10 18:12:40: Loss and accuracy at step 6383: 0.0368114, 0.0145\n",
      "2017-11-10 18:12:42: Loss and accuracy at step 6384: 0.0367988, 0.0109\n",
      "2017-11-10 18:12:44: Loss and accuracy at step 6385: 0.0368872, 0.0178\n",
      "2017-11-10 18:12:46: Loss and accuracy at step 6386: 0.0367382, 0.0097\n",
      "2017-11-10 18:12:48: Loss and accuracy at step 6387: 0.0368227, 0.0123\n",
      "2017-11-10 18:12:50: Loss and accuracy at step 6388: 0.0367307, 0.0271\n",
      "2017-11-10 18:12:52: Loss and accuracy at step 6389: 0.0367867, 0.0098\n",
      "2017-11-10 18:12:54: Loss and accuracy at step 6390: 0.0367176, 0.0125\n",
      "2017-11-10 18:12:56: Loss and accuracy at step 6391: 0.0367333, 0.0187\n",
      "2017-11-10 18:12:59: Loss and accuracy at step 6392: 0.0367794, 0.01\n",
      "2017-11-10 18:13:01: Loss and accuracy at step 6393: 0.0367997, 0.0181\n",
      "2017-11-10 18:13:03: Loss and accuracy at step 6394: 0.0367752, 0.0206\n",
      "2017-11-10 18:13:05: Loss and accuracy at step 6395: 0.0368294, 0.0095\n",
      "2017-11-10 18:13:07: Loss and accuracy at step 6396: 0.036754, 0.0169\n",
      "2017-11-10 18:13:09: Loss and accuracy at step 6397: 0.0367431, 0.012\n",
      "2017-11-10 18:13:11: Loss and accuracy at step 6398: 0.0368169, 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:13:13: Loss and accuracy at step 6399: 0.0367843, 0.0169\n",
      "2017-11-10 18:13:15: Loss and accuracy at step 6400: 0.0367233, 0.0124\n",
      "2017-11-10 18:13:17: Loss and accuracy at step 6401: 0.0368223, 0.0119\n",
      "2017-11-10 18:13:19: Loss and accuracy at step 6402: 0.0367482, 0.018\n",
      "2017-11-10 18:13:22: Loss and accuracy at step 6403: 0.0366251, 0.0117\n",
      "2017-11-10 18:13:24: Loss and accuracy at step 6404: 0.036715, 0.0119\n",
      "2017-11-10 18:13:26: Loss and accuracy at step 6405: 0.036728, 0.014\n",
      "2017-11-10 18:13:28: Loss and accuracy at step 6406: 0.0366592, 0.0068\n",
      "2017-11-10 18:13:30: Loss and accuracy at step 6407: 0.0367643, 0.0185\n",
      "2017-11-10 18:13:32: Loss and accuracy at step 6408: 0.0367649, 0.0114\n",
      "2017-11-10 18:13:34: Loss and accuracy at step 6409: 0.0366536, 0.0131\n",
      "2017-11-10 18:13:36: Loss and accuracy at step 6410: 0.0367691, 0.0149\n",
      "2017-11-10 18:13:38: Loss and accuracy at step 6411: 0.0367679, 0.0107\n",
      "2017-11-10 18:13:40: Loss and accuracy at step 6412: 0.036766, 0.0153\n",
      "2017-11-10 18:13:42: Loss and accuracy at step 6413: 0.0367771, 0.0115\n",
      "2017-11-10 18:13:45: Loss and accuracy at step 6414: 0.0367441, 0.0137\n",
      "2017-11-10 18:13:47: Loss and accuracy at step 6415: 0.0368388, 0.0129\n",
      "2017-11-10 18:13:49: Loss and accuracy at step 6416: 0.0368216, 0.0114\n",
      "2017-11-10 18:13:51: Loss and accuracy at step 6417: 0.036701, 0.0141\n",
      "2017-11-10 18:13:53: Loss and accuracy at step 6418: 0.0368184, 0.014\n",
      "2017-11-10 18:13:55: Loss and accuracy at step 6419: 0.0367682, 0.0101\n",
      "2017-11-10 18:13:57: Loss and accuracy at step 6420: 0.0367789, 0.0161\n",
      "2017-11-10 18:13:59: Loss and accuracy at step 6421: 0.0367129, 0.01\n",
      "2017-11-10 18:14:01: Loss and accuracy at step 6422: 0.0368231, 0.0139\n",
      "2017-11-10 18:14:03: Loss and accuracy at step 6423: 0.0367689, 0.0131\n",
      "2017-11-10 18:14:05: Loss and accuracy at step 6424: 0.0368328, 0.0102\n",
      "2017-11-10 18:14:07: Loss and accuracy at step 6425: 0.0366691, 0.0147\n",
      "2017-11-10 18:14:09: Loss and accuracy at step 6426: 0.0367569, 0.0105\n",
      "2017-11-10 18:14:12: Loss and accuracy at step 6427: 0.0367723, 0.0184\n",
      "2017-11-10 18:14:14: Loss and accuracy at step 6428: 0.0367462, 0.0134\n",
      "2017-11-10 18:14:16: Loss and accuracy at step 6429: 0.0367572, 0.0092\n",
      "2017-11-10 18:14:18: Loss and accuracy at step 6430: 0.0366679, 0.0159\n",
      "2017-11-10 18:14:20: Loss and accuracy at step 6431: 0.0368292, 0.0088\n",
      "2017-11-10 18:14:22: Loss and accuracy at step 6432: 0.0367355, 0.0173\n",
      "2017-11-10 18:14:24: Loss and accuracy at step 6433: 0.0368505, 0.0102\n",
      "2017-11-10 18:14:26: Loss and accuracy at step 6434: 0.0367924, 0.0098\n",
      "2017-11-10 18:14:28: Loss and accuracy at step 6435: 0.0367892, 0.015\n",
      "2017-11-10 18:14:30: Loss and accuracy at step 6436: 0.036771, 0.0105\n",
      "2017-11-10 18:14:33: Loss and accuracy at step 6437: 0.0368246, 0.0148\n",
      "2017-11-10 18:14:35: Loss and accuracy at step 6438: 0.0366905, 0.0116\n",
      "2017-11-10 18:14:37: Loss and accuracy at step 6439: 0.0367849, 0.0095\n",
      "2017-11-10 18:14:39: Loss and accuracy at step 6440: 0.0367813, 0.015\n",
      "2017-11-10 18:14:41: Loss and accuracy at step 6441: 0.036724, 0.0104\n",
      "2017-11-10 18:14:43: Loss and accuracy at step 6442: 0.0366722, 0.0149\n",
      "2017-11-10 18:14:45: Loss and accuracy at step 6443: 0.0367401, 0.0105\n",
      "2017-11-10 18:14:47: Loss and accuracy at step 6444: 0.0367664, 0.015\n",
      "2017-11-10 18:14:49: Loss and accuracy at step 6445: 0.0368205, 0.0102\n",
      "2017-11-10 18:14:51: Loss and accuracy at step 6446: 0.0367057, 0.0115\n",
      "2017-11-10 18:14:53: Loss and accuracy at step 6447: 0.0367438, 0.02\n",
      "2017-11-10 18:14:56: Loss and accuracy at step 6448: 0.036692, 0.0085\n",
      "2017-11-10 18:14:58: Loss and accuracy at step 6449: 0.0366856, 0.017\n",
      "2017-11-10 18:15:00: Loss and accuracy at step 6450: 0.0366821, 0.0099\n",
      "2017-11-10 18:15:02: Loss and accuracy at step 6451: 0.0367108, 0.0102\n",
      "2017-11-10 18:15:04: Loss and accuracy at step 6452: 0.0368578, 0.0168\n",
      "2017-11-10 18:15:06: Loss and accuracy at step 6453: 0.0367924, 0.0097\n",
      "2017-11-10 18:15:08: Loss and accuracy at step 6454: 0.0368248, 0.0163\n",
      "2017-11-10 18:15:10: Loss and accuracy at step 6455: 0.0366872, 0.0131\n",
      "2017-11-10 18:15:12: Loss and accuracy at step 6456: 0.0366968, 0.015\n",
      "2017-11-10 18:15:14: Loss and accuracy at step 6457: 0.0367986, 0.0119\n",
      "2017-11-10 18:15:16: Loss and accuracy at step 6458: 0.0367531, 0.0143\n",
      "2017-11-10 18:15:19: Loss and accuracy at step 6459: 0.0367169, 0.0117\n",
      "2017-11-10 18:15:21: Loss and accuracy at step 6460: 0.0367896, 0.0128\n",
      "2017-11-10 18:15:23: Loss and accuracy at step 6461: 0.0367358, 0.0125\n",
      "2017-11-10 18:15:25: Loss and accuracy at step 6462: 0.0368008, 0.0096\n",
      "2017-11-10 18:15:27: Loss and accuracy at step 6463: 0.0367599, 0.0146\n",
      "2017-11-10 18:15:29: Loss and accuracy at step 6464: 0.0367469, 0.0078\n",
      "2017-11-10 18:15:31: Loss and accuracy at step 6465: 0.0368131, 0.0165\n",
      "2017-11-10 18:15:33: Loss and accuracy at step 6466: 0.0368048, 0.0132\n",
      "2017-11-10 18:15:35: Loss and accuracy at step 6467: 0.036756, 0.01\n",
      "2017-11-10 18:15:37: Loss and accuracy at step 6468: 0.0368392, 0.0129\n",
      "2017-11-10 18:15:39: Loss and accuracy at step 6469: 0.0367994, 0.0111\n",
      "2017-11-10 18:15:42: Loss and accuracy at step 6470: 0.0367312, 0.0108\n",
      "2017-11-10 18:15:44: Loss and accuracy at step 6471: 0.0367066, 0.0143\n",
      "2017-11-10 18:15:46: Loss and accuracy at step 6472: 0.0367257, 0.008\n",
      "2017-11-10 18:15:48: Loss and accuracy at step 6473: 0.0367482, 0.011\n",
      "2017-11-10 18:15:50: Loss and accuracy at step 6474: 0.0367775, 0.0114\n",
      "2017-11-10 18:15:52: Loss and accuracy at step 6475: 0.0367708, 0.0151\n",
      "2017-11-10 18:15:54: Loss and accuracy at step 6476: 0.0367725, 0.0115\n",
      "2017-11-10 18:15:56: Loss and accuracy at step 6477: 0.0368188, 0.014\n",
      "2017-11-10 18:15:58: Loss and accuracy at step 6478: 0.036691, 0.0175\n",
      "2017-11-10 18:16:00: Loss and accuracy at step 6479: 0.0367834, 0.0115\n",
      "2017-11-10 18:16:02: Loss and accuracy at step 6480: 0.0368128, 0.0131\n",
      "2017-11-10 18:16:04: Loss and accuracy at step 6481: 0.0367647, 0.0149\n",
      "2017-11-10 18:16:07: Loss and accuracy at step 6482: 0.0367504, 0.0104\n",
      "2017-11-10 18:16:09: Loss and accuracy at step 6483: 0.0367597, 0.0109\n",
      "2017-11-10 18:16:11: Loss and accuracy at step 6484: 0.036752, 0.0142\n",
      "2017-11-10 18:16:13: Loss and accuracy at step 6485: 0.0367951, 0.0139\n",
      "2017-11-10 18:16:15: Loss and accuracy at step 6486: 0.0368389, 0.0113\n",
      "2017-11-10 18:16:17: Loss and accuracy at step 6487: 0.0368272, 0.0121\n",
      "2017-11-10 18:16:19: Loss and accuracy at step 6488: 0.0367152, 0.0124\n",
      "2017-11-10 18:16:21: Loss and accuracy at step 6489: 0.0367741, 0.0119\n",
      "2017-11-10 18:16:23: Loss and accuracy at step 6490: 0.0367832, 0.0104\n",
      "2017-11-10 18:16:25: Loss and accuracy at step 6491: 0.03679, 0.0188\n",
      "2017-11-10 18:16:28: Loss and accuracy at step 6492: 0.0367419, 0.0079\n",
      "2017-11-10 18:16:30: Loss and accuracy at step 6493: 0.0367238, 0.0139\n",
      "2017-11-10 18:16:32: Loss and accuracy at step 6494: 0.0367591, 0.0146\n",
      "2017-11-10 18:16:34: Loss and accuracy at step 6495: 0.0368699, 0.0123\n",
      "2017-11-10 18:16:36: Loss and accuracy at step 6496: 0.0368012, 0.009\n",
      "2017-11-10 18:16:38: Loss and accuracy at step 6497: 0.0367314, 0.0145\n",
      "2017-11-10 18:16:40: Loss and accuracy at step 6498: 0.0367445, 0.0159\n",
      "2017-11-10 18:16:42: Loss and accuracy at step 6499: 0.0367443, 0.0097\n",
      "2017-11-10 18:16:45: Loss and accuracy at step 6500: 0.0367446, 0.0131\n",
      "2017-11-10 18:16:47: Loss and accuracy at step 6501: 0.0367412, 0.0168\n",
      "2017-11-10 18:16:49: Loss and accuracy at step 6502: 0.0368261, 0.008\n",
      "2017-11-10 18:16:51: Loss and accuracy at step 6503: 0.0367802, 0.0159\n",
      "2017-11-10 18:16:53: Loss and accuracy at step 6504: 0.0367676, 0.0114\n",
      "2017-11-10 18:16:55: Loss and accuracy at step 6505: 0.0368413, 0.0126\n",
      "2017-11-10 18:16:57: Loss and accuracy at step 6506: 0.0367874, 0.0127\n",
      "2017-11-10 18:16:59: Loss and accuracy at step 6507: 0.0368679, 0.0072\n",
      "2017-11-10 18:17:01: Loss and accuracy at step 6508: 0.036855, 0.0211\n",
      "2017-11-10 18:17:03: Loss and accuracy at step 6509: 0.0368362, 0.0084\n",
      "2017-11-10 18:17:05: Loss and accuracy at step 6510: 0.0367958, 0.013\n",
      "2017-11-10 18:17:07: Loss and accuracy at step 6511: 0.0367628, 0.0146\n",
      "2017-11-10 18:17:10: Loss and accuracy at step 6512: 0.0367155, 0.0092\n",
      "2017-11-10 18:17:12: Loss and accuracy at step 6513: 0.0367124, 0.0181\n",
      "2017-11-10 18:17:14: Loss and accuracy at step 6514: 0.0367017, 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:17:16: Loss and accuracy at step 6515: 0.0367036, 0.0188\n",
      "2017-11-10 18:17:18: Loss and accuracy at step 6516: 0.0367586, 0.0095\n",
      "2017-11-10 18:17:20: Loss and accuracy at step 6517: 0.0366698, 0.0103\n",
      "2017-11-10 18:17:22: Loss and accuracy at step 6518: 0.0368137, 0.0169\n",
      "2017-11-10 18:17:24: Loss and accuracy at step 6519: 0.0366605, 0.0091\n",
      "2017-11-10 18:17:26: Loss and accuracy at step 6520: 0.0368488, 0.0196\n",
      "2017-11-10 18:17:28: Loss and accuracy at step 6521: 0.0368663, 0.0089\n",
      "2017-11-10 18:17:31: Loss and accuracy at step 6522: 0.0368532, 0.0153\n",
      "2017-11-10 18:17:33: Loss and accuracy at step 6523: 0.0368568, 0.0107\n",
      "2017-11-10 18:17:35: Loss and accuracy at step 6524: 0.0368015, 0.0095\n",
      "2017-11-10 18:17:37: Loss and accuracy at step 6525: 0.0367092, 0.0183\n",
      "2017-11-10 18:17:39: Loss and accuracy at step 6526: 0.0367529, 0.0066\n",
      "2017-11-10 18:17:41: Loss and accuracy at step 6527: 0.0368319, 0.0215\n",
      "2017-11-10 18:17:43: Loss and accuracy at step 6528: 0.0367498, 0.0127\n",
      "2017-11-10 18:17:45: Loss and accuracy at step 6529: 0.0367713, 0.0068\n",
      "2017-11-10 18:17:47: Loss and accuracy at step 6530: 0.0368536, 0.0399\n",
      "2017-11-10 18:17:49: Loss and accuracy at step 6531: 0.0368091, 0.0085\n",
      "2017-11-10 18:17:51: Loss and accuracy at step 6532: 0.0368372, 0.0183\n",
      "2017-11-10 18:17:54: Loss and accuracy at step 6533: 0.0367402, 0.0147\n",
      "2017-11-10 18:17:56: Loss and accuracy at step 6534: 0.036736, 0.0119\n",
      "2017-11-10 18:17:58: Loss and accuracy at step 6535: 0.0367799, 0.0135\n",
      "2017-11-10 18:18:00: Loss and accuracy at step 6536: 0.0367707, 0.0202\n",
      "2017-11-10 18:18:02: Loss and accuracy at step 6537: 0.0368486, 0.0151\n",
      "2017-11-10 18:18:04: Loss and accuracy at step 6538: 0.0367082, 0.0089\n",
      "2017-11-10 18:18:07: Loss and accuracy at step 6539: 0.0367121, 0.0291\n",
      "2017-11-10 18:18:09: Loss and accuracy at step 6540: 0.0367615, 0.0102\n",
      "2017-11-10 18:18:11: Loss and accuracy at step 6541: 0.0368537, 0.0148\n",
      "2017-11-10 18:18:13: Loss and accuracy at step 6542: 0.0367401, 0.0157\n",
      "2017-11-10 18:18:15: Loss and accuracy at step 6543: 0.0367899, 0.01\n",
      "2017-11-10 18:18:17: Loss and accuracy at step 6544: 0.036771, 0.0243\n",
      "2017-11-10 18:18:20: Loss and accuracy at step 6545: 0.0367338, 0.0097\n",
      "2017-11-10 18:18:22: Loss and accuracy at step 6546: 0.0368136, 0.0158\n",
      "2017-11-10 18:18:24: Loss and accuracy at step 6547: 0.0367761, 0.0264\n",
      "2017-11-10 18:18:26: Loss and accuracy at step 6548: 0.0367855, 0.0089\n",
      "2017-11-10 18:18:28: Loss and accuracy at step 6549: 0.0367882, 0.015\n",
      "2017-11-10 18:18:30: Loss and accuracy at step 6550: 0.0368085, 0.0265\n",
      "2017-11-10 18:18:32: Loss and accuracy at step 6551: 0.0367819, 0.008\n",
      "2017-11-10 18:18:34: Loss and accuracy at step 6552: 0.0367913, 0.0136\n",
      "2017-11-10 18:18:36: Loss and accuracy at step 6553: 0.0368253, 0.0229\n",
      "2017-11-10 18:18:39: Loss and accuracy at step 6554: 0.036809, 0.0107\n",
      "2017-11-10 18:18:41: Loss and accuracy at step 6555: 0.0368002, 0.0119\n",
      "2017-11-10 18:18:43: Loss and accuracy at step 6556: 0.0367795, 0.0185\n",
      "2017-11-10 18:18:45: Loss and accuracy at step 6557: 0.036848, 0.0155\n",
      "2017-11-10 18:18:47: Loss and accuracy at step 6558: 0.0367639, 0.012\n",
      "2017-11-10 18:18:49: Loss and accuracy at step 6559: 0.036867, 0.0242\n",
      "2017-11-10 18:18:51: Loss and accuracy at step 6560: 0.0367692, 0.012\n",
      "2017-11-10 18:18:54: Loss and accuracy at step 6561: 0.036725, 0.0122\n",
      "2017-11-10 18:18:56: Loss and accuracy at step 6562: 0.0368432, 0.0169\n",
      "2017-11-10 18:18:58: Loss and accuracy at step 6563: 0.036814, 0.0091\n",
      "2017-11-10 18:19:00: Loss and accuracy at step 6564: 0.0368334, 0.0115\n",
      "2017-11-10 18:19:02: Loss and accuracy at step 6565: 0.0369184, 0.0095\n",
      "2017-11-10 18:19:04: Loss and accuracy at step 6566: 0.0368138, 0.0112\n",
      "2017-11-10 18:19:06: Loss and accuracy at step 6567: 0.0367522, 0.0165\n",
      "2017-11-10 18:19:08: Loss and accuracy at step 6568: 0.0368709, 0.0074\n",
      "2017-11-10 18:19:10: Loss and accuracy at step 6569: 0.0367224, 0.0192\n",
      "2017-11-10 18:19:13: Loss and accuracy at step 6570: 0.0367836, 0.0184\n",
      "2017-11-10 18:19:15: Loss and accuracy at step 6571: 0.0367315, 0.0078\n",
      "2017-11-10 18:19:17: Loss and accuracy at step 6572: 0.0368231, 0.0169\n",
      "2017-11-10 18:19:19: Loss and accuracy at step 6573: 0.0367948, 0.015\n",
      "2017-11-10 18:19:21: Loss and accuracy at step 6574: 0.0367128, 0.0106\n",
      "2017-11-10 18:19:23: Loss and accuracy at step 6575: 0.0367604, 0.0163\n",
      "2017-11-10 18:19:25: Loss and accuracy at step 6576: 0.036786, 0.0142\n",
      "2017-11-10 18:19:27: Loss and accuracy at step 6577: 0.0367837, 0.0139\n",
      "2017-11-10 18:19:30: Loss and accuracy at step 6578: 0.0368051, 0.0208\n",
      "2017-11-10 18:19:32: Loss and accuracy at step 6579: 0.0368227, 0.0092\n",
      "2017-11-10 18:19:34: Loss and accuracy at step 6580: 0.0367762, 0.0216\n",
      "2017-11-10 18:19:36: Loss and accuracy at step 6581: 0.0367458, 0.0149\n",
      "2017-11-10 18:19:38: Loss and accuracy at step 6582: 0.0367118, 0.0138\n",
      "2017-11-10 18:19:40: Loss and accuracy at step 6583: 0.0367862, 0.0289\n",
      "2017-11-10 18:19:42: Loss and accuracy at step 6584: 0.0367886, 0.0125\n",
      "2017-11-10 18:19:45: Loss and accuracy at step 6585: 0.0367584, 0.0142\n",
      "2017-11-10 18:19:47: Loss and accuracy at step 6586: 0.0367432, 0.0206\n",
      "2017-11-10 18:19:49: Loss and accuracy at step 6587: 0.0368289, 0.0162\n",
      "2017-11-10 18:19:51: Loss and accuracy at step 6588: 0.0367927, 0.0155\n",
      "2017-11-10 18:19:53: Loss and accuracy at step 6589: 0.0367384, 0.0115\n",
      "2017-11-10 18:19:55: Loss and accuracy at step 6590: 0.0368008, 0.031\n",
      "2017-11-10 18:19:57: Loss and accuracy at step 6591: 0.0368278, 0.0089\n",
      "2017-11-10 18:19:59: Loss and accuracy at step 6592: 0.0367416, 0.0122\n",
      "2017-11-10 18:20:01: Loss and accuracy at step 6593: 0.0368899, 0.0598\n",
      "2017-11-10 18:20:04: Loss and accuracy at step 6594: 0.0368359, 0.0055\n",
      "2017-11-10 18:20:06: Loss and accuracy at step 6595: 0.0368009, 0.0155\n",
      "2017-11-10 18:20:08: Loss and accuracy at step 6596: 0.0368714, 0.0963\n",
      "2017-11-10 18:20:10: Loss and accuracy at step 6597: 0.0368904, 0.0073\n",
      "2017-11-10 18:20:12: Loss and accuracy at step 6598: 0.0367464, 0.0169\n",
      "2017-11-10 18:20:14: Loss and accuracy at step 6599: 0.0368281, 0.0639\n",
      "2017-11-10 18:20:16: Loss and accuracy at step 6600: 0.0368706, 0.0134\n",
      "2017-11-10 18:20:18: Loss and accuracy at step 6601: 0.0368005, 0.0145\n",
      "2017-11-10 18:20:20: Loss and accuracy at step 6602: 0.0367121, 0.0343\n",
      "2017-11-10 18:20:23: Loss and accuracy at step 6603: 0.0367714, 0.0149\n",
      "2017-11-10 18:20:25: Loss and accuracy at step 6604: 0.0367371, 0.0185\n",
      "2017-11-10 18:20:27: Loss and accuracy at step 6605: 0.0367972, 0.0203\n",
      "2017-11-10 18:20:29: Loss and accuracy at step 6606: 0.0367767, 0.014\n",
      "2017-11-10 18:20:31: Loss and accuracy at step 6607: 0.0368053, 0.022\n",
      "2017-11-10 18:20:33: Loss and accuracy at step 6608: 0.0367325, 0.0112\n",
      "2017-11-10 18:20:35: Loss and accuracy at step 6609: 0.036792, 0.0088\n",
      "2017-11-10 18:20:37: Loss and accuracy at step 6610: 0.036732, 0.0214\n",
      "2017-11-10 18:20:39: Loss and accuracy at step 6611: 0.0368834, 0.014\n",
      "2017-11-10 18:20:42: Loss and accuracy at step 6612: 0.0367946, 0.01\n",
      "2017-11-10 18:20:44: Loss and accuracy at step 6613: 0.0367754, 0.0166\n",
      "2017-11-10 18:20:46: Loss and accuracy at step 6614: 0.0367337, 0.0137\n",
      "2017-11-10 18:20:48: Loss and accuracy at step 6615: 0.0369059, 0.0101\n",
      "2017-11-10 18:20:50: Loss and accuracy at step 6616: 0.0368397, 0.0139\n",
      "2017-11-10 18:20:52: Loss and accuracy at step 6617: 0.0366751, 0.0166\n",
      "2017-11-10 18:20:55: Loss and accuracy at step 6618: 0.0367563, 0.012\n",
      "2017-11-10 18:20:57: Loss and accuracy at step 6619: 0.0367509, 0.0199\n",
      "2017-11-10 18:20:59: Loss and accuracy at step 6620: 0.0367006, 0.0148\n",
      "2017-11-10 18:21:01: Loss and accuracy at step 6621: 0.0368307, 0.0136\n",
      "2017-11-10 18:21:03: Loss and accuracy at step 6622: 0.0367548, 0.0177\n",
      "2017-11-10 18:21:05: Loss and accuracy at step 6623: 0.0367838, 0.0103\n",
      "2017-11-10 18:21:07: Loss and accuracy at step 6624: 0.0367324, 0.0214\n",
      "2017-11-10 18:21:09: Loss and accuracy at step 6625: 0.0367145, 0.017\n",
      "2017-11-10 18:21:12: Loss and accuracy at step 6626: 0.0368302, 0.0089\n",
      "2017-11-10 18:21:14: Loss and accuracy at step 6627: 0.0367955, 0.0239\n",
      "2017-11-10 18:21:16: Loss and accuracy at step 6628: 0.0368306, 0.0104\n",
      "2017-11-10 18:21:18: Loss and accuracy at step 6629: 0.0367331, 0.0126\n",
      "2017-11-10 18:21:20: Loss and accuracy at step 6630: 0.0367606, 0.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-10 18:21:22: Loss and accuracy at step 6631: 0.0368122, 0.0081\n",
      "2017-11-10 18:21:24: Loss and accuracy at step 6632: 0.0367666, 0.0149\n",
      "2017-11-10 18:21:26: Loss and accuracy at step 6633: 0.0367654, 0.0141\n",
      "2017-11-10 18:21:29: Loss and accuracy at step 6634: 0.0367928, 0.0091\n",
      "2017-11-10 18:21:31: Loss and accuracy at step 6635: 0.0367243, 0.0189\n",
      "2017-11-10 18:21:33: Loss and accuracy at step 6636: 0.0367024, 0.012\n",
      "2017-11-10 18:21:35: Loss and accuracy at step 6637: 0.0367732, 0.0112\n",
      "2017-11-10 18:21:37: Loss and accuracy at step 6638: 0.0368069, 0.0259\n",
      "2017-11-10 18:21:39: Loss and accuracy at step 6639: 0.0368623, 0.0098\n",
      "2017-11-10 18:21:41: Loss and accuracy at step 6640: 0.0367947, 0.0113\n",
      "2017-11-10 18:21:43: Loss and accuracy at step 6641: 0.0367505, 0.0271\n",
      "2017-11-10 18:21:45: Loss and accuracy at step 6642: 0.0368182, 0.0123\n",
      "2017-11-10 18:21:48: Loss and accuracy at step 6643: 0.0368639, 0.0107\n",
      "2017-11-10 18:21:50: Loss and accuracy at step 6644: 0.0367635, 0.0173\n",
      "2017-11-10 18:21:52: Loss and accuracy at step 6645: 0.0367629, 0.0134\n",
      "2017-11-10 18:21:54: Loss and accuracy at step 6646: 0.03683, 0.0139\n",
      "2017-11-10 18:21:56: Loss and accuracy at step 6647: 0.036853, 0.0155\n",
      "2017-11-10 18:21:58: Loss and accuracy at step 6648: 0.0368055, 0.0117\n",
      "2017-11-10 18:22:00: Loss and accuracy at step 6649: 0.0367022, 0.0124\n",
      "2017-11-10 18:22:02: Loss and accuracy at step 6650: 0.0367887, 0.0097\n",
      "2017-11-10 18:22:04: Loss and accuracy at step 6651: 0.0368638, 0.0143\n",
      "2017-11-10 18:22:06: Loss and accuracy at step 6652: 0.0367474, 0.0155\n",
      "2017-11-10 18:22:09: Loss and accuracy at step 6653: 0.0367693, 0.0112\n",
      "2017-11-10 18:22:11: Loss and accuracy at step 6654: 0.0367868, 0.0125\n",
      "2017-11-10 18:22:13: Loss and accuracy at step 6655: 0.0368278, 0.013\n",
      "2017-11-10 18:22:15: Loss and accuracy at step 6656: 0.0367569, 0.0096\n",
      "2017-11-10 18:22:17: Loss and accuracy at step 6657: 0.0368836, 0.0185\n",
      "2017-11-10 18:22:19: Loss and accuracy at step 6658: 0.0368246, 0.0122\n",
      "2017-11-10 18:22:21: Loss and accuracy at step 6659: 0.0367138, 0.0113\n",
      "2017-11-10 18:22:23: Loss and accuracy at step 6660: 0.0368061, 0.03\n",
      "2017-11-10 18:22:25: Loss and accuracy at step 6661: 0.0367915, 0.0119\n",
      "2017-11-10 18:22:27: Loss and accuracy at step 6662: 0.0367495, 0.0137\n",
      "2017-11-10 18:22:30: Loss and accuracy at step 6663: 0.0366678, 0.0199\n",
      "2017-11-10 18:22:32: Loss and accuracy at step 6664: 0.0368063, 0.0101\n",
      "2017-11-10 18:22:34: Loss and accuracy at step 6665: 0.0368052, 0.0074\n",
      "2017-11-10 18:22:36: Loss and accuracy at step 6666: 0.0367734, 0.0195\n",
      "2017-11-10 18:22:38: Loss and accuracy at step 6667: 0.0366897, 0.0121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-121eea65a2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: Loss and accuracy at step %s: %s, %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-be9295578f83>\u001b[0m in \u001b[0;36mnext_training_batch\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnext_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mboards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandomBoard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboardPartialMineCounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodeCountsOneHot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidGuesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(1000001):\n",
    "    x_batch, y_batch, _ = next_training_batch(10000)\n",
    "    loss, accuracy = model.train_on_batch(np.array(x_batch), np.array(y_batch))\n",
    "    print('%s: Loss and accuracy at step %s: %s, %s' % (dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), iteration, loss, accuracy))\n",
    "    if iteration % 500 == 0:\n",
    "        model.save('%s/model-%s.hs' % savePath, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
